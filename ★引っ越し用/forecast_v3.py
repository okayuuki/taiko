#äºˆæ¸¬ç”¨

#! ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®import
import os
import pandas as pd
import warnings
import numpy as np
import pandas as pd
#%matplotlib inline#Jupyter Notebook å°‚ç”¨ã®ãƒã‚¸ãƒƒã‚¯ã‚³ãƒãƒ³ãƒ‰ã€‚ãƒ¡ãƒ³ãƒ†ç”¨ã§åˆ©ç”¨
import matplotlib.pyplot as plt
import shap
import seaborn as sns
import matplotlib as mpl
from dateutil.relativedelta import relativedelta
from IPython.display import display, clear_output
from sklearn.ensemble import RandomForestRegressor
from sklearn.impute import SimpleImputer
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
from matplotlib.gridspec import GridSpec
from datetime import datetime
from datetime import timedelta
from PIL import Image
from sklearn.linear_model import Ridge
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Lasso
from sklearn.metrics import mean_squared_error, max_error, mean_absolute_error
import streamlit as st
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime, date, time
import pickle
from sklearn.preprocessing import StandardScaler
import streamlit.components.v1 as components
import base64

#! è‡ªä½œãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®import
from read_v3 import read_data, process_Activedata, read_syozailt_by_using_archive_data, read_activedata_by_using_archive_data,read_zaiko_by_using_archive_data, calculate_supplier_truck_arrival_types2

from functions_v3 import process_shiresakibin_flag

#! ãƒªãƒŸãƒƒãƒˆè¨ˆç®—
def show_forecast( unique_product, start_datetime, selected_zaiko):

    start_date = '2024-05-01-00'
    end_date = '2024-08-31-00'

    #! å“ç•ªã€æ•´å‚™å®¤ã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡º
    product = unique_product.split('_')[0]
    seibishitsu = unique_product.split('_')[1]

    #! ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
    prediction_hours = 24#ä½•æ™‚é–“å…ˆã¾ã§äºˆæ¸¬ã™ã‚‹ã®ã‹
    past_hours = 5
    lookback_hours = past_hours+2

    # ã‚¿ã‚¤ãƒˆãƒ«è¡¨ç¤º
    st.header('äºˆæ¸¬çµæœ')

    #!----------------------------------------------------------------------- 
    #! è‡ªå‹•ãƒ©ãƒƒã‚¯ã®åœ¨åº«ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨å‡¦ç†
    #!-----------------------------------------------------------------------
    zaiko_df = read_zaiko_by_using_archive_data(start_date, end_date)
    # å“ç•ªåˆ—ã®ç©ºç™½ã‚’å‰Šé™¤
    zaiko_df['å“ç•ª'] = zaiko_df['å“ç•ª'].str.strip()
    # 'è¨ˆæ¸¬æ—¥æ™‚'ã‚’datetimeå‹ã«å¤‰æ›
    #zaiko_df['è¨ˆæ¸¬æ—¥æ™‚'] = pd.to_datetime(zaiko_df['è¨ˆæ¸¬æ—¥æ™‚'], errors='coerce')
    # åˆ—å 'è¨ˆæ¸¬æ—¥æ™‚' ã‚’ 'æ—¥æ™‚' ã«å¤‰æ›´
    #zaiko_df = zaiko_df.rename(columns={'è¨ˆæ¸¬æ—¥æ™‚': 'æ—¥æ™‚'})
    # ç‰¹å®šã®å“ç•ªã®å•†å“ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
    zaiko_df = zaiko_df[zaiko_df['å“ç•ª'] == product]
    # ç‰¹å®šã®æ—¥æ™‚ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
    zaiko_df = zaiko_df[zaiko_df['æ—¥æ™‚'] == start_datetime]
    # æ—¥æ™‚ã‚’å†åº¦datetimeå‹ã«å¤‰æ›ï¼ˆå¿µã®ãŸã‚ï¼‰
    zaiko_df['æ—¥æ™‚'] = pd.to_datetime(zaiko_df['æ—¥æ™‚'])
    # 'æ—¥æ™‚' ã¨ 'åœ¨åº«æ•°ï¼ˆç®±ï¼‰' ã®åˆ—ã®ã¿ã‚’æŠ½å‡º
    zaiko_extracted = zaiko_df[['æ—¥æ™‚', 'åœ¨åº«æ•°ï¼ˆç®±ï¼‰']]

    #!-----------------------------------------------------------------------
    #! æ‰€åœ¨ç®¡ç†ãƒªãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ ã®ãƒ‡ãƒ¼ã‚¿
    #!-----------------------------------------------------------------------
    #file_path = 'ä¸­é–“æˆæœç‰©/æ‰€åœ¨ç®¡ç†MBãƒ†ã‚™ãƒ¼ã‚¿_çµ±åˆæ¸ˆ&ç‰¹å®šæ—¥æ™‚æŠ½å‡ºæ¸ˆ.csv'
    Timestamp_df = read_syozailt_by_using_archive_data(start_date, end_date)
    # 'æ›´æ–°æ—¥æ™‚'åˆ—ã«ç„¡åŠ¹ãªæ—¥æ™‚ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹è¡Œã‚’å‰Šé™¤ã™ã‚‹
    data_cleaned = Timestamp_df.dropna(subset=['æ¤œåæ—¥æ™‚'])
    st.dataframe(data_cleaned.head(50000))
    # ç‰¹å®šã®å“ç•ªã®å•†å“ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
    data_cleaned = data_cleaned[(data_cleaned['å“ç•ª'] == product) & (data_cleaned['æ•´å‚™å®¤ã‚³ãƒ¼ãƒ‰'] == seibishitsu)]
    # æ™‚é–“ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã€å„æ™‚é–“ã§ã®ã‹ã‚“ã°ã‚“æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆã™ã‚‹
    data_cleaned['æ—¥æ™‚'] = data_cleaned['æ¤œåæ—¥æ™‚'].dt.floor('H')  # æ™‚é–“å˜ä½ã«ä¸¸ã‚ã‚‹
    hourly_kanban_count = data_cleaned.groupby('æ—¥æ™‚').size().reset_index(name='ç´å…¥äºˆå®šã‹ã‚“ã°ã‚“æ•°')
    #st.dataframe(hourly_kanban_count)

    # æ™‚é–“ã®ç¯„å›²ã‚’æ±ºå®šã—ã€æ¬ ææ™‚é–“å¸¯ã‚’è£œå®Œã™ã‚‹
    full_time_range = pd.date_range(start=hourly_kanban_count['æ—¥æ™‚'].min(),end=hourly_kanban_count['æ—¥æ™‚'].max(),freq='H')

    # å…¨ã¦ã®æ™‚é–“ã‚’å«ã‚€ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä½œæˆã—ã€æ¬ æå€¤ã‚’0ã§åŸ‹ã‚ã‚‹
    hourly_kanban_count_full = pd.DataFrame(full_time_range, columns=['æ—¥æ™‚']).merge(hourly_kanban_count, on='æ—¥æ™‚', how='left').fillna(0)

    # ã‹ã‚“ã°ã‚“æ•°ã‚’æ•´æ•°ã«æˆ»ã™
    hourly_kanban_count_full['ç´å…¥äºˆå®šã‹ã‚“ã°ã‚“æ•°'] = hourly_kanban_count_full['ç´å…¥äºˆå®šã‹ã‚“ã°ã‚“æ•°'].astype(int)

    # 'äºˆæ¸¬å…¥åº«æ™‚é–“'åˆ—ã¨ã—ã¦ã€5æ™‚é–“å‰ã®ã‹ã‚“ã°ã‚“æ•°ã‚’è¿½åŠ ã™ã‚‹
    hourly_kanban_count_full['å·¥å ´åˆ°ç€äºˆå®šã‹ã‚“ã°ã‚“æ•°'] = hourly_kanban_count_full['ç´å…¥äºˆå®šã‹ã‚“ã°ã‚“æ•°'].shift(past_hours)

    # æ¬ æå€¤ï¼ˆæœ€åˆã®5æ™‚é–“åˆ†ï¼‰ã‚’0ã§åŸ‹ã‚ã‚‹
    hourly_kanban_count_full['å·¥å ´åˆ°ç€äºˆå®šã‹ã‚“ã°ã‚“æ•°'] = hourly_kanban_count_full['å·¥å ´åˆ°ç€äºˆå®šã‹ã‚“ã°ã‚“æ•°'].fillna(0).astype(int)

    #!-----------------------------------------------------------------------
    #! Activedataã®å‡¦ç†
    #!-----------------------------------------------------------------------
    activedata = read_activedata_by_using_archive_data(start_date, end_date, 0)
    # ç‰¹å®šã®å“ç•ªã®å•†å“ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
    activedata = activedata[activedata['å“ç•ª'] == product]
    #st.dataframe(activedata)
    #! ç¨¼åƒæ™‚é–“ã§å‰²ã‚‹å‡¦ç† (ä¼‘æ†©æ™‚é–“ã®è€ƒæ…®ãŒå¿…è¦ã‹ï¼Ÿ)
    activedata['æ—¥é‡æ•°ï¼ˆç®±æ•°ï¼‰'] = activedata['æ—¥é‡æ•°']/activedata['åå®¹æ•°']
    activedata['æ—¥é‡æ•°ï¼ˆç®±æ•°ï¼‰/ç¨¼åƒæ™‚é–“'] = activedata['æ—¥é‡æ•°ï¼ˆç®±æ•°ï¼‰'] / 16.5
    activedata['æ—¥ä»˜'] = pd.to_datetime(activedata['æ—¥ä»˜'])#ã“ã‚Œã—ãªã„ã¨æ¬¡ã®.resample('H')ã§ã‚¨ãƒ©ãƒ¼ãŒå‡ºã‚‹
    # æ—¥ä»˜ã‚’åŸºæº–ã«1æ™‚é–“ã”ã¨ã®ãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›
    activedata = activedata.set_index('æ—¥ä»˜').resample('H').ffill().reset_index()
    # 'æ—¥ä»˜' ã‚’datetimeå‹ã«å¤‰æ›
    activedata['æ—¥ä»˜'] = pd.to_datetime(activedata['æ—¥ä»˜'])
    activedata = activedata.rename(columns={'æ—¥ä»˜': 'æ—¥æ™‚'})
    # 'æ—¥ä»˜' ã¨ 'æ—¥é‡æ•°ï¼ˆç®±æ•°ï¼‰' ã®åˆ—ã®ã¿ã‚’æŠ½å‡º
    activedata_extracted = activedata[['æ—¥æ™‚', 'æ—¥é‡æ•°ï¼ˆç®±æ•°ï¼‰/ç¨¼åƒæ™‚é–“']]

    # åœ¨åº«ãƒ‡ãƒ¼ã‚¿ã®é–‹å§‹æ™‚åˆ»ã‚’å–å¾—
    start_time = zaiko_extracted.iloc[0]['æ—¥æ™‚']
    # é–‹å§‹æ™‚åˆ»ã‹ã‚‰20æ™‚é–“å¾Œã¾ã§ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
    end_time = start_time + pd.Timedelta(hours=prediction_hours)
    filtered_activedata = activedata_extracted[(activedata_extracted['æ—¥æ™‚'] >= start_time) & (activedata_extracted['æ—¥æ™‚'] < end_time)]

    # å„æ™‚é–“å¾Œã®æ¶ˆè²»é‡ãŠã‚ˆã³å…¥åº«é‡ã‚’è€ƒæ…®ã—ãŸåœ¨åº«æ•°ã‚’è¨ˆç®—
    inventory_after_adjustments = []
    # ç¾åœ¨ã®åœ¨åº«æ•°ã‚’åˆæœŸå€¤ã¨ã—ã¦è¨­å®š
    current_inventory = selected_zaiko#zaiko_extracted.iloc[0]['åœ¨åº«æ•°ï¼ˆç®±ï¼‰']

    # 3ã¤ã®åˆ—ã‚’ä½œæˆ
    col1, col2 = st.columns(2)
    col1.metric(label="é¸æŠã•ã‚ŒãŸæ—¥æ™‚", value=str(start_datetime))#, delta="1 mph")
    col2.metric(label="å…¥åŠ›ã•ã‚ŒãŸçµ„ç«‹ãƒ©ã‚¤ãƒ³ã®åœ¨åº«æ•°ï¼ˆç®±ï¼‰", value=int(current_inventory))

    # æ™‚é–“ã”ã¨ã®åœ¨åº«æ•°ã‚’æ›´æ–°ã—ãªãŒã‚‰ãƒªã‚¹ãƒˆã«è¿½åŠ 
    for i, row in filtered_activedata.iterrows():
        kanban_row = hourly_kanban_count_full[hourly_kanban_count_full['æ—¥æ™‚'] == row['æ—¥æ™‚']]
        incoming_kanban = kanban_row['å·¥å ´åˆ°ç€äºˆå®šã‹ã‚“ã°ã‚“æ•°'].values[0] if not kanban_row.empty else 0
        inventory_after_adjustments.append({
            'æ—¥æ™‚': row['æ—¥æ™‚'],
            'åœ¨åº«æ•°ï¼ˆç®±ï¼‰': current_inventory
        })
        # æœ€åˆã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã§ã¯æ¶ˆè²»ã‚’å¼•ã‹ãªã„ãŒã€ä»¥é™ã¯æ¶ˆè²»é‡ã¨å…¥åº«é‡ã‚’èª¿æ•´
        if i != 0:
            current_inventory = current_inventory - row['æ—¥é‡æ•°ï¼ˆç®±æ•°ï¼‰/ç¨¼åƒæ™‚é–“']  # æ¶ˆè²»é‡ã‚’å¼•ã
            current_inventory = current_inventory + incoming_kanban  # å…¥åº«é‡ã‚’è¶³ã™
            

    # è¨ˆç®—çµæœã‚’DataFrameã«å¤‰æ›
    inventory_df_adjusted = pd.DataFrame(inventory_after_adjustments)

    # æœ€åˆã®æ™‚é–“ã®ãƒ‡ãƒ¼ã‚¿ï¼ˆå®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ï¼‰ã¨ãã‚Œä»¥é™ã®äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ã«åˆ†å‰²
    actual_data = inventory_df_adjusted.iloc[0:1]  # æœ€åˆã®1æ™‚é–“åˆ†ã¯å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿
    forecast_data = inventory_df_adjusted.iloc[1:]  # ãã‚Œä»¥é™ã¯äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿

    # æ™‚é–“è»¸ã‚’çµ±ä¸€ã™ã‚‹ãŸã‚ã€å…¨æ™‚é–“ã®ç¯„å›²ã‚’ä½œæˆ
    #full_time_range = pd.date_range(start=actual_data['æ—¥æ™‚'].min(), end=forecast_data['æ—¥æ™‚'].max(), freq='H')

    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ãã‚Œãã‚Œã“ã®æ™‚é–“è»¸ã«åˆã‚ã›ã¦å†æ§‹ç¯‰ã—ã€æ¬ æå€¤ã‚’åŸ‹ã‚ã‚‹
    #actual_data = actual_data.set_index('æ—¥æ™‚').reindex(full_time_range).reset_index().rename(columns={'index': 'æ—¥æ™‚'})
    #forecast_data = forecast_data.set_index('æ—¥æ™‚').reindex(full_time_range).reset_index().rename(columns={'index': 'æ—¥æ™‚'})

    # æ¬ æå€¤ã¯ãã‚Œãã‚Œ0ã«ç½®ãæ›ãˆã‚‹ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
    #actual_data['åœ¨åº«æ•°ï¼ˆç®±ï¼‰'].fillna(0, inplace=True)
    #forecast_data['åœ¨åº«æ•°ï¼ˆç®±ï¼‰'].fillna(0, inplace=True)

    # ã‚°ãƒ©ãƒ•ã®ä½œæˆ
    fig = go.Figure()

    # å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã‚’é’è‰²ã§æç”»
    fig.add_trace(go.Bar(
        x=actual_data['æ—¥æ™‚'], 
        y=actual_data['åœ¨åº«æ•°ï¼ˆç®±ï¼‰'], 
        name='å®Ÿç¸¾', 
        marker_color='blue', 
        opacity=0.3
    ))

    # äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ã‚’ã‚ªãƒ¬ãƒ³ã‚¸è‰²ã§è¿½åŠ æç”»
    fig.add_trace(go.Bar(
        x=forecast_data['æ—¥æ™‚'], 
        y=forecast_data['åœ¨åº«æ•°ï¼ˆç®±ï¼‰'], 
        name='äºˆæ¸¬', 
        marker_color='orange', 
        opacity=0.3
    ))

    # xè»¸ã‚’1æ™‚é–“ã”ã¨ã«è¡¨ç¤ºã™ã‚‹è¨­å®š
    fig.update_layout(
        title='äºˆæ¸¬çµæœ',  # ã“ã“ã§ã‚¿ã‚¤ãƒˆãƒ«ã‚’è¨­å®š
        xaxis_title='æ—¥æ™‚',  # xè»¸ã‚¿ã‚¤ãƒˆãƒ«
        yaxis_title='åœ¨åº«æ•°ï¼ˆç®±ï¼‰',  # yè»¸ã‚¿ã‚¤ãƒˆãƒ«
        xaxis=dict(
            tickformat="%Y-%m-%d %H:%M",  # æ—¥æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’æŒ‡å®š
            dtick=3600000  # 1æ™‚é–“ã”ã¨ã«è¡¨ç¤º (3600000ãƒŸãƒªç§’ = 1æ™‚é–“)
        ),
        barmode='group'  # è¤‡æ•°ã®ãƒãƒ¼ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    )

    # ã‚°ãƒ©ãƒ•ã‚’Streamlitã§è¡¨ç¤º
    st.plotly_chart(fig)

    # 5æ™‚é–“å‰ã®æ—¥æ™‚ã‚’è¨ˆç®—
    hours_before = start_time - pd.Timedelta(hours=lookback_hours)

    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«çµæœã‚’è¡¨ç¤ºã™ã‚‹
    hourly_kanban_count_full = hourly_kanban_count_full[(hourly_kanban_count_full['æ—¥æ™‚'] >= hours_before) & (hourly_kanban_count_full['æ—¥æ™‚'] < end_time)]

    # æ–°ã—ã„åˆ—ã€Œå‚™è€ƒã€ã‚’è¿½åŠ ã—ã€start_timeã«åŸºã¥ã„ã¦ã€Œéå»ã€ã€Œæœªæ¥ã€ã¨è¡¨ç¤º
    hourly_kanban_count_full['â€»æ³¨é‡ˆ                                                                               '] = hourly_kanban_count_full['æ—¥æ™‚'].apply(
        lambda x: 'ã‚ãªãŸã¯ã“ã®æ™‚é–“ã‚’é¸æŠã—ã¾ã—ãŸ' if x == start_time else ('éå»' if x < start_time else 'æœªæ¥')
    )

    # 'æ—¥æ™‚'åˆ—ã§start_timeã«ä¸€è‡´ã™ã‚‹è¡Œã‚’ãƒã‚¤ãƒ©ã‚¤ãƒˆ
    def highlight_start_time(row):
        return ['background-color: yellow' if row['æ—¥æ™‚'] == start_time else '' for _ in row]
    
    st.code(f"ğŸ“ è¨ˆç®—å¼ï¼šæœªæ¥ã®åœ¨åº«æ•° = åœ¨åº«æ•° + å·¥å ´åˆ°ç€äºˆå®šã‹ã‚“ã°ã‚“æ•° - æ—¥é‡ç®±æ•°/ç¨¼åƒæ™‚é–“")

    # æ³¨é‡ˆã‚’è¿½åŠ ï¼ˆä¾‹ã¨ã—ã¦start_timeã‚’è¡¨ç¤ºï¼‰
    st.markdown(f"")
    st.markdown(f"")
    st.markdown(f"**ä¸‹ã®è¡¨ã§äºˆæ¸¬ã®å†…å®¹ã‚’ç¢ºèªã§ãã¾ã™ã€‚**")
    #st.code(f"è¨ˆç®—å¼ï¼šåœ¨åº«æ•° + å·¥å ´åˆ°ç€äºˆå®šã‹ã‚“ã°ã‚“æ•° - æ—¥é‡ç®±æ•°/ç¨¼åƒæ™‚é–“")

    # 'hourly_kanban_count_full' ã¨ 'inventory_df_adjusted' ã‚’ 'æ—¥æ™‚' ã‚’ã‚­ãƒ¼ã«çµåˆ
    merged_df = pd.merge(hourly_kanban_count_full, inventory_df_adjusted, on='æ—¥æ™‚', how='outer')
    activedata_extracted = activedata_extracted[(activedata_extracted['æ—¥æ™‚'] >= hours_before) & (activedata_extracted['æ—¥æ™‚'] < end_time)]
    merged_df = pd.merge(merged_df, activedata_extracted, on='æ—¥æ™‚', how='outer')

    # å¿…è¦ã«å¿œã˜ã¦NaNã‚’0ã«ç½®ãæ›ãˆã‚‹ï¼ˆåœ¨åº«æ•°ã‚„ã‹ã‚“ã°ã‚“æ•°ã«é–¢ã—ã¦ï¼‰
    merged_df.fillna(0, inplace=True)

    # Streamlitã§è¡¨ç¤º
    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®åˆ—ã®é †ç•ªã‚’æŒ‡å®š
    new_column_order = ['æ—¥æ™‚', 'ç´å…¥äºˆå®šã‹ã‚“ã°ã‚“æ•°', 'å·¥å ´åˆ°ç€äºˆå®šã‹ã‚“ã°ã‚“æ•°', 'æ—¥é‡æ•°ï¼ˆç®±æ•°ï¼‰/ç¨¼åƒæ™‚é–“', 'åœ¨åº«æ•°ï¼ˆç®±ï¼‰','â€»æ³¨é‡ˆ                                                                               ']
    # åˆ—ã®é †ç•ªã‚’å¤‰æ›´
    merged_df = merged_df[new_column_order]

    # æ¡ä»¶ã«è©²å½“ã™ã‚‹è¡Œã®åœ¨åº«æ•°ã‚’ "-" ã«ã™ã‚‹
    merged_df.loc[
        (merged_df['æ—¥æ™‚'] >= hours_before) & 
        (merged_df['æ—¥æ™‚'] < start_time), 
        'åœ¨åº«æ•°ï¼ˆç®±ï¼‰'
    ] = "-"

    # 'æ—¥æ™‚'åˆ—ã§start_timeã«ä¸€è‡´ã™ã‚‹è¡Œã‚’ãƒã‚¤ãƒ©ã‚¤ãƒˆã—ã¦è¡¨ç¤º
    st.dataframe(merged_df.style.apply(highlight_start_time, axis=1))

#! åœ¨åº«ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
def show_zaiko_simulation( selected_datetime, change_rate):

    #! æ—¥é‡ç®±æ•°ã‚’æ™‚é–“å˜ä½ã«ã™ã‚‹ãŸã‚ã«
    #todo ç¨¼åƒæ™‚é–“ãªã©ã‚’è€ƒãˆã‚‹ãªã‚‰ã€16.5ã§å‰²ã‚‹å¿…è¦ãŒã‚ã‚‹ã‹ã‚‚ã ãŒã€ãã®å ´åˆã¯ã©ã®æ™‚é–“å¸¯ãŒç¨¼åƒæ™‚é–“åˆ†ã‹ã‚‹å¿…è¦ãŒã‚ã‚‹
    kado_time = 24

    #! é¸æŠæƒ…å ±è¡¨ç¤º
    col1, col2 = st.columns(2)
    col1.metric(label="é¸æŠæ—¥æ™‚", value=selected_datetime.strftime("%Y-%m-%d %H:%M"))
    col2.metric(label="é¸æŠå¤‰å‹•ç‡", value=change_rate)

    # 1æ™‚é–“ã”ã¨ã®æ™‚é–“åˆ—ï¼ˆ24æ™‚é–“åˆ†ï¼‰ã‚’ä½œæˆ
    time_series = pd.date_range(start=selected_datetime, periods=24, freq="H")
    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä½œæˆ
    time_df = pd.DataFrame({"æ—¥æ™‚": time_series})
    #st.dataframe(time_df)

    #! è‡ªå‹•ãƒ©ãƒƒã‚¯ã®åœ¨åº«ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    # todo å¼•æ•°é–¢ä¿‚ãªãå…¨ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã—ã¦ã‚‹
    zaiko_df = read_zaiko_by_using_archive_data(selected_datetime.strftime('%Y-%m-%d-%H'), selected_datetime.strftime('%Y-%m-%d-%H'))
    #! å“ç•ªåˆ—ã‚’æ˜‡é †ã«ã‚½ãƒ¼ãƒˆ
    zaiko_df = zaiko_df.sort_values(by='å“ç•ª', ascending=True)
    #! ç„¡åŠ¹ãªå€¤ã‚’ NaN ã«å¤‰æ›
    zaiko_df['æ‹ ç‚¹æ‰€ç•ªåœ°'] = pd.to_numeric(zaiko_df['æ‹ ç‚¹æ‰€ç•ªåœ°'], errors='coerce')
    #! å“ç•ªã”ã¨ã«æ¬ æå€¤ï¼ˆNaNï¼‰ã‚’åŸ‹ã‚ã‚‹(å‰æ–¹åŸ‹ã‚å¾Œæ–¹åŸ‹ã‚)
    zaiko_df['æ‹ ç‚¹æ‰€ç•ªåœ°'] = zaiko_df.groupby('å“ç•ª')['æ‹ ç‚¹æ‰€ç•ªåœ°'].transform(lambda x: x.fillna(method='ffill').fillna(method='bfill'))
    #! ãã‚Œã§ã‚‚ç½®æ›ã§ããªã„ã‚‚ã®ã¯NaN ã‚’ 0 ã§åŸ‹ã‚ã‚‹
    zaiko_df['æ‹ ç‚¹æ‰€ç•ªåœ°'] = zaiko_df['æ‹ ç‚¹æ‰€ç•ªåœ°'].fillna(0).astype(int).astype(str)
    #! strå‹ã«å¤‰æ›
    zaiko_df['æ‹ ç‚¹æ‰€ç•ªåœ°'] = zaiko_df['æ‹ ç‚¹æ‰€ç•ªåœ°'].astype(int).astype(str)
    #! å—å…¥å ´æ‰€æƒ…å ±æº–å‚™
    file_path = 'temp/ãƒã‚¹ã‚¿ãƒ¼_å“ç•ª&ä»•å…¥å…ˆå&ä»•å…¥å…ˆå·¥å ´å.csv'
    syozaikyotenchi_data = pd.read_csv(file_path, encoding='shift_jis')
    #! ç©ºç™½æ–‡å­—åˆ—ã‚„éæ•°å€¤ãƒ‡ãƒ¼ã‚¿ã‚’NaNã«å¤‰æ›
    syozaikyotenchi_data['æ‹ ç‚¹æ‰€ç•ªåœ°'] = pd.to_numeric(syozaikyotenchi_data['æ‹ ç‚¹æ‰€ç•ªåœ°'], errors='coerce')
    #! strå‹ã«å¤‰æ›
    syozaikyotenchi_data['æ‹ ç‚¹æ‰€ç•ªåœ°'] = syozaikyotenchi_data['æ‹ ç‚¹æ‰€ç•ªåœ°'].fillna(0).astype(int).astype(str)
    #! å—å…¥å ´æ‰€è¿½åŠ 
    zaiko_df = pd.merge(zaiko_df, syozaikyotenchi_data[['å“ç•ª','æ‹ ç‚¹æ‰€ç•ªåœ°','å—å…¥å ´æ‰€']], on=['å“ç•ª', 'æ‹ ç‚¹æ‰€ç•ªåœ°'], how='left')
    #! æ—¥ä»˜åˆ—ã‚’ä½œæˆ
    zaiko_df['æ—¥ä»˜'] = zaiko_df['æ—¥æ™‚'].dt.date
    #! å“ç•ª_å—å…¥ç•ªå·ä½œæˆ
    zaiko_df['å“ç•ª_å—å…¥å ´æ‰€'] = zaiko_df['å“ç•ª'].astype(str) + "_" + zaiko_df['å—å…¥å ´æ‰€'].astype(str)
    # productåˆ—ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªå€¤ã‚’å–å¾—
    #unique_hinbans = zaiko_df['å“ç•ª_å—å…¥å ´æ‰€'].unique()
    #st.dataframe(zaiko_df.head(10000))

    # 24æ™‚é–“å¾Œ
    start_datetime = selected_datetime - timedelta(hours=6)
    end_datetime = selected_datetime + timedelta(days=1)
    #st.write(start_datetime,end_datetime)
    Timestamp_df = read_syozailt_by_using_archive_data(start_datetime.strftime('%Y-%m-%d-%H'), end_datetime.strftime('%Y-%m-%d-%H'))
    Timestamp_df['ä»•å…¥å…ˆå·¥å ´å'] = Timestamp_df['ä»•å…¥å…ˆå·¥å ´å'].apply(lambda x: '< NULL >' if pd.isna(x) else x)
    Timestamp_df = Timestamp_df.rename(columns={'ä»•å…¥å…ˆå·¥å ´å': 'ç™ºé€å ´æ‰€å'})# ã‚³ãƒ©ãƒ åå¤‰æ›´

    #! Activedataã®çµ±åˆ
    file_path = 'temp/activedata.csv'#ã‚¹ãƒ†ãƒƒãƒ—ï¼‘,2ã§ä½µç”¨ã—ã¦ã„ã‚‹ãŸã‚ã€å¤‰æ•°ã§ã¯ãªãä¸€æ™‚ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼ã«æ ¼ç´ã—ã¦ä½¿ç”¨
    Activedata = pd.read_csv(file_path, encoding='shift_jis')
    # æ—¥ä»˜åˆ—ã‚’datetimeå‹ã«å¤‰æ›
    Activedata['æ—¥ä»˜'] = pd.to_datetime(Activedata['æ—¥ä»˜'], errors='coerce')
    Activedata['å“ç•ª_å—å…¥å ´æ‰€'] = Activedata['å“ç•ª'].astype(str) + "_" + Activedata['å—å…¥å ´æ‰€'].astype(str)
    Activedata['æ—¥é‡ç®±æ•°'] = Activedata['æ—¥é‡æ•°']/Activedata['åå®¹æ•°']
    Activedata['å‡ºåº«äºˆå®šã‹ã‚“ã°ã‚“æ•°ï¼ˆtï¼‰'] = Activedata['æ—¥é‡ç®±æ•°']/kado_time
    # productåˆ—ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªå€¤ã‚’å–å¾—
    unique_hinbans = Activedata['å“ç•ª_å—å…¥å ´æ‰€'].unique()
    #st.dataframe(Activedata)

    # testç”¨
    #unique_hinbans = Activedata['å“ç•ª_å—å…¥å ´æ‰€'].unique()[:20]\
    
    # ç©ºã®ãƒªã‚¹ãƒˆã‚’ä½œæˆ
    hinban_list = []
    data_list = []

    #! ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªå“ç•ªã®çµ„ã¿åˆã‚ã›ã®æ•°ã ã‘å‡¦ç†ã‚’è¡Œã†
    for unique_hinban in unique_hinbans:

        # æœ€åˆã® _ ã§ 2 ã¤ã«åˆ†å‰²
        part_number, seibishitsu = unique_hinban.split("_", 1)
        #st.write(part_number, seibishitsu)

        # testç”¨
        #part_number = "9036340085"
        #seibishitsu = "1Y"

        #! ---------------------------åœ¨åº«ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™------------------------------
        #! å…¨ãƒ‡ãƒ¼ã‚¿ã€€â‡’ã€€å“ç•ªã€å—å…¥å ´æ‰€æŠ½å‡ºã€€â‡’ã€€selected_datetimeã®ã¿æŠ½å‡º
        #! ------------------------------------------------------------------------
        filtered_zaiko_df = zaiko_df[(zaiko_df['å“ç•ª'] == part_number) & (zaiko_df['å—å…¥å ´æ‰€'] == seibishitsu)]
        # æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹è¡Œã‚’å–å¾—
        filtered_zaiko_df = filtered_zaiko_df[filtered_zaiko_df["æ—¥æ™‚"] == selected_datetime]
        #! åœ¨åº«ãƒ‡ãƒ¼ã‚¿ãªã„ãªã‚‰ãã®å“ç•ªã¯ã‚¹ã‚­ãƒƒãƒ—
        if len(filtered_zaiko_df) == 0:
            continue
        # å®Ÿè¡Œçµæœã®ç¢ºèª
        #st.dataframe(filtered_zaiko_df)

        #! -----------------------------Activedataã®æº–å‚™----------------------------
        #! å…¨ãƒ‡ãƒ¼ã‚¿ â‡’ å“ç•ªã€æ•´å‚™å®¤æŠ½å‡º â‡’ æŒ‡å®šæœŸé–“æŠ½å‡º
        #! -------------------------------------------------------------------------
        #! åŒå“ç•ªã€åŒæ•´å‚™å®¤ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
        filtered_Activedata = Activedata[(Activedata['å“ç•ª'] == part_number) & (Activedata['æ•´å‚™å®¤'] == seibishitsu)]
        #st.dataframe(filtered_Activedata)
        # todoï¼ˆãƒ€ãƒ–ã‚Šæ¶ˆã™ã€è¨­è¨ˆå€¤é•ã†ãªã©ã§ãƒ€ãƒ–ã‚ŠãŒã‚ã‚‹ï¼‰
        before_rows = len(filtered_Activedata)# é©ç”¨å‰ã®è¡Œæ•°ã‚’è¨˜éŒ²
        filtered_Activedata = filtered_Activedata.drop_duplicates(subset=["æ—¥ä»˜"], keep="first")  # æœ€åˆã®è¡Œã‚’æ¡ç”¨
        after_rows = len(filtered_Activedata)# é©ç”¨å¾Œã®è¡Œæ•°ã‚’è¨˜éŒ²
        # ã‚‚ã—è¡Œæ•°ãŒå¤‰ã‚ã£ãŸã‚‰ã€å‰Šé™¤ãŒæ©Ÿèƒ½ã—ãŸã¨åˆ¤å®šã—ã¦ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‡ºåŠ›
        # if before_rows != after_rows:
        #     st.write(f"{part_number}, {seibishitsu}é‡è¤‡å‰Šé™¤ãŒé©ç”¨ã•ã‚Œã¾ã—ãŸ: {before_rows - after_rows} è¡ŒãŒå‰Šé™¤ã•ã‚Œã¾ã—ãŸã€‚")
        # todo
        #! 1æ™‚é–“ã”ã¨ã«å¤‰æ›
        filtered_Activedata = filtered_Activedata.set_index('æ—¥ä»˜').resample('H').ffill().reset_index()
        filtered_Activedata = filtered_Activedata.reset_index(drop=True)
        filtered_Activedata = filtered_Activedata.rename(columns={'æ—¥ä»˜': 'æ—¥æ™‚'})
        filtered_Activedata['æ—¥æ™‚'] = pd.to_datetime(filtered_Activedata['æ—¥æ™‚'])
        #st.dataframe(filtered_Activedata)
        #! æ˜¼å‹¤å¤œå‹¤ã®è€ƒæ…®é–¢æ•°
        def adjust_datetime(x):
            if 0 <= x.hour < 8:
                # æ—¥ä»˜ã‚’å‰æ—¥ã«å¤‰æ›´ã—ã€æ™‚é–“ã¯ãã®ã¾ã¾
                return x + pd.Timedelta(days=1)
            else:
                # ãã®ã¾ã¾ã®æ—¥ä»˜ã‚’è¿”ã™
                return x
        #! æ˜¼å‹¤å¤œå‹¤ã®è€ƒæ…®
        filtered_Activedata['æ—¥æ™‚'] = filtered_Activedata['æ—¥æ™‚'].apply(adjust_datetime)
        #! æŒ‡å®šæœŸé–“ã®ã¿æŠ½å‡º
        filtered_Activedata = filtered_Activedata[filtered_Activedata['æ—¥æ™‚'].isin(time_df['æ—¥æ™‚'])].copy()
        #! Activeãƒ‡ãƒ¼ã‚¿ãªã„ãªã‚‰ãã®å“ç•ªã¯ã‚¹ã‚­ãƒƒãƒ—
        if len(filtered_Activedata) == 0:
            continue
        #st.write(part_number, seibishitsu, len(filtered_Activedata))
        #st.dataframe(filtered_Activedata)

        #! ---------------------------Activeã¨åœ¨åº«ãƒ‡ãƒ¼ã‚¿ã®çµ±åˆ----------------------
        basedata = pd.merge(filtered_Activedata[['æ—¥æ™‚','å“ç•ª_å—å…¥å ´æ‰€','å“å','æ—¥é‡æ•°','åå®¹æ•°','è¨­è¨ˆå€¤MIN','è¨­è¨ˆå€¤MAX','æ—¥é‡ç®±æ•°','å‡ºåº«äºˆå®šã‹ã‚“ã°ã‚“æ•°ï¼ˆtï¼‰']], filtered_zaiko_df[['æ—¥æ™‚', 'å“ç•ª_å—å…¥å ´æ‰€', 'åœ¨åº«æ•°ï¼ˆç®±ï¼‰']], on=['å“ç•ª_å—å…¥å ´æ‰€', 'æ—¥æ™‚'], how='left')#! è‡ªå‹•ãƒ©ãƒƒã‚¯åœ¨åº«çµåˆ
        # å®Ÿè¡Œçµæœã®ç¢ºèª
        #st.dataframe(basedata)

        #! ---------------------------ç´å…¥äºˆå®šã‹ã‚“ã°ã‚“æ•°ã®è¨ˆç®—----------------------
        #! ç´å…¥äºˆå®šã‹ã‚“ã°ã‚“æ•°ï¼ˆtï¼‰ã®è¨ˆç®—é–¢æ•°
        def calculate_scheduled_nouyu_kanban(df, start_date, end_date):
            """
            æŒ‡å®šæœŸé–“å†…ã®ç´å…¥ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã—ã€ç´å…¥äºˆå®šæ—¥æ™‚ã”ã¨ã«é›†è¨ˆã™ã‚‹é–¢æ•°ã€‚

            Args:
                df (pd.DataFrame): ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã€‚
                start_date (str): æŠ½å‡ºé–‹å§‹æ—¥ï¼ˆä¾‹ï¼š'2024/3/5'ï¼‰ã€‚
                end_date (str): æŠ½å‡ºçµ‚äº†æ—¥ã€‚

            Returns:
                pd.DataFrame: ç´å…¥äºˆå®šæ—¥æ™‚ã”ã¨ã®é›†è¨ˆçµæœã‚’æ ¼ç´ã—ãŸãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã€‚
            """
            # æ—¥ä»˜ã‚’datetimeå½¢å¼ã«å¤‰æ›
            #todo æ—¥å˜ä½ã®ãŸã‚ã€00ã«ã—ãªã„ã¨ã€ãã®æ—¥ã®ç´å…¥æ—¥ãŒå…¥ã‚‰ãªã„
            start_date = datetime.strptime(start_date, '%Y-%m-%d-%H')
            start_date = start_date.replace(hour=0, minute=0, second=0)
            start_date = pd.to_datetime(start_date)
            end_date = pd.to_datetime(end_date) + pd.Timedelta(days=1)
            #st.write(start_date,end_date)

            # â‘  "ç´å…¥æ—¥"åˆ—ãŒæœŸé–“å†…ã«è©²å½“ã™ã‚‹è¡Œã‚’æŠ½å‡º
            filtered_df = df[(pd.to_datetime(df['ç´å…¥æ—¥']) >= start_date) & (pd.to_datetime(df['ç´å…¥æ—¥']) < end_date)]
            #st.dataframe(filtered_df)

            if len(filtered_df) != 0:

                #st.header("å®šåˆ»ä¾¿ç¢ºèª")
                #st.dataframe(filtered_df)

                # â‘¡ æŠ½å‡ºã—ãŸãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦å‡¦ç†
                # â‘¡-1 "ç´å…¥ä¾¿"åˆ—ã‹ã‚‰æ•°å€¤ã‚’å–å¾—
                filtered_df['B'] = filtered_df['ç´å…¥ä¾¿'].astype(int)

                # â‘¡-2 "Bä¾¿_å®šåˆ»"åˆ—ã®å€¤ã‚’å–å¾—ã—ã¦æ–°ã—ã„åˆ—"ç´å…¥äºˆå®šæ™‚é–“"ã«æ ¼ç´
                filtered_df['ç´å…¥äºˆå®šæ™‚é–“'] = filtered_df.apply(lambda row: row[f"{row['B']}ä¾¿_å®šåˆ»"] if f"{row['B']}ä¾¿_å®šåˆ»" in df.columns else None, axis=1)

                # â‘¡-3 "ç´å…¥äºˆå®šæ™‚é–“"åˆ—ãŒ0æ™‚ï½8æ™‚ã®å ´åˆã«"ç´å…¥æ—¥_è£œæ­£"åˆ—ã‚’1æ—¥å¾Œã«è¨­å®š
                filtered_df['ç´å…¥äºˆå®šæ™‚é–“'] = pd.to_datetime(filtered_df['ç´å…¥äºˆå®šæ™‚é–“'], format='%H:%M:%S', errors='coerce').dt.time
                #st.dataframe(filtered_df)
                #todo å¤œå‹¤ä¾¿ã¯+1ãŒå¿…è¦ï¼ï¼ï¼ï¼
                #todo ä»Šã®è¨ˆç®—ã§ã„ã„ã‹ä¸æ˜ï¼ï¼\
                filtered_df['ç´å…¥æ—¥_è£œæ­£'] = filtered_df.apply(lambda row: (pd.to_datetime(row['ç´å…¥æ—¥']) + pd.Timedelta(days=1)).strftime('%Y-%m-%d')
                                                            if row['ç´å…¥äºˆå®šæ™‚é–“'] and 0 <= row['ç´å…¥äºˆå®šæ™‚é–“'].hour < 6 else row['ç´å…¥æ—¥'], axis=1)

                # â‘¡-4 "ç´å…¥æ—¥_è£œæ­£"åˆ—ã¨"ç´å…¥äºˆå®šæ™‚é–“"åˆ—ã‚’çµ±åˆã—"ç´å…¥äºˆå®šæ—¥æ™‚"åˆ—ã‚’ä½œæˆ
                filtered_df['ç´å…¥äºˆå®šæ—¥æ™‚'] = pd.to_datetime(filtered_df['ç´å…¥æ—¥_è£œæ­£']) + pd.to_timedelta(filtered_df['ç´å…¥äºˆå®šæ™‚é–“'].astype(str))

                #st.write(len(filtered_df))

                # â‘¡-5 "ç´å…¥äºˆå®šæ—¥æ™‚"åˆ—ã§é›†è¨ˆã—ã€æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«æ ¼ç´
                nonyu_yotei_df = filtered_df.groupby('ç´å…¥äºˆå®šæ—¥æ™‚').agg(
                    ç´å…¥äºˆå®šã‹ã‚“ã°ã‚“æ•°=('ç´å…¥äºˆå®šæ—¥æ™‚', 'size'),
                    ç´å…¥äºˆå®šä¾¿ä¸€è¦§=('ç´å…¥ä¾¿', lambda x: list(x)),
                    ç´å…¥äºˆå®šã‹ã‚“ã°ã‚“ä¸€è¦§=('ã‹ã‚“ã°ã‚“ã‚·ãƒªã‚¢ãƒ«', lambda x: list(x)),
                    ç´å…¥äºˆå®šä¾¿=('ç´å…¥ä¾¿', lambda x: list(set(x))[0] if len(set(x)) == 1 else list(set(x)))  # ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªç´å…¥ä¾¿ã‚’ã‚¹ã‚«ãƒ©ãƒ¼ã«å¤‰æ›
                ).reset_index()
                
                nonyu_yotei_df['ç´å…¥äºˆå®šæ—¥æ™‚_raw'] = nonyu_yotei_df['ç´å…¥äºˆå®šæ—¥æ™‚']
                # "ç´å…¥äºˆå®šæ—¥æ™‚"åˆ—ã®åˆ†ä»¥é™ã‚’0ã«è¨­å®š
                nonyu_yotei_df['ç´å…¥äºˆå®šæ—¥æ™‚'] = nonyu_yotei_df['ç´å…¥äºˆå®šæ—¥æ™‚'].apply(lambda x: x.replace(minute=0, second=0) if pd.notna(x) else x)

                nonyu_yotei_df = nonyu_yotei_df.rename(columns={'ç´å…¥äºˆå®šã‹ã‚“ã°ã‚“æ•°': 'ç´å…¥äºˆå®šã‹ã‚“ã°ã‚“æ•°ï¼ˆtï¼‰'})# ã‚³ãƒ©ãƒ åå¤‰æ›´
                nonyu_yotei_df = nonyu_yotei_df.rename(columns={'ç´å…¥äºˆå®šæ—¥æ™‚': 'æ—¥æ™‚'})# ã‚³ãƒ©ãƒ åå¤‰æ›´

                #todo æ¤œåæ—¥æ™‚2æ™‚56åˆ†ã ã¨ã€2æ™‚ã«ãªã‚‹ãªã€‚
                kensyu_df = filtered_df.groupby('æ¤œåæ—¥æ™‚').agg(
                    æ¤œåã‹ã‚“ã°ã‚“æ•°=('æ¤œåæ—¥æ™‚', 'size'),
                    æ¤œåã‹ã‚“ã°ã‚“ä¸€è¦§=('ã‹ã‚“ã°ã‚“ã‚·ãƒªã‚¢ãƒ«', lambda x: list(x))
                ).reset_index()

                kensyu_df['æ¤œåæ—¥æ™‚_raw'] = kensyu_df['æ¤œåæ—¥æ™‚']
                kensyu_df['æ¤œåæ—¥æ™‚'] = kensyu_df['æ¤œåæ—¥æ™‚'].apply(lambda x: x.replace(minute=0, second=0) if pd.notna(x) else x)
                kensyu_df = kensyu_df.rename(columns={'æ¤œåæ—¥æ™‚': 'æ—¥æ™‚'})# ã‚³ãƒ©ãƒ åå¤‰æ›´

            else:
                nonyu_yotei_df = pd.DataFrame(columns=["æ—¥æ™‚", "ç´å…¥äºˆå®šã‹ã‚“ã°ã‚“æ•°ï¼ˆtï¼‰", "ç´å…¥äºˆå®šä¾¿ä¸€è¦§",
                                                       "ç´å…¥äºˆå®šã‹ã‚“ã°ã‚“ä¸€è¦§","ç´å…¥äºˆå®šä¾¿","ç´å…¥äºˆå®šæ—¥æ™‚_raw"])
                kensyu_df = pd.DataFrame(columns=["æ—¥æ™‚", "æ¤œåæ—¥æ™‚_raw", "æ¤œåã‹ã‚“ã°ã‚“æ•°",
                                                       "æ¤œåã‹ã‚“ã°ã‚“ä¸€è¦§"])

            return nonyu_yotei_df, kensyu_df
        
        #! æ‰€åœ¨ç®¡ç†MBã®ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ‡ãƒ¼ã‚¿
        #! åŒå“ç•ªã€åŒæ•´å‚™å®¤ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
        filtered_Timestamp_df = Timestamp_df[(Timestamp_df['å“ç•ª'] == part_number) & (Timestamp_df['æ•´å‚™å®¤ã‚³ãƒ¼ãƒ‰'] == seibishitsu)]
        #! ä»•å…¥å…ˆåã€ä»•å…¥å…ˆå·¥å ´åæŠ½å‡º
        unique_shiresaki = filtered_Timestamp_df['ä»•å…¥å…ˆå'].unique()[0]
        unique_shiresaki_kojo = filtered_Timestamp_df['ç™ºé€å ´æ‰€å'].unique()[0]
        #st.write(unique_shiresaki,unique_shiresaki_kojo)
        #st.dataframe(filtered_Timestamp_df)
        #! ä»•å…¥å…ˆä¾¿æƒ…å ±æŠ½å‡º
        arrival_times_df = calculate_supplier_truck_arrival_types2()
        #! ä¸€è‡´ã™ã‚‹ä»•å…¥ã‚Œå…ˆãƒ•ãƒ©ã‚°ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€ã‚¨ãƒ©ãƒ¼ã‚’å‡ºã™
        #! 3ã¤ã®åˆ—ï¼ˆä»•å…¥å…ˆåã€ç™ºé€å ´æ‰€åã€æ•´å‚™å®¤ã‚³ãƒ¼ãƒ‰ï¼‰ã§æ¡ä»¶ã‚’æº€ãŸã™è¡Œã‚’arrival_times_dfã‹ã‚‰æŠ½å‡ºã—ã€æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ matched_arrival_times_dfã‚’ä½œæˆ
        # æ¡ä»¶ã¯ã€lagged_featuresã¨åŒã˜ä»•å…¥å…ˆåã€ç™ºé€å ´æ‰€åã€æ•´å‚™å®¤ã‚³ãƒ¼ãƒ‰ã‚’æŒã¤ã‚‚ã®
        matched_arrival_times_df = arrival_times_df[
            (arrival_times_df['ä»•å…¥å…ˆå'].isin([unique_shiresaki])) &
            (arrival_times_df['ç™ºé€å ´æ‰€å'].isin([unique_shiresaki_kojo])) &
            (arrival_times_df['å—å…¥'].isin([seibishitsu]))
        ]
        matched_arrival_times_df = matched_arrival_times_df.rename(columns={'å—å…¥': 'æ•´å‚™å®¤ã‚³ãƒ¼ãƒ‰'})# ã‚³ãƒ©ãƒ åå¤‰æ›´
        #st.dataframe(matched_arrival_times_df)
        # çµ±åˆã™ã‚‹åˆ—ã®é¸åˆ¥
        columns_to_extract_t = ['ã‹ã‚“ã°ã‚“ã‚·ãƒªã‚¢ãƒ«','å“å','ç´å…¥æ—¥', 'ç´å…¥ä¾¿','æ¤œåæ—¥æ™‚','ä»•å…¥å…ˆå', 'ç™ºé€å ´æ‰€å', 'æ•´å‚™å®¤ã‚³ãƒ¼ãƒ‰']
        columns_to_extract_l = matched_arrival_times_df.filter(regex='ä¾¿_å®šåˆ»').columns.tolist() + ['ä»•å…¥å…ˆå', 'ç™ºé€å ´æ‰€å', 'æ•´å‚™å®¤ã‚³ãƒ¼ãƒ‰','ç´å…¥å…ˆ']
        # çµ±åˆ
        filtered_Timestamp_df = pd.merge(filtered_Timestamp_df[columns_to_extract_t], matched_arrival_times_df[columns_to_extract_l], on=['ä»•å…¥å…ˆå', 'ç™ºé€å ´æ‰€å', 'æ•´å‚™å®¤ã‚³ãƒ¼ãƒ‰'], how='left')
        #st.dataframe(filtered_Timestamp_df)
        #! ç´å…¥ã‚¿ã‚¤ãƒ—æŠ½å‡º
        unique_nonyu_type = filtered_Timestamp_df['ç´å…¥å…ˆ'].unique()[0]
        #st.write(unique_nonyu_type)
        #! ç´å…¥äºˆå®šã‹ã‚“ã°ã‚“æ•°ï¼ˆtï¼‰ã®è¨ˆç®—
        #st.write(start_datetime,end_datetime)
        nonyu_yotei_df, kensyu_df = calculate_scheduled_nouyu_kanban(filtered_Timestamp_df, start_datetime.strftime('%Y-%m-%d-%H'), end_datetime.strftime('%Y-%m-%d-%H'))
        #st.dataframe(nonyu_yotei_df)
        #st.write(part_number, seibishitsu, nonyu_yotei_df["ç´å…¥äºˆå®šã‹ã‚“ã°ã‚“æ•°ï¼ˆtï¼‰"].mean(), filtered_Activedata['æ—¥é‡ç®±æ•°'].mean()/filtered_Activedata['ã‚µã‚¤ã‚¯ãƒ«å›æ•°'].mean(), before_rows - after_rows)

        # 1æ™‚é–“ã”ã¨ã®æ™‚é–“åˆ—ï¼ˆ24æ™‚é–“åˆ†ï¼‰ã‚’ä½œæˆ
        time_series = pd.date_range(start=start_datetime, periods=24+5, freq="H")
        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä½œæˆ
        nonyu_data_df = pd.DataFrame({"æ—¥æ™‚": time_series})
        #st.dataframe(nonyu_data_df)

        #! æ—¥æ™‚ã§ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’çµåˆ
        nonyu_data_df = pd.merge(nonyu_data_df, nonyu_yotei_df, on='æ—¥æ™‚', how='left')
        nonyu_data_df = pd.merge(nonyu_data_df, kensyu_df, on='æ—¥æ™‚', how='left')
        #! ã™ã¹ã¦ã®Noneå€¤ã‚’0ã«ç½®ãæ›ãˆ
        # basedataã«çµ±åˆã™ã‚‹éš›ã€nonyu_yotei_dfã«å­˜åœ¨ã—ãªã„æ—¥æ™‚ã¯Noneã«ãªã‚‹ãŸã‚
        nonyu_data_df = nonyu_data_df.fillna(0)
        #st.dataframe(nonyu_data_df)

        if unique_nonyu_type == "è¥¿å°¾æ±":
            nonyu_lt = 5
        else:
            nonyu_lt = 1

        # nonyu_ltæ™‚é–“å¾Œã«ã‚·ãƒ•ãƒˆ
        # æ˜‡é †ã«ä¸¦ã³æ›¿ãˆ
        nonyu_data_df = nonyu_data_df.sort_values(by="æ—¥æ™‚")
        nonyu_data_df["å…¥åº«äºˆå®šã‹ã‚“ã°ã‚“æ•°ï¼ˆtï¼‰"] = nonyu_data_df["ç´å…¥äºˆå®šã‹ã‚“ã°ã‚“æ•°ï¼ˆtï¼‰"].shift(nonyu_lt)
        #st.dataframe(nonyu_data_df)

        #! ---------------------------------------ç´å…¥äºˆå®šã‹ã‚“ã°ã‚“æ•°ãƒ‡ãƒ¼ã‚¿ã®çµ±åˆ--------------------------------------------------------
        #! æ—¥æ™‚ã§ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’çµåˆ
        basedata = basedata.sort_values(by="æ—¥æ™‚")
        basedata = basedata.fillna(0)
        basedata = pd.merge(basedata, nonyu_data_df, on='æ—¥æ™‚', how='left')
        #st.dataframe(basedata)

        #! åœ¨åº«ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        # åœ¨åº«æ•°ã‚’è¨ˆç®—ï¼ˆç´¯ç©è¨ˆç®—ï¼‰
        basedata["åœ¨åº«æ•°ï¼ˆç®±ï¼‰_äºˆæ¸¬å€¤"]=basedata["åœ¨åº«æ•°ï¼ˆç®±ï¼‰"]
        for i in range(1, len(basedata)):
            basedata.loc[i, "åœ¨åº«æ•°ï¼ˆç®±ï¼‰_äºˆæ¸¬å€¤"] = (
                basedata.loc[i - 1, "åœ¨åº«æ•°ï¼ˆç®±ï¼‰_äºˆæ¸¬å€¤"]  # 1ã¤ä¸Šã®è¡Œã®åœ¨åº«æ•°
                + basedata.loc[i, "å…¥åº«äºˆå®šã‹ã‚“ã°ã‚“æ•°ï¼ˆtï¼‰"]  # ç´å…¥åˆ†ã‚’åŠ ç®—
                - basedata.loc[i, "å‡ºåº«äºˆå®šã‹ã‚“ã°ã‚“æ•°ï¼ˆtï¼‰"]  # å‡ºåº«åˆ†ã‚’æ¸›ç®—
            )
        #st.dataframe(basedata)

        #! åˆ¤å®š
        basedata["ä¸‹é™å‰²ã‚Œ"] = (basedata["åœ¨åº«æ•°ï¼ˆç®±ï¼‰_äºˆæ¸¬å€¤"] < basedata["è¨­è¨ˆå€¤MIN"]).astype(int)
        basedata["ä¸Šé™è¶Šãˆ"] = (basedata["åœ¨åº«æ•°ï¼ˆç®±ï¼‰_äºˆæ¸¬å€¤"] > basedata["è¨­è¨ˆå€¤MAX"]).astype(int)
        basedata["åœ¨åº«0"] = (basedata["åœ¨åº«æ•°ï¼ˆç®±ï¼‰_äºˆæ¸¬å€¤"] < 0).astype(int)

        #! å„é …ç›®ã®åˆè¨ˆã‚’è¨ˆç®—
        total_lower_limit = basedata["ä¸‹é™å‰²ã‚Œ"].sum()
        total_upper_exceed = basedata["ä¸Šé™è¶Šãˆ"].sum()
        total_stock_zero = basedata["åœ¨åº«0"].sum()
        # æ¡ä»¶åˆ†å²ã§OK/NGã«å¤‰æ›
        total_lower_limit = "NG" if total_lower_limit > 0 else "OK"
        total_upper_exceed = "NG" if total_upper_exceed > 0 else "OK"
        total_stock_zero = "NG" if total_stock_zero > 0 else "OK"

        #st.dataframe(basedata)

        # ---- å¿…è¦ãªåˆ—ã‚’æŠ½å‡º ----
        basedata_filtered = basedata[["æ—¥æ™‚", "åœ¨åº«æ•°ï¼ˆç®±ï¼‰_äºˆæ¸¬å€¤", "è¨­è¨ˆå€¤MIN", "è¨­è¨ˆå€¤MAX"]]

        # Matplotlibã§ãƒ—ãƒ­ãƒƒãƒˆä½œæˆ
        fig, ax = plt.subplots(figsize=(10, 6))
        ax.plot(basedata_filtered["æ—¥æ™‚"], basedata_filtered["åœ¨åº«æ•°ï¼ˆç®±ï¼‰_äºˆæ¸¬å€¤"], label="åœ¨åº«æ•°ï¼ˆç®±ï¼‰_äºˆæ¸¬å€¤", marker="o")
        ax.fill_between(basedata_filtered["æ—¥æ™‚"], basedata_filtered["è¨­è¨ˆå€¤MIN"], basedata_filtered["è¨­è¨ˆå€¤MAX"], 
                        color="lightgray", alpha=0.5, label="è¨­è¨ˆå€¤ç¯„å›² (MIN-MAX)")
        ax.axhline(y=basedata_filtered["è¨­è¨ˆå€¤MIN"].iloc[0], color="blue", linestyle="--", label="è¨­è¨ˆå€¤MIN")
        ax.axhline(y=basedata_filtered["è¨­è¨ˆå€¤MAX"].iloc[0], color="red", linestyle="--", label="è¨­è¨ˆå€¤MAX")

        # ---- ã‚°ãƒ©ãƒ•ã®è£…é£¾ ----
        ax.set_title("åœ¨åº«æ•°ã¨è¨­è¨ˆå€¤ã®æ¯”è¼ƒ", fontsize=14)
        ax.set_xlabel("æ—¥æ™‚", fontsize=12)
        ax.set_ylabel("åœ¨åº«æ•°", fontsize=12)
        ax.legend()
        ax.grid(True)
        plt.xticks(rotation=45)

        # ç”»é¢ã«è¡¨ç¤º
        #st.pyplot(fig)

        # ---- PNGãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ ----
        save_dir = "temp/åœ¨åº«ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"
        os.makedirs(save_dir, exist_ok=True)
        output_file = f"{save_dir}/{unique_hinban}.png"
        fig.savefig(output_file, format="png", dpi=300, bbox_inches="tight")

        #! å¿…è¦ãƒ‡ãƒ¼ã‚¿ã ã‘æº–å‚™
        hinban_list.append(output_file)
        unique_hinmei = filtered_Timestamp_df['å“å'].unique()[0]
        data_list.append({"å“ç•ª_æ•´å‚™å®¤": unique_hinban, "å“å": unique_hinmei,
                           "ä»•å…¥å…ˆå": unique_shiresaki, "ç™ºé€å·¥å ´å": unique_shiresaki_kojo,
                           "ä¸‹é™å‰²ã‚Œ":total_lower_limit,"ä¸Šé™è¶Šãˆ":total_upper_exceed,"åœ¨åº«0":total_stock_zero})

    # ãƒ­ãƒ¼ã‚«ãƒ«ã® PNG ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ Base64 ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã™ã‚‹é–¢æ•°
    def img_to_base64(file_path):
        with open(file_path, "rb") as f:
            data = f.read()
        # Base64 ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã¦æ–‡å­—åˆ—ã«å¤‰æ›
        return base64.b64encode(data).decode("utf-8")

    # DataFrame ã«å¤‰æ›
    df_A = pd.DataFrame(data_list)
    # ç”»åƒã‚’ Base64 å¤‰æ›
    base64_images = [img_to_base64(p) for p in hinban_list]
    # DataFrame ã«å¤‰æ›
    df_B = pd.DataFrame(base64_images, columns=["ç”»åƒbase64"])

    # DataFrame ã‚’çµ±åˆï¼ˆæ¨ªæ–¹å‘ã«çµåˆï¼‰
    data = pd.concat([df_A, df_B], axis=1)

    df = pd.DataFrame(data)

    #st.dataframe(df)

    # ---- HTMLã‚’çµ„ã¿ç«‹ã¦ã‚‹ ----
    html_code = """
    <!DOCTYPE html>
    <html>
    <head>
        <meta charset="utf-8">
        <style>
            table {
                border-collapse: collapse;
                width: 100%;
                margin-bottom: 1em;
            }
            th, td {
                border: 1px solid #ccc;
                padding: 8px;
                text-align: center;
            }
            th {
                background-color: #f7f7f7;
            }
            /* æŠ˜ã‚ŠãŸãŸã¾ã‚Œã¦ã„ã‚‹è¦ç´ ã‚’éè¡¨ç¤º */
            .hidden-content {
                display: none;
            }
            .toggle-button {
                padding: 6px 12px;
                background-color: #008CBA;
                color: white;
                border: none;
                cursor: pointer;
                border-radius: 4px;
            }
            .toggle-button:hover {
                background-color: #006F9A;
            }
        </style>
    </head>
    <body>

    <script>
        // ãƒœã‚¿ãƒ³æŠ¼ä¸‹æ™‚ã«è¡¨ç¤ºãƒ»éè¡¨ç¤ºã‚’åˆ‡ã‚Šæ›¿ãˆã‚‹é–¢æ•°
        function toggleImage(id) {
            var elem = document.getElementById(id);
            if (elem.style.display === 'none' || elem.style.display === '') {
                elem.style.display = 'block';
            } else {
                elem.style.display = 'none';
            }
        }
    </script>

    <table>
        <thead>
            <tr>
                <th>å“ç•ª_æ•´å‚™å®¤</th>
                <th>å“å</th>
                <th>ä»•å…¥å…ˆå</th>
                <th>ä»•å…¥å…ˆå·¥å ´å</th>
                <th>ä¸‹é™å‰²ã‚Œ</th>
                <th>åœ¨åº«0</th>
                <th>ä¸Šé™è¶Šãˆ</th>
                <th>ã‚°ãƒ©ãƒ•</th>
            </tr>
        </thead>
        <tbody>
    """

    # DataFrameã®å„è¡Œã‚’ãƒ«ãƒ¼ãƒ—ã—ã¦ãƒ†ãƒ¼ãƒ–ãƒ«HTMLä½œæˆ
    for i, row in df.iterrows():

        name = row["å“ç•ª_æ•´å‚™å®¤"]
        price = row["å“å"]
        stock = row["ä»•å…¥å…ˆå"]
        stock1 = row["ç™ºé€å·¥å ´å"]
        stock2 = row["ä¸‹é™å‰²ã‚Œ"]
        stock3 = row["åœ¨åº«0"]
        stock4 = row["ä¸Šé™è¶Šãˆ"]
        img_b64 = row["ç”»åƒbase64"]
        
        # PNGã®å ´åˆ => data:image/png;base64, ...
        data_url = f"data:image/png;base64,{img_b64}"
        
        html_code += f"""
        <tr>
        <td>{name}</td>
        <td>{price}</td>
        <td>{stock}</td>
        <td>{stock1}</td>
        <td>{stock2}</td>
        <td>{stock3}</td>
        <td>{stock4}</td>
        <td>
            <!-- onclick ã§ toggleImage() ã‚’å‘¼ã³å‡ºã—ã€IDæŒ‡å®šã§è¦ç´ ã‚’è¡¨ç¤º/éè¡¨ç¤º -->
            <button class="toggle-button" onclick="toggleImage('hidden-content-{i}')">è¡¨ç¤º</button>
            <!-- æœ€åˆã¯ hidden-content ã‚¯ãƒ©ã‚¹ã§éè¡¨ç¤ºçŠ¶æ…‹ -->
            <div id="hidden-content-{i}" class="hidden-content">
                <img src="{data_url}" style="max-width: 200px; margin-top: 8px;">
            </div>
        </td>
        </tr>
        """

    html_code += """
        </tbody>
    </table>
    </body>
    </html>
    """

    # Streamlit ã§ HTML ã‚’æç”» (é«˜ã•ã‚„ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã¯å¿…è¦ã«å¿œã˜ã¦å¤‰æ›´)
    components.html(html_code, height=600, scrolling=True)


#! åœ¨åº«äºˆæ¸¬
def show_forecast2( unique_product, start_datetime, selected_zaiko):

    start_date = '2024-05-01-00'
    end_date = '2024-08-31-00'

    #! å“ç•ªã€æ•´å‚™å®¤ã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡º
    product = unique_product.split('_')[0]
    seibishitsu = unique_product.split('_')[1]

    #! ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
    prediction_hours = 24#ä½•æ™‚é–“å…ˆã¾ã§äºˆæ¸¬ã™ã‚‹ã®ã‹
    past_hours = 5
    lookback_hours = past_hours+2

    # ã‚¿ã‚¤ãƒˆãƒ«è¡¨ç¤º
    st.header('äºˆæ¸¬çµæœ')

    #!----------------------------------------------------------------------- 
    #! è‡ªå‹•ãƒ©ãƒƒã‚¯ã®åœ¨åº«ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨å‡¦ç†
    #!-----------------------------------------------------------------------
    zaiko_df = read_zaiko_by_using_archive_data(start_date, end_date)
    # å“ç•ªåˆ—ã®ç©ºç™½ã‚’å‰Šé™¤
    zaiko_df['å“ç•ª'] = zaiko_df['å“ç•ª'].str.strip()
    # 'è¨ˆæ¸¬æ—¥æ™‚'ã‚’datetimeå‹ã«å¤‰æ›
    #zaiko_df['è¨ˆæ¸¬æ—¥æ™‚'] = pd.to_datetime(zaiko_df['è¨ˆæ¸¬æ—¥æ™‚'], errors='coerce')
    # åˆ—å 'è¨ˆæ¸¬æ—¥æ™‚' ã‚’ 'æ—¥æ™‚' ã«å¤‰æ›´
    #zaiko_df = zaiko_df.rename(columns={'è¨ˆæ¸¬æ—¥æ™‚': 'æ—¥æ™‚'})
    # ç‰¹å®šã®å“ç•ªã®å•†å“ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
    zaiko_df = zaiko_df[zaiko_df['å“ç•ª'] == product]
    # ç‰¹å®šã®æ—¥æ™‚ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
    zaiko_df = zaiko_df[zaiko_df['æ—¥æ™‚'] == start_datetime]
    # æ—¥æ™‚ã‚’å†åº¦datetimeå‹ã«å¤‰æ›ï¼ˆå¿µã®ãŸã‚ï¼‰
    zaiko_df['æ—¥æ™‚'] = pd.to_datetime(zaiko_df['æ—¥æ™‚'])
    # 'æ—¥æ™‚' ã¨ 'åœ¨åº«æ•°ï¼ˆç®±ï¼‰' ã®åˆ—ã®ã¿ã‚’æŠ½å‡º
    zaiko_extracted = zaiko_df[['æ—¥æ™‚', 'åœ¨åº«æ•°ï¼ˆç®±ï¼‰']]

    #!-----------------------------------------------------------------------
    #! æ‰€åœ¨ç®¡ç†ãƒªãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ ã®ãƒ‡ãƒ¼ã‚¿
    #!-----------------------------------------------------------------------
    #file_path = 'ä¸­é–“æˆæœç‰©/æ‰€åœ¨ç®¡ç†MBãƒ†ã‚™ãƒ¼ã‚¿_çµ±åˆæ¸ˆ&ç‰¹å®šæ—¥æ™‚æŠ½å‡ºæ¸ˆ.csv'
    Timestamp_df = read_syozailt_by_using_archive_data(start_date, end_date)
    # 'æ›´æ–°æ—¥æ™‚'åˆ—ã«ç„¡åŠ¹ãªæ—¥æ™‚ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹è¡Œã‚’å‰Šé™¤ã™ã‚‹
    data_cleaned = Timestamp_df.dropna(subset=['æ¤œåæ—¥æ™‚'])
    st.dataframe(data_cleaned.head(50000))
    # ç‰¹å®šã®å“ç•ªã®å•†å“ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
    data_cleaned = data_cleaned[(data_cleaned['å“ç•ª'] == product) & (data_cleaned['æ•´å‚™å®¤ã‚³ãƒ¼ãƒ‰'] == seibishitsu)]
    # æ™‚é–“ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã€å„æ™‚é–“ã§ã®ã‹ã‚“ã°ã‚“æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆã™ã‚‹
    data_cleaned['æ—¥æ™‚'] = data_cleaned['æ¤œåæ—¥æ™‚'].dt.floor('H')  # æ™‚é–“å˜ä½ã«ä¸¸ã‚ã‚‹
    hourly_kanban_count = data_cleaned.groupby('æ—¥æ™‚').size().reset_index(name='ç´å…¥äºˆå®šã‹ã‚“ã°ã‚“æ•°')
    #st.dataframe(hourly_kanban_count)

    # æ™‚é–“ã®ç¯„å›²ã‚’æ±ºå®šã—ã€æ¬ ææ™‚é–“å¸¯ã‚’è£œå®Œã™ã‚‹
    full_time_range = pd.date_range(start=hourly_kanban_count['æ—¥æ™‚'].min(),end=hourly_kanban_count['æ—¥æ™‚'].max(),freq='H')

    # å…¨ã¦ã®æ™‚é–“ã‚’å«ã‚€ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä½œæˆã—ã€æ¬ æå€¤ã‚’0ã§åŸ‹ã‚ã‚‹
    hourly_kanban_count_full = pd.DataFrame(full_time_range, columns=['æ—¥æ™‚']).merge(hourly_kanban_count, on='æ—¥æ™‚', how='left').fillna(0)

    # ã‹ã‚“ã°ã‚“æ•°ã‚’æ•´æ•°ã«æˆ»ã™
    hourly_kanban_count_full['ç´å…¥äºˆå®šã‹ã‚“ã°ã‚“æ•°'] = hourly_kanban_count_full['ç´å…¥äºˆå®šã‹ã‚“ã°ã‚“æ•°'].astype(int)

    # 'äºˆæ¸¬å…¥åº«æ™‚é–“'åˆ—ã¨ã—ã¦ã€5æ™‚é–“å‰ã®ã‹ã‚“ã°ã‚“æ•°ã‚’è¿½åŠ ã™ã‚‹
    hourly_kanban_count_full['å·¥å ´åˆ°ç€äºˆå®šã‹ã‚“ã°ã‚“æ•°'] = hourly_kanban_count_full['ç´å…¥äºˆå®šã‹ã‚“ã°ã‚“æ•°'].shift(past_hours)

    # æ¬ æå€¤ï¼ˆæœ€åˆã®5æ™‚é–“åˆ†ï¼‰ã‚’0ã§åŸ‹ã‚ã‚‹
    hourly_kanban_count_full['å·¥å ´åˆ°ç€äºˆå®šã‹ã‚“ã°ã‚“æ•°'] = hourly_kanban_count_full['å·¥å ´åˆ°ç€äºˆå®šã‹ã‚“ã°ã‚“æ•°'].fillna(0).astype(int)

    #!-----------------------------------------------------------------------
    #! Activedataã®å‡¦ç†
    #!-----------------------------------------------------------------------
    activedata = read_activedata_by_using_archive_data(start_date, end_date, 0)
    # ç‰¹å®šã®å“ç•ªã®å•†å“ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
    activedata = activedata[activedata['å“ç•ª'] == product]
    #st.dataframe(activedata)
    #! ç¨¼åƒæ™‚é–“ã§å‰²ã‚‹å‡¦ç† (ä¼‘æ†©æ™‚é–“ã®è€ƒæ…®ãŒå¿…è¦ã‹ï¼Ÿ)
    activedata['æ—¥é‡æ•°ï¼ˆç®±æ•°ï¼‰'] = activedata['æ—¥é‡æ•°']/activedata['åå®¹æ•°']
    activedata['æ—¥é‡æ•°ï¼ˆç®±æ•°ï¼‰/ç¨¼åƒæ™‚é–“'] = activedata['æ—¥é‡æ•°ï¼ˆç®±æ•°ï¼‰'] / 16.5
    activedata['æ—¥ä»˜'] = pd.to_datetime(activedata['æ—¥ä»˜'])#ã“ã‚Œã—ãªã„ã¨æ¬¡ã®.resample('H')ã§ã‚¨ãƒ©ãƒ¼ãŒå‡ºã‚‹
    # æ—¥ä»˜ã‚’åŸºæº–ã«1æ™‚é–“ã”ã¨ã®ãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›
    activedata = activedata.set_index('æ—¥ä»˜').resample('H').ffill().reset_index()
    # 'æ—¥ä»˜' ã‚’datetimeå‹ã«å¤‰æ›
    activedata['æ—¥ä»˜'] = pd.to_datetime(activedata['æ—¥ä»˜'])
    activedata = activedata.rename(columns={'æ—¥ä»˜': 'æ—¥æ™‚'})
    # 'æ—¥ä»˜' ã¨ 'æ—¥é‡æ•°ï¼ˆç®±æ•°ï¼‰' ã®åˆ—ã®ã¿ã‚’æŠ½å‡º
    activedata_extracted = activedata[['æ—¥æ™‚', 'æ—¥é‡æ•°ï¼ˆç®±æ•°ï¼‰/ç¨¼åƒæ™‚é–“']]

    # åœ¨åº«ãƒ‡ãƒ¼ã‚¿ã®é–‹å§‹æ™‚åˆ»ã‚’å–å¾—
    start_time = zaiko_extracted.iloc[0]['æ—¥æ™‚']
    # é–‹å§‹æ™‚åˆ»ã‹ã‚‰20æ™‚é–“å¾Œã¾ã§ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
    end_time = start_time + pd.Timedelta(hours=prediction_hours)
    filtered_activedata = activedata_extracted[(activedata_extracted['æ—¥æ™‚'] >= start_time) & (activedata_extracted['æ—¥æ™‚'] < end_time)]

    # å„æ™‚é–“å¾Œã®æ¶ˆè²»é‡ãŠã‚ˆã³å…¥åº«é‡ã‚’è€ƒæ…®ã—ãŸåœ¨åº«æ•°ã‚’è¨ˆç®—
    inventory_after_adjustments = []
    # ç¾åœ¨ã®åœ¨åº«æ•°ã‚’åˆæœŸå€¤ã¨ã—ã¦è¨­å®š
    current_inventory = zaiko_extracted.iloc[0]['åœ¨åº«æ•°ï¼ˆç®±ï¼‰']

    # 3ã¤ã®åˆ—ã‚’ä½œæˆ
    col1, col2 = st.columns(2)
    col1.metric(label="é¸æŠã•ã‚ŒãŸæ—¥æ™‚", value=str(start_datetime))#, delta="1 mph")
    col2.metric(label="å…¥åŠ›ã•ã‚ŒãŸçµ„ç«‹ãƒ©ã‚¤ãƒ³ã®åœ¨åº«æ•°ï¼ˆç®±ï¼‰", value=int(current_inventory))

    # æ™‚é–“ã”ã¨ã®åœ¨åº«æ•°ã‚’æ›´æ–°ã—ãªãŒã‚‰ãƒªã‚¹ãƒˆã«è¿½åŠ 
    for i, row in filtered_activedata.iterrows():
        kanban_row = hourly_kanban_count_full[hourly_kanban_count_full['æ—¥æ™‚'] == row['æ—¥æ™‚']]
        incoming_kanban = kanban_row['å·¥å ´åˆ°ç€äºˆå®šã‹ã‚“ã°ã‚“æ•°'].values[0] if not kanban_row.empty else 0
        inventory_after_adjustments.append({
            'æ—¥æ™‚': row['æ—¥æ™‚'],
            'åœ¨åº«æ•°ï¼ˆç®±ï¼‰': current_inventory
        })
        # æœ€åˆã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã§ã¯æ¶ˆè²»ã‚’å¼•ã‹ãªã„ãŒã€ä»¥é™ã¯æ¶ˆè²»é‡ã¨å…¥åº«é‡ã‚’èª¿æ•´
        if i != 0:
            current_inventory = current_inventory - row['æ—¥é‡æ•°ï¼ˆç®±æ•°ï¼‰/ç¨¼åƒæ™‚é–“']  # æ¶ˆè²»é‡ã‚’å¼•ã
            current_inventory = current_inventory + incoming_kanban  # å…¥åº«é‡ã‚’è¶³ã™
            

    # è¨ˆç®—çµæœã‚’DataFrameã«å¤‰æ›
    inventory_df_adjusted = pd.DataFrame(inventory_after_adjustments)

    # æœ€åˆã®æ™‚é–“ã®ãƒ‡ãƒ¼ã‚¿ï¼ˆå®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ï¼‰ã¨ãã‚Œä»¥é™ã®äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ã«åˆ†å‰²
    actual_data = inventory_df_adjusted.iloc[0:1]  # æœ€åˆã®1æ™‚é–“åˆ†ã¯å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿
    forecast_data = inventory_df_adjusted.iloc[1:]  # ãã‚Œä»¥é™ã¯äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿

    # æ™‚é–“è»¸ã‚’çµ±ä¸€ã™ã‚‹ãŸã‚ã€å…¨æ™‚é–“ã®ç¯„å›²ã‚’ä½œæˆ
    #full_time_range = pd.date_range(start=actual_data['æ—¥æ™‚'].min(), end=forecast_data['æ—¥æ™‚'].max(), freq='H')

    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ãã‚Œãã‚Œã“ã®æ™‚é–“è»¸ã«åˆã‚ã›ã¦å†æ§‹ç¯‰ã—ã€æ¬ æå€¤ã‚’åŸ‹ã‚ã‚‹
    #actual_data = actual_data.set_index('æ—¥æ™‚').reindex(full_time_range).reset_index().rename(columns={'index': 'æ—¥æ™‚'})
    #forecast_data = forecast_data.set_index('æ—¥æ™‚').reindex(full_time_range).reset_index().rename(columns={'index': 'æ—¥æ™‚'})

    # æ¬ æå€¤ã¯ãã‚Œãã‚Œ0ã«ç½®ãæ›ãˆã‚‹ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
    #actual_data['åœ¨åº«æ•°ï¼ˆç®±ï¼‰'].fillna(0, inplace=True)
    #forecast_data['åœ¨åº«æ•°ï¼ˆç®±ï¼‰'].fillna(0, inplace=True)

    # ã‚°ãƒ©ãƒ•ã®ä½œæˆ
    fig = go.Figure()

    # å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã‚’é’è‰²ã§æç”»
    fig.add_trace(go.Bar(
        x=actual_data['æ—¥æ™‚'], 
        y=actual_data['åœ¨åº«æ•°ï¼ˆç®±ï¼‰'], 
        name='å®Ÿç¸¾', 
        marker_color='blue', 
        opacity=0.3
    ))

    # äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ã‚’ã‚ªãƒ¬ãƒ³ã‚¸è‰²ã§è¿½åŠ æç”»
    fig.add_trace(go.Bar(
        x=forecast_data['æ—¥æ™‚'], 
        y=forecast_data['åœ¨åº«æ•°ï¼ˆç®±ï¼‰'], 
        name='äºˆæ¸¬', 
        marker_color='orange', 
        opacity=0.3
    ))

    # xè»¸ã‚’1æ™‚é–“ã”ã¨ã«è¡¨ç¤ºã™ã‚‹è¨­å®š
    fig.update_layout(
        title='äºˆæ¸¬çµæœ',  # ã“ã“ã§ã‚¿ã‚¤ãƒˆãƒ«ã‚’è¨­å®š
        xaxis_title='æ—¥æ™‚',  # xè»¸ã‚¿ã‚¤ãƒˆãƒ«
        yaxis_title='åœ¨åº«æ•°ï¼ˆç®±ï¼‰',  # yè»¸ã‚¿ã‚¤ãƒˆãƒ«
        xaxis=dict(
            tickformat="%Y-%m-%d %H:%M",  # æ—¥æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’æŒ‡å®š
            dtick=3600000  # 1æ™‚é–“ã”ã¨ã«è¡¨ç¤º (3600000ãƒŸãƒªç§’ = 1æ™‚é–“)
        ),
        barmode='group'  # è¤‡æ•°ã®ãƒãƒ¼ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    )

    # ã‚°ãƒ©ãƒ•ã‚’Streamlitã§è¡¨ç¤º
    st.plotly_chart(fig)

    # 5æ™‚é–“å‰ã®æ—¥æ™‚ã‚’è¨ˆç®—
    hours_before = start_time - pd.Timedelta(hours=lookback_hours)

    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«çµæœã‚’è¡¨ç¤ºã™ã‚‹
    hourly_kanban_count_full = hourly_kanban_count_full[(hourly_kanban_count_full['æ—¥æ™‚'] >= hours_before) & (hourly_kanban_count_full['æ—¥æ™‚'] < end_time)]

    # æ–°ã—ã„åˆ—ã€Œå‚™è€ƒã€ã‚’è¿½åŠ ã—ã€start_timeã«åŸºã¥ã„ã¦ã€Œéå»ã€ã€Œæœªæ¥ã€ã¨è¡¨ç¤º
    hourly_kanban_count_full['â€»æ³¨é‡ˆ                                                                               '] = hourly_kanban_count_full['æ—¥æ™‚'].apply(
        lambda x: 'ã‚ãªãŸã¯ã“ã®æ™‚é–“ã‚’é¸æŠã—ã¾ã—ãŸ' if x == start_time else ('éå»' if x < start_time else 'æœªæ¥')
    )

    # 'æ—¥æ™‚'åˆ—ã§start_timeã«ä¸€è‡´ã™ã‚‹è¡Œã‚’ãƒã‚¤ãƒ©ã‚¤ãƒˆ
    def highlight_start_time(row):
        return ['background-color: yellow' if row['æ—¥æ™‚'] == start_time else '' for _ in row]
    
    st.code(f"ğŸ“ è¨ˆç®—å¼ï¼šæœªæ¥ã®åœ¨åº«æ•° = åœ¨åº«æ•° + å·¥å ´åˆ°ç€äºˆå®šã‹ã‚“ã°ã‚“æ•° - æ—¥é‡ç®±æ•°/ç¨¼åƒæ™‚é–“")

    # æ³¨é‡ˆã‚’è¿½åŠ ï¼ˆä¾‹ã¨ã—ã¦start_timeã‚’è¡¨ç¤ºï¼‰
    st.markdown(f"")
    st.markdown(f"")
    st.markdown(f"**ä¸‹ã®è¡¨ã§äºˆæ¸¬ã®å†…å®¹ã‚’ç¢ºèªã§ãã¾ã™ã€‚**")
    #st.code(f"è¨ˆç®—å¼ï¼šåœ¨åº«æ•° + å·¥å ´åˆ°ç€äºˆå®šã‹ã‚“ã°ã‚“æ•° - æ—¥é‡ç®±æ•°/ç¨¼åƒæ™‚é–“")

    # 'hourly_kanban_count_full' ã¨ 'inventory_df_adjusted' ã‚’ 'æ—¥æ™‚' ã‚’ã‚­ãƒ¼ã«çµåˆ
    merged_df = pd.merge(hourly_kanban_count_full, inventory_df_adjusted, on='æ—¥æ™‚', how='outer')
    activedata_extracted = activedata_extracted[(activedata_extracted['æ—¥æ™‚'] >= hours_before) & (activedata_extracted['æ—¥æ™‚'] < end_time)]
    merged_df = pd.merge(merged_df, activedata_extracted, on='æ—¥æ™‚', how='outer')

    # å¿…è¦ã«å¿œã˜ã¦NaNã‚’0ã«ç½®ãæ›ãˆã‚‹ï¼ˆåœ¨åº«æ•°ã‚„ã‹ã‚“ã°ã‚“æ•°ã«é–¢ã—ã¦ï¼‰
    merged_df.fillna(0, inplace=True)

    # Streamlitã§è¡¨ç¤º
    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®åˆ—ã®é †ç•ªã‚’æŒ‡å®š
    new_column_order = ['æ—¥æ™‚', 'ç´å…¥äºˆå®šã‹ã‚“ã°ã‚“æ•°', 'å·¥å ´åˆ°ç€äºˆå®šã‹ã‚“ã°ã‚“æ•°', 'æ—¥é‡æ•°ï¼ˆç®±æ•°ï¼‰/ç¨¼åƒæ™‚é–“', 'åœ¨åº«æ•°ï¼ˆç®±ï¼‰','â€»æ³¨é‡ˆ                                                                               ']
    # åˆ—ã®é †ç•ªã‚’å¤‰æ›´
    merged_df = merged_df[new_column_order]

    # æ¡ä»¶ã«è©²å½“ã™ã‚‹è¡Œã®åœ¨åº«æ•°ã‚’ "-" ã«ã™ã‚‹
    merged_df.loc[
        (merged_df['æ—¥æ™‚'] >= hours_before) & 
        (merged_df['æ—¥æ™‚'] < start_time), 
        'åœ¨åº«æ•°ï¼ˆç®±ï¼‰'
    ] = "-"

    # 'æ—¥æ™‚'åˆ—ã§start_timeã«ä¸€è‡´ã™ã‚‹è¡Œã‚’ãƒã‚¤ãƒ©ã‚¤ãƒˆã—ã¦è¡¨ç¤º
    st.dataframe(merged_df.style.apply(highlight_start_time, axis=1))








