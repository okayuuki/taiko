#ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®import
import os
import pandas as pd
import warnings
import numpy as np
import pandas as pd
#%matplotlib inline#Jupyter Notebook å°‚ç”¨ã®ãƒã‚¸ãƒƒã‚¯ã‚³ãƒãƒ³ãƒ‰ã€‚ãƒ¡ãƒ³ãƒ†ç”¨ã§åˆ©ç”¨
import matplotlib.pyplot as plt
import re
import time
import shutil
import shap
import locale
import seaborn as sns
import matplotlib as mpl
from dateutil.relativedelta import relativedelta
from IPython.display import display, clear_output
from sklearn.ensemble import RandomForestRegressor
from sklearn.impute import SimpleImputer
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
from matplotlib.gridspec import GridSpec
from datetime import datetime
from datetime import timedelta
from PIL import Image
from sklearn.linear_model import Ridge
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Lasso
from sklearn.metrics import mean_squared_error, max_error, mean_absolute_error
import streamlit as st
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime, date, time
import sys


# ãƒ•ã‚©ãƒ³ãƒˆè¨­å®šã®å¤‰æ›´ï¼ˆæ—¥æœ¬èªå¯¾å¿œã®ãŸã‚ï¼‰
mpl.rcParams['font.family'] = 'MS Gothic'

#ãƒ‡ãƒ¼ã‚¿èª­ã¿å–ã‚Šç”¨
from read_v2 import read_data
#ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ç”¨
from functions_v2 import display_corr_matrix, calculate_hourly_counts,calculate_business_time_base,calculate_business_time_order, \
    calculate_business_time_reception,calculate_median_lt,find_best_lag_range,create_lagged_features,add_part_supplier_info, \
        find_columns_with_word_in_name,calculate_elapsed_time_since_last_dispatch,timedelta_to_hhmmss,set_arrival_flag, \
            drop_columns_with_word,calculate_window_width,process_shiresakibin_flag,feature_engineering, display_shap_contributions
    
def show_analysis(product):

    #å­¦ç¿’æœŸé–“ï¼ˆè§£ææœŸé–“ï¼‰ä»»æ„ã«è¨­å®šã§ãã‚‹ã‚ˆã†ã«
    start_date = '2023-10-01'
    end_date = '2024-03-31'

    #å‰å‡¦ç†æ¸ˆã¿ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    AutomatedRack_Details_df, arrival_times_df, kumitate_df, teikibin_df, Timestamp_df, zaiko_df = read_data()

    # è¨­å®š
    order_time_col = 'ç™ºæ³¨æ—¥æ™‚'
    reception_time_col = 'æ¤œåæ—¥æ™‚'
    target_time_col = 'é †ç«‹è£…ç½®å…¥åº«æ—¥æ™‚'
    leave_time_col = 'é †ç«‹è£…ç½®å‡ºåº«æ—¥æ™‚'

    # å…¨ã¦ã®è­¦å‘Šã‚’ç„¡è¦–ã™ã‚‹
    warnings.filterwarnings('ignore')
        
    #-------------------------------------------------------------
    
    # çµæœã‚’ä¿å­˜ã™ã‚‹ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’åˆæœŸåŒ–
    results_df = pd.DataFrame(columns=['å“ç•ª','ä»•å…¥å…ˆå','å¹³å‡åœ¨åº«','Ridgeå›å¸°ã®å¹³å‡èª¤å·®', 'Ridgeå›å¸°ã®ãƒã‚¤ãƒŠã‚¹æ–¹å‘ã®æœ€å¤§èª¤å·®', 'Ridgeå›å¸°ã®ãƒ—ãƒ©ã‚¹æ–¹å‘ã®æœ€å¤§èª¤å·®',
                                           'ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã®å¹³å‡èª¤å·®', 'ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã®ãƒã‚¤ãƒŠã‚¹æ–¹å‘ã®æœ€å¤§èª¤å·®', 'ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã®ãƒ—ãƒ©ã‚¹æ–¹å‘ã®æœ€å¤§èª¤å·®'],dtype=object)
    
    #Timestamp_dfã¯æ‰€åœ¨ç®¡ç†MBã®ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆã—ãŸã‚‚ã®
    #LINKSã¨è‡ªå‹•ãƒ©ãƒƒã‚¯QRã®ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆã—ãŸã‚‚ã®ã€ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—å½¢å¼

    #zaiko_dfã¯è‡ªå‹•ãƒ©ãƒƒã‚¯ã®åœ¨åº«

    #Timestamp_df, zaiko_df, teikibin_df

    #å“ç•ªã®æ•°ã ã‘ãƒ«ãƒ¼ãƒ—ã‚’å›ã™
    #ä»Šã¯1å“ç•ªã§
    count = 0
    for part_number in [product]:
        
        # ç¢ºèªç”¨ï¼šå®Ÿè¡Œæ™‚ã®æ¡ä»¶ç¢ºèª
        # filtered_Timestamp_df = Timestamp_df[Timestamp_df['å“ç•ª'] == part_number]#ç‰¹å®šå“ç•ªã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
        # suppliers = filtered_Timestamp_df['ä»•å…¥å…ˆå'].unique()#è©²å½“ä»•å…¥å…ˆåã‚’æŠ½å‡º
        # supplier = str(suppliers[0])
        # count = count + 1
        # print("å“ç•ªï¼š", part_number)
        # print("ä»•å…¥å…ˆåï¼š", supplier)
        # print("ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªå“ç•ªã®æ•°ï¼š", len(Timestamp_df['å“ç•ª'].unique()))
        # print("ãƒ«ãƒ¼ãƒ—ï¼š", count)

        #! å†…å®¹ï¼šé–¢æ‰€æ¯ã®ã‹ã‚“ã°ã‚“æ•°ï¼ˆ1æ™‚é–“å˜ä½ï¼‰ã‚’è¨ˆç®—
        #! Argsï¼šé–¢æ‰€æ¯ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãƒ‡ãƒ¼ã‚¿ã€é–‹å§‹æ™‚é–“ã€çµ‚äº†æ™‚é–“
        #! Returnï¼šé–¢æ‰€æ¯ã®ã‹ã‚“ã°ã‚“æ•°ï¼ˆ1æ™‚é–“å˜ä½ï¼‰
        hourly_counts_of_order, _ , _ = calculate_hourly_counts(Timestamp_df, part_number, order_time_col, start_date, end_date)#ç™ºæ³¨
        hourly_counts_of_reception, delivery_info, reception_times = calculate_hourly_counts(Timestamp_df, part_number, reception_time_col, start_date, end_date)#æ¤œå
        hourly_counts_of_in, _ , _ = calculate_hourly_counts(Timestamp_df, part_number, target_time_col, start_date, end_date)#å…¥åº«
        hourly_counts_of_out, _ , _ = calculate_hourly_counts(Timestamp_df, part_number, leave_time_col, start_date, end_date)#å‡ºåº«

        #! å†…å®¹ï¼šæ™‚é–“é…ã‚Œã‚’è¨ˆç®—ã€‚ç™ºæ³¨ã‹ã‚‰å…¥åº«ã¾ã§ã®æ™‚é–“ã€æ¤œåã‹ã‚‰å…¥åº«ã¾ã§ã®æ™‚é–“ã‚’è¨ˆç®—ï¼ˆéç¨¼å‹•æ—¥æ™‚é–“ã‚’ã®å–ã‚Šé™¤ã„ã¦ï¼‰
        #! Argsï¼šå“ç•ªã€é–¢æ‰€æ¯ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãƒ‡ãƒ¼ã‚¿
        #! Returnï¼šç™ºæ³¨ã€œå…¥åº«LTã€æ¤œåã€œå…¥åº«LTï¼ˆæ—¥å˜ä½ï¼‰ã®ä¸­å¤®å€¤
        median_lt_order, median_lt_reception = calculate_median_lt(part_number,Timestamp_df)
        
        # Todoï¼šç™ºæ³¨æ—¥æ™‚ã¯2å±±ã‚ã‚‹ã€‚ç™ºæ³¨ã—ã¦4æ—¥å¾Œã«ç´å…¥ã›ã‚ˆã¨ã‹ã‚ã‚‹ã€åœŸæ—¥ã®å½±éŸ¿ï¼Ÿ
        #! å†…å®¹ï¼šç™ºæ³¨ã‹ã‚“ã°ã‚“æ•°ã®æœ€é©ãªå½±éŸ¿æ™‚é–“ç¯„å›²ã‚’è¦‹ã¤ã‘ã‚‹
        #! Argsï¼š1æ™‚é–“ã”ã¨ã®ç™ºæ³¨ã‹ã‚“ã°ã‚“æ•°ã€1æ™‚é–“ã”ã¨ã®å…¥åº«ã‹ã‚“ã°ã‚“æ•°ã€æ¢ç´¢æ™‚é–“ç¯„å›²
        #! Returnï¼šæœ€é©ç›¸é–¢å€¤ã€æœ€é©é–‹å§‹é…ã‚Œã€çµ‚äº†ç¯„å›²é…ã‚Œ
        min_lag =int(median_lt_order * 24)-4  # LTä¸­å¤®å€¤ã‚’åŸºæº–ã«æœ€å°é…ã‚Œæ™‚é–“ã‚’è¨­å®š
        max_lag =int(median_lt_order * 24)+4  # LTä¸­å¤®å€¤ã‚’åŸºæº–ã«æœ€å¤§é…ã‚Œæ™‚é–“ã‚’è¨­å®š
        best_corr_order, best_range_start_order, best_range_end_order = find_best_lag_range(hourly_counts_of_order, hourly_counts_of_in, min_lag, max_lag, 'ç™ºæ³¨ã‹ã‚“ã°ã‚“æ•°')

        # Todoï¼šæ¤œåã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã¯ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§å–å¾—ã§ããªã„ãŸã‚ã€ä¿¡ç”¨ã§ããªã„ã€‚ä¼ç¥¨å˜ä½ã§å–å¾—ã—ã¦ã„ã‚‹
        #! å†…å®¹ï¼šç™ºæ³¨ã‹ã‚“ã°ã‚“æ•°ã®æœ€é©ãªå½±éŸ¿æ™‚é–“ç¯„å›²ã‚’è¦‹ã¤ã‘ã‚‹
        #! Argsï¼š1æ™‚é–“ã”ã¨ã®æ¤œåã‹ã‚“ã°ã‚“æ•°ã€1æ™‚é–“ã”ã¨ã®å…¥åº«ã‹ã‚“ã°ã‚“æ•°ã€æ¢ç´¢æ™‚é–“ç¯„å›²
        #! Returnï¼šæœ€é©ç›¸é–¢å€¤ã€æœ€é©é–‹å§‹é…ã‚Œã€çµ‚äº†ç¯„å›²é…ã‚Œ
        min_lag = int(median_lt_reception * 24)-4  # LTä¸­å¤®å€¤ã‚’åŸºæº–ã«æœ€å°é…ã‚Œæ™‚é–“ã‚’è¨­å®š
        max_lag = int(median_lt_reception * 24)+4  # LTä¸­å¤®å€¤ã‚’åŸºæº–ã«æœ€å¤§é…ã‚Œæ™‚é–“ã‚’è¨­å®š
        best_corr_reception, best_range_start_reception, best_range_end_reception = find_best_lag_range(hourly_counts_of_reception, hourly_counts_of_in, min_lag, max_lag, 'ç´å…¥ã‹ã‚“ã°ã‚“æ•°')
        
        # ç¢ºèªç”¨ï¼šå®Ÿè¡Œçµæœã®ç¢ºèª
        #print(f"Best range for ç™ºæ³¨: {best_range_start_order}æ™‚é–“å‰ã‹ã‚‰{best_range_end_order}æ™‚é–“å‰ã¾ã§")
        #print(f"Best correlation for ç™ºæ³¨: {best_corr_order}")
        #print(f"æ¤œåã€œå…¥åº«LTä¸­å¤®å€¤ï¼š{median_lt_reception}æ—¥,æ¤œåã€œå…¥åº«æ™‚é–“ä¸­å¤®å€¤ï¼š{median_lt_reception*24}æ™‚é–“")
        #print(f"Best range for æ¤œå: {best_range_start_reception}æ™‚é–“å‰ã‹ã‚‰{best_range_end_reception}æ™‚é–“å‰ã¾ã§")
        #print(f"Best correlation for æ¤œå: {best_corr_reception}")

        #! å†…å®¹ï¼šæœ€é©ãªå½±éŸ¿æ™‚é–“ç¯„å›²ã«åŸºã¥ã„ã¦ç™ºæ³¨ã‹ã‚“ã°ã‚“æ•°ã¨æ¤œåã‹ã‚“ã°ã‚“æ•°ã‚’è¨ˆç®—
        #! Argsï¼š1æ™‚é–“ã”ã¨ã®ç™ºæ³¨ã‹ã‚“ã°ã‚“æ•°ã€1æ™‚é–“ã”ã¨ã®å…¥åº«ã‹ã‚“ã°ã‚“æ•°ã€æœ€é©æ™‚é–“é…ã‚Œç¯„å›²
        #! Returnï¼šæœ€é©æ™‚é–“é…ã‚Œã§è¨ˆç®—ã—ãŸç™ºæ³¨ã‹ã‚“ã°ã‚“æ•°ã€å…¥åº«ã‹ã‚“ã°ã‚“æ•°
        lagged_features_order = create_lagged_features(hourly_counts_of_order, hourly_counts_of_in, hourly_counts_of_out, best_range_start_order, best_range_end_order, 'ç™ºæ³¨ã‹ã‚“ã°ã‚“æ•°', delivery_info, reception_times)
        lagged_features_reception = create_lagged_features(hourly_counts_of_reception, hourly_counts_of_in, hourly_counts_of_out, best_range_start_reception, best_range_end_reception, 'ç´å…¥ã‹ã‚“ã°ã‚“æ•°', delivery_info, reception_times)
        # å‰å‡¦ç†ï¼šé‡è¤‡ã®ã‚ã‚‹target åˆ—ã‚’å‰Šé™¤
        lagged_features_reception = lagged_features_reception.drop(columns=['å…¥åº«ã‹ã‚“ã°ã‚“æ•°ï¼ˆtï¼‰'])
        lagged_features_reception = lagged_features_reception.drop(columns=['å‡ºåº«ã‹ã‚“ã°ã‚“æ•°ï¼ˆtï¼‰'])
        # æœ€é©ãªå½±éŸ¿æ™‚é–“ç¯„å›²ã«åŸºã¥ã„ãŸç™ºæ³¨ã‹ã‚“ã°ã‚“æ•°ã¨ã€æ¤œåã‹ã‚“ã°ã‚“æ•°ã‚’çµ±åˆ
        lagged_features = lagged_features_order.join(lagged_features_reception, how='outer')

        #ç¢ºèªï¼šå®Ÿè¡Œçµæœ
        #st.dataframe(lagged_features.head(300))

        #! å†…å®¹ï¼šå„ç¨®æƒ…å ±ã‚’è¿½åŠ 
        #lagged_featuresã«æƒ…å ±è¿½åŠ 
        lagged_features['åœ¨åº«å¢—æ¸›æ•°ï¼ˆtï¼‰'] = lagged_features['å…¥åº«ã‹ã‚“ã°ã‚“æ•°ï¼ˆtï¼‰'] - lagged_features['å‡ºåº«ã‹ã‚“ã°ã‚“æ•°ï¼ˆtï¼‰']#åœ¨åº«å¢—æ¸›æ•°ã‚’è¨ˆç®—
        lagged_features['ç™ºæ³¨ã‹ã‚“ã°ã‚“æ•°ï¼ˆtï¼‰'] = hourly_counts_of_order#ç™ºæ³¨ã‹ã‚“ã°ã‚“æ•°(t)ã‚’è¨ˆç®—
        lagged_features['ç´å…¥ã‹ã‚“ã°ã‚“æ•°ï¼ˆtï¼‰'] = hourly_counts_of_reception#ç´å…¥ã‹ã‚“ã°ã‚“æ•°(t)ã‚’è¨ˆç®—
        lagged_features = add_part_supplier_info(Timestamp_df, lagged_features, part_number)#å“ç•ªã¨ä»•å…¥å…ˆåã‚’è¿½åŠ 
        lagged_features = lagged_features.rename(columns={'ä»•å…¥å…ˆå·¥å ´å': 'ç™ºé€å ´æ‰€å'})#ã‚³ãƒ©ãƒ åå¤‰æ›´
        lagged_features, median_interval = calculate_elapsed_time_since_last_dispatch(lagged_features)# éå»ã®å‡ºåº«ã‹ã‚‰ã®çµŒéæ™‚é–“ã‚’è¨ˆç®—
        lagged_features = pd.merge(lagged_features, zaiko_df[['æ—¥æ™‚', 'å“ç•ª','åœ¨åº«æ•°ï¼ˆç®±ï¼‰']], on=['å“ç•ª', 'æ—¥æ™‚'], how='left')#è‡ªå‹•ãƒ©ãƒƒã‚¯åœ¨åº«çµåˆ
        lagged_features = pd.merge(lagged_features, AutomatedRack_Details_df, on=['æ—¥æ™‚'], how='left')#1æ™‚é–“ã‚ã‚ãŸã‚Šã®é–“å£åˆ¥åœ¨åº«ã®è¨ˆç®—
        for col in lagged_features.columns:
            if pd.api.types.is_timedelta64_dtype(lagged_features[col]):
                lagged_features[col] = lagged_features[col].fillna(pd.Timedelta(0))
            else:
                lagged_features[col] = lagged_features[col].fillna(0)
        lagged_features = process_shiresakibin_flag(lagged_features, arrival_times_df)#ä»•å…¥å…ˆä¾¿åˆ°ç€ãƒ•ãƒ©ã‚°è¨ˆç®—
        lagged_features = pd.merge(lagged_features,kumitate_df[['æ—¥æ™‚','ç”Ÿç”£å°æ•°_åŠ é‡å¹³å‡æ¸ˆ','è¨ˆç”»ç”Ÿç”£å°æ•°_åŠ é‡å¹³å‡æ¸ˆ','è¨ˆç”»é”æˆç‡_åŠ é‡å¹³å‡æ¸ˆ']], on='æ—¥æ™‚', how='left')# lagged_features ã¨ kumitate_df ã‚’æ—¥æ™‚ã§ãƒãƒ¼ã‚¸
        
        #ç¢ºèªï¼šå®Ÿè¡Œçµæœ
        #st.dataframe(lagged_features.head(300))
        
        best_range_order = int((best_range_start_order + best_range_end_order)/2)#æœ€é©ãªç™ºæ³¨ã‹ã‚“ã°ã‚“æ•°ã®å¹…
        best_range_reception = int((best_range_start_reception + best_range_end_reception)/2)#æœ€é©ãªç´å…¥ã‹ã‚“ã°ã‚“æ•°ã®å¹…
        
        #å®šæœŸä¾¿
        lagged_features = pd.merge(lagged_features, teikibin_df[['æ—¥æ™‚', 'è·å½¹æ™‚é–“(t-4)','è·å½¹æ™‚é–“(t-5)','è·å½¹æ™‚é–“(t-6)']], on='æ—¥æ™‚', how='left')
        #ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
        lagged_features = feature_engineering(lagged_features)

        #ãƒ­ãƒ¼ãƒªãƒ³ã‚°ç‰¹å¾´é‡
        #è§£æçª“ã§è¨ˆç®—
        lagged_features = calculate_window_width(lagged_features, best_range_end_order, 0, best_range_order, best_range_reception)

        # ä¸è¦ãªåˆ—ã‚’å‰Šé™¤
        #lagged_features = lagged_features.drop(['ç”Ÿç”£å°æ•°_åŠ é‡å¹³å‡æ¸ˆ','ç”Ÿç”£å°æ•°_åŠ é‡å¹³å‡æ¸ˆ'],axis=1)

        # NaNå€¤ã‚’å‡¦ç†ã™ã‚‹ï¼ˆä¾‹: 0ã§åŸ‹ã‚ã‚‹ï¼‰
        lagged_features = lagged_features.fillna(0)

        # columns_printã¯'ç™ºè¡Œã‹ã‚“ã°ã‚“'ã‚’å«ã‚€åˆ—å
        columns_enter = find_columns_with_word_in_name(lagged_features, 'å…¥åº«ã‹ã‚“ã°ã‚“æ•°ï¼ˆt-0~')
        #lagged_features['éƒ¨å“ç½®ãå ´ã‹ã‚‰ã®æŠ•å…¥'] = lagged_features[columns_enter] - lagged_features[f'ç™ºæ³¨ã‹ã‚“ã°ã‚“æ•°ï¼ˆt-{best_range_order}~t-{best_range_order*2}ï¼‰']
        # columns_printã¯'ç™ºæ³¨ã‹ã‚“ã°ã‚“'ã‚’å«ã‚€åˆ—å
        columns_order = find_columns_with_word_in_name(lagged_features, 'ç™ºæ³¨ã‹ã‚“ã°ã‚“æ•°ï¼ˆt-')
        # columns_printã¯'ç™ºæ³¨ã‹ã‚“ã°ã‚“'ã‚’å«ã‚€åˆ—å
        columns_reception = find_columns_with_word_in_name(lagged_features, 'ç´å…¥ã‹ã‚“ã°ã‚“æ•°ï¼ˆt-')
        lagged_features['ç´å…¥ãƒ•ãƒ¬ï¼ˆè² ã¯æœªç´ã‚„æ­£ã¯æŒ½å›ç´å…¥æ•°ã‚’è¡¨ã™ï¼‰'] = lagged_features[columns_reception] - lagged_features[columns_order]

        #display_corr_matrix(lagged_features)
        #st.dataframe(lagged_features.head(300))
        
        #    ##å…¨éƒ¨çµ‚ã‚ã£ãŸå¾Œã«éç¨¼å‹•æ—¥æ™‚é–“ã®ãƒ‡ãƒ¼ã‚¿è¿½åŠ ã€‚ä¸Šã¾ã§é…ã‚Œè¨ˆç®—ã§åœŸæ—¥ãªã©ã‚’é™¤å¤–ã—ã¦ã„ã‚‹ã®ã§ã€‚
        #    # è£œå®Œã™ã‚‹æ™‚é–“ç¯„å›²ã‚’æ±ºå®š
        #    full_range = pd.date_range(start=start_date, end=end_date, freq='H')
        #    # full_rangeã‚’ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«å¤‰æ›
        #    full_df = pd.DataFrame(full_range, columns=['æ—¥æ™‚'])
        #    # å…ƒã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã¨ãƒãƒ¼ã‚¸ã—ã¦æ¬ æå€¤ã‚’è£œå®Œ
        #    lagged_features = pd.merge(full_df, lagged_features, on='æ—¥æ™‚', how='left')
        #    # æ¬ æå€¤ã‚’0ã§è£œå®Œ
        #    lagged_features.fillna(0, inplace=True)
        #    #
        #    lagged_features = lagged_features.drop(columns=['åœ¨åº«æ•°ï¼ˆç®±ï¼‰'])
        #    lagged_features['å“ç•ª']=part_number
        #    lagged_features = pd.merge(lagged_features, df2[['æ—¥æ™‚', 'å“ç•ª','åœ¨åº«æ•°ï¼ˆç®±ï¼‰']], on=['å“ç•ª', 'æ—¥æ™‚'], how='left')#è‡ªå‹•ãƒ©ãƒƒã‚¯åœ¨åº«çµåˆ

        #------------------------------------------------------------------------------------------------------------------
        #å‰Šé™¤ã€ä»Šã¯24å¹´åº¦ã®ãƒ‡ãƒ¼ã‚¿ãŒãªã„ã‹ã‚‰
        start = '2023-12-30'
        end = '2024-03-31'
        # æ—¥ä»˜ç¯„å›²ã«åŸºã¥ã„ã¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ã¦å‰Šé™¤
        lagged_features= lagged_features[~((lagged_features['æ—¥æ™‚'] >= start) & (lagged_features['æ—¥æ™‚'] <= end))]
        #------------------------------------------------------------------------------------------------------------------

        data_temp = lagged_features#é…ã‚Œåˆ†å‰Šé™¤
        data = data_temp.iloc[300:]#é…ã‚Œåˆ†å‰Šé™¤
        end_hours_ago = 0
        reception_timelag = best_range_reception
        #data['å·®åˆ†']=data[f'ç™ºæ³¨ã‹ã‚“ã°ã‚“æ•°ï¼ˆt-{timelag}~t-{timelag*2}ï¼‰']-data[f'ç´å…¥ã‹ã‚“ã°ã‚“æ•°ï¼ˆt-{reception_timelag}~t-{timelag+reception_timelag}ï¼‰']
        # èª¬æ˜å¤‰æ•°ã®å®šç¾©

        st.dataframe(lagged_features.head(300))

        data = data.rename(columns={'ä»•å…¥å…ˆä¾¿åˆ°ç€ãƒ•ãƒ©ã‚°': 'ä»•å…¥å…ˆä¾¿åˆ°ç€çŠ¶æ³'})#ã‚³ãƒ©ãƒ åå¤‰æ›´
        data['å®šæœŸä¾¿å‡ºç™ºçŠ¶æ³']=data['è·å½¹æ™‚é–“(t-4)']/50+data['è·å½¹æ™‚é–“(t-4)']/50+data['è·å½¹æ™‚é–“(t-4)']/50

        X = data[[f'ç™ºæ³¨ã‹ã‚“ã°ã‚“æ•°ï¼ˆt-{best_range_order}~t-{best_range_order*2}ï¼‰',f'è¨ˆç”»çµ„ç«‹ç”Ÿç”£å°æ•°_åŠ é‡å¹³å‡ï¼ˆt-{end_hours_ago}~t-{best_range_order}ï¼‰',f'è¨ˆç”»é”æˆç‡_åŠ é‡å¹³å‡ï¼ˆt-{end_hours_ago}~t-{best_range_order}ï¼‰',
                  'ç´å…¥ãƒ•ãƒ¬ï¼ˆè² ã¯æœªç´ã‚„æ­£ã¯æŒ½å›ç´å…¥æ•°ã‚’è¡¨ã™ï¼‰','ä»•å…¥å…ˆä¾¿åˆ°ç€çŠ¶æ³','å®šæœŸä¾¿å‡ºç™ºçŠ¶æ³',#'è·å½¹æ™‚é–“(t-4)','è·å½¹æ™‚é–“(t-5)','è·å½¹æ™‚é–“(t-6)',
                  f'é–“å£ã®å¹³å‡å……è¶³ç‡ï¼ˆt-{end_hours_ago}~t-{best_range_order}ï¼‰',#f'é–“å£_A1ã®å……è¶³ç‡ï¼ˆt-{end_hours_ago}~t-{best_range_order}ï¼‰',f'é–“å£_A2ã®å……è¶³ç‡ï¼ˆt-{end_hours_ago}~t-{best_range_order}ï¼‰', f'é–“å£_B1ã®å……è¶³ç‡ï¼ˆt-{end_hours_ago}~t-{best_range_order}ï¼‰', f'é–“å£_B2ã®å……è¶³ç‡ï¼ˆt-{end_hours_ago}~t-{best_range_order}ï¼‰',f'é–“å£_B3ã®å……è¶³ç‡ï¼ˆt-{end_hours_ago}~t-{best_range_order}ï¼‰', f'é–“å£_B4ã®å……è¶³ç‡ï¼ˆt-{end_hours_ago}~t-{best_range_order}ï¼‰',
                  f'éƒ¨å“ç½®ãå ´ã®å…¥åº«æ»ç•™çŠ¶æ³ï¼ˆt-{end_hours_ago}~t-{best_range_order}ï¼‰',#f'éƒ¨å“ç½®ãå ´ã‹ã‚‰ã®å…¥åº«ï¼ˆt-{end_hours_ago}~t-{best_range_order}ï¼‰',f'éƒ¨å“ç½®ãå ´ã§æ»ç•™ï¼ˆt-{end_hours_ago}~t-{best_range_order}ï¼‰',
                  f'å®šæœŸä¾¿ã«ãƒ¢ãƒç„¡ã—ï¼ˆt-{end_hours_ago}~t-{best_range_order}ï¼‰']]
        # ç›®çš„å¤‰æ•°ã®å®šç¾©
        #â˜…
        y = data[f'åœ¨åº«å¢—æ¸›æ•°ï¼ˆt-0~t-{best_range_order}ï¼‰']
        #y = data[f'åœ¨åº«å¢—æ¸›æ•°(t)']

        # ãƒ‡ãƒ¼ã‚¿ã‚’å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«åˆ†å‰²
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


        # Lassoå›å¸°ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ
        ridge = Ridge(alpha=0.1)
        # ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´
        ridge.fit(X_train, y_train)
        # äºˆæ¸¬
        y_pred_train = ridge.predict(X_train)
        y_pred_test = ridge.predict(X_test)
        # è©•ä¾¡
        mse_train = mean_squared_error(y_train, y_pred_train)
        mse_test = mean_squared_error(y_test, y_pred_test)
        max_error_train = max_error(y_train, y_pred_train)
        max_error_test = max_error(y_test, y_pred_test)
        # ãƒã‚¤ãƒŠã‚¹æ–¹å‘ã®æœ€å¤§èª¤å·®ã‚’è¨ˆç®—
        min_error_train = np.min(y_train - y_pred_train)
        min_error_test = np.min(y_test - y_pred_test)

        #print(f'Ridgeå›å¸° - è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®MSE: {mse_train}')
        #print(f'Ridgeå›å¸° - ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®MSE: {mse_test}')
        #print(f'Ridgeå›å¸° - è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®æœ€å¤§èª¤å·®: {max_error_train}')
        #print(f'Ridgeå›å¸° - ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æœ€å¤§èª¤å·®: {max_error_test}')
        #print(f'Ridgeå›å¸° - è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ãƒã‚¤ãƒŠã‚¹æ–¹å‘ã®æœ€å¤§èª¤å·®: {min_error_train}')
        #print(f'Ridgeå›å¸° - ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ãƒã‚¤ãƒŠã‚¹æ–¹å‘ã®æœ€å¤§èª¤å·®: {min_error_test}')
        # å¹³å‡èª¤å·®ã‚’è¨ˆç®—
        mae = mean_absolute_error(y_test, y_pred_test)
        #print(f'Ridgeå›å¸° - ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å¹³å‡èª¤å·®: {mae}')

        #--------------------------------------------------------------------------------------------------------

        # ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´
        rf_model = RandomForestRegressor(n_estimators=10, max_depth=20,random_state=42)
        rf_model.fit(X_train, y_train)
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§äºˆæ¸¬ã—ã€MSEã‚’è¨ˆç®—
        y_pred = rf_model.predict(X_test)
        mse = mean_squared_error(y_test, y_pred)
        print(f'ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ - ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®MSE: {mse}')
        # æœ€å¤§èª¤å·®ã‚’è¨ˆç®—
        max_err = max_error(y_test, y_pred)
        print(f'ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ - ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æœ€å¤§èª¤å·®: {max_err}')
        # ãƒã‚¤ãƒŠã‚¹æ–¹å‘ã®æœ€å¤§èª¤å·®ã‚’è¨ˆç®—
        min_err = np.min(y_test - y_pred)
        print(f'ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ - ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ãƒã‚¤ãƒŠã‚¹æ–¹å‘ã®æœ€å¤§èª¤å·®: {min_err}')
        # å¹³å‡èª¤å·®ã‚’è¨ˆç®—
        mae2 = mean_absolute_error(y_test, y_pred)
        print(f'ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ - ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å¹³å‡èª¤å·®: {mae2}')
        #--------------------------------------------------------------------------------------------------------
        
        unique_hinban_list = lagged_features['ä»•å…¥å…ˆå'].unique()
        supply = str(unique_hinban_list[0])
        zaikozaiko = lagged_features['åœ¨åº«æ•°ï¼ˆç®±ï¼‰'].mean()
        
        #appendãƒ¡ã‚½ãƒƒãƒ‰ã¯pandasã®æœ€æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§ã¯å»ƒæ­¢
        # çµæœã‚’ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«è¿½åŠ 
        #results_df = results_df.append({'å“ç•ª': part_number,'ä»•å…¥å…ˆå':supply,'å¹³å‡åœ¨åº«':zaikozaiko,'Ridgeå›å¸°ã®å¹³å‡èª¤å·®': mae, 'Ridgeå›å¸°ã®ãƒã‚¤ãƒŠã‚¹æ–¹å‘ã®æœ€å¤§èª¤å·®': min_error_test, 'Ridgeå›å¸°ã®ãƒ—ãƒ©ã‚¹æ–¹å‘ã®æœ€å¤§èª¤å·®': max_error_test,
                                        #'ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã®å¹³å‡èª¤å·®': mae2, 'ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã®ãƒã‚¤ãƒŠã‚¹æ–¹å‘ã®æœ€å¤§èª¤å·®': min_err, 'ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã®ãƒ—ãƒ©ã‚¹æ–¹å‘ã®æœ€å¤§èª¤å·®': max_err}, ignore_index=True)

        new_row = pd.DataFrame([{'å“ç•ª': part_number,'ä»•å…¥å…ˆå':supply,'å¹³å‡åœ¨åº«':zaikozaiko,'Ridgeå›å¸°ã®å¹³å‡èª¤å·®': mae, 'Ridgeå›å¸°ã®ãƒã‚¤ãƒŠã‚¹æ–¹å‘ã®æœ€å¤§èª¤å·®': min_error_test, 'Ridgeå›å¸°ã®ãƒ—ãƒ©ã‚¹æ–¹å‘ã®æœ€å¤§èª¤å·®': max_error_test}])
        results_df = pd.concat([results_df, new_row], ignore_index=True)

        print("çµ‚äº†")
        
        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        with open("ä¸€æ™‚ä¿å­˜ãƒ‡ãƒ¼ã‚¿.csv", mode='w',newline='', encoding='shift_jis',errors='ignore') as f:
            data.to_csv(f)
        
        return data, rf_model, X

        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ Xã‹ã‚‰100è¡Œç›®ã‹ã‚‰300è¡Œç›®ã¾ã§ã®ãƒ‡ãƒ¼ã‚¿ã‚’é¸æŠ
        #X_subset = X.iloc[0:3000]
        # ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ã¦X_subsetã‹ã‚‰äºˆæ¸¬å€¤ã‚’è¨ˆç®—
        #y_pred_subset = rf_model.predict(X_subset)
        # y_test_subset ã‚’ç”¨æ„ã™ã‚‹å¿…è¦ãŒã‚ã‚‹
        # ã“ã®ä¾‹ã§ã¯å˜ã« y_test ã®å¯¾å¿œã™ã‚‹éƒ¨åˆ†ã‚’é¸æŠã™ã‚‹ã“ã¨ã‚’ä»®å®š
        #y_test_subset = y_test.loc[X_subset.index]

        ###å…¨éƒ¨çµ‚ã‚ã£ãŸå¾Œã«éç¨¼å‹•æ—¥æ™‚é–“ã®ãƒ‡ãƒ¼ã‚¿è¿½åŠ ã€‚ä¸Šã¾ã§é…ã‚Œè¨ˆç®—ã§åœŸæ—¥ãªã©ã‚’é™¤å¤–ã—ã¦ã„ã‚‹ã®ã§ã€‚
        ## è£œå®Œã™ã‚‹æ™‚é–“ç¯„å›²ã‚’æ±ºå®š
        #full_range = pd.date_range(start=start_date, end=end_date, freq='H')
        ## full_rangeã‚’ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«å¤‰æ›
        #full_df = pd.DataFrame(full_range, columns=['æ—¥æ™‚'])
        ## å…ƒã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã¨ãƒãƒ¼ã‚¸ã—ã¦æ¬ æå€¤ã‚’è£œå®Œ
        #lagged_features = pd.merge(full_df, lagged_features, on='æ—¥æ™‚', how='left')
        ## æ¬ æå€¤ã‚’0ã§è£œå®Œ
        #lagged_features.fillna(0, inplace=True)
        ##
        #lagged_features = lagged_features.drop(columns=['åœ¨åº«æ•°ï¼ˆç®±ï¼‰'])
        #lagged_features['å“ç•ª']=part_number
        #lagged_features = pd.merge(lagged_features, df2[['æ—¥æ™‚', 'å“ç•ª','åœ¨åº«æ•°ï¼ˆç®±ï¼‰']], on=['å“ç•ª', 'æ—¥æ™‚'], how='left')#è‡ªå‹•ãƒ©ãƒƒã‚¯åœ¨åº«çµåˆ

#---------------------------------------------------------------------------------------------------------------------------------

def step2(data, rf_model, X, start_index, end_index):

    #æŠ˜ã‚Šè¿”ã—ç·šã‚’è¿½åŠ 
    st.markdown("---")

    #ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒ300ã‚¹ã‚¿ãƒ¼ãƒˆãªã®ã§ãƒªã‚»ãƒƒãƒˆ
    data = data.reset_index(drop=True)

    #start_index, end_index = visualize_stock_trend(data)#åœ¨åº«å¯è¦–åŒ–

    # SHAPè¨ˆç®—
    #explainer = shap.TreeExplainer(rf_model, feature_dependence='tree_path_dependent', model_output='margin')
    explainer = shap.TreeExplainer(rf_model, model_output='raw')
    shap_values = explainer.shap_values(X)

    first_datetime_df = data['æ—¥æ™‚'].iloc[0]
    print(f"dataã®æ—¥æ™‚åˆ—ã®æœ€åˆã®å€¤: {first_datetime_df}")

    # ãƒªã‚¹ãƒˆã‹ã‚‰æ•´æ•°ã«å¤‰æ›
    start_index_int = start_index[0]#-300
    end_index_int = end_index[0]#-300

    #start = start_index_int#0#0
    #end = end_index_int#2999

    df = data.iloc[start_index_int:end_index_int]
    print(df.head())

    #st.dataframe(df.head(300))

    first_datetime_df = df.iloc[0]
    print(f"dfã®æ—¥æ™‚åˆ—ã®æœ€åˆã®å€¤: {first_datetime_df}")

    X_subset = X.iloc[start_index_int:end_index_int]
    # ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ã¦X_subsetã‹ã‚‰äºˆæ¸¬å€¤ã‚’è¨ˆç®—
    y_pred_subset = rf_model.predict(X_subset)

    df['æ—¥æ™‚'] = pd.to_datetime(df['æ—¥æ™‚'])
    df.set_index('æ—¥æ™‚', inplace=True)

    #df2 = df['åœ¨åº«å¢—æ¸›æ•°ï¼ˆt-52~t-0ï¼‰']
    df2 = df['åœ¨åº«æ•°ï¼ˆç®±ï¼‰']
    print(df2.head())

    #åœ¨åº«æ•°ï¼ˆç®±ï¼‰ã‚’è¨ˆç®—ã™ã‚‹
    best_range_order = find_columns_with_word_in_name(df, 'åœ¨åº«æ•°ï¼ˆç®±ï¼‰ï¼ˆt-')
    yyyy = df[f'{best_range_order}']
    y_base_subset = yyyy

    #st.dataframe(y_base_subset.head(300))

    #åœ¨åº«å¢—æ¸›æ•°ã®å¹³å‡å€¤ã‚’ç¢ºèªç”¨
    #mean_value = y.mean()

    # SHAPå€¤ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä½œæˆ
    shap_df = pd.DataFrame(shap_values, columns=X.columns)

    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®å¹³å‡SHAPå€¤ã«åŸºã¥ã„ã¦ç‰¹å¾´é‡ã‚’ä¸¦ã³æ›¿ãˆ
    shap_df_mean = shap_df.abs().mean().sort_values(ascending=False)
    sorted_columns = shap_df_mean.index

    shap_df_sorted = shap_df[sorted_columns]

    dfdf = shap_df_sorted.iloc[start_index_int:end_index_int].T

    # ãƒ—ãƒ­ãƒƒãƒˆã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã®ç¯„å›²ã‚’ã‚¹ãƒ©ã‚¤ã‚¹
    dfdf_subset = dfdf#.iloc[:, start_idx:end_idx]

    dfdf_subset2 = dfdf_subset

    # å‰ã®å€¤ã¨ã®å·®åˆ†ã‚’è¨ˆç®—
    # å·®åˆ†ã¨å·®åˆ†åˆ¤å®šã‚’foræ–‡ã§è¨ˆç®—
    #difference = [None]  # æœ€åˆã®å·®åˆ†ã¯ãªã—
    #for i in range(1,len(df2_subset)):
    #    diff = df2_subset.iloc[i] - df2_subset.iloc[i-1]
    #    difference.append(diff)
    #    if i < len(dfdf_subset2):
    #        if diff > 0:
    #            dfdf_subset2.iloc[i] = dfdf_subset2.iloc[i]
    #        elif diff < 0:
    #            dfdf_subset2.iloc[i] = -1*dfdf_subset2.iloc[i]

    #--------------------------------------------------------------------------------------------

    df = dfdf_subset2

    # dfã®åˆ—æ•°ã¨df2_subsetã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ•°ã‚’ç¢ºèª
    print(f"data index: {len(data)}")
    print(f"df columns: {len(df.columns)}")
    #print(f"df2_subset index: {len(df2_subset.index)}")
    print(f"shap_df_sorted index: {len(shap_df_sorted)}")
    print(f"dfdf index: {len(dfdf)}")
    print(f"dfdf_subset2 index: {len(dfdf_subset2)}")


    # ã‚«ãƒ©ãƒ¼ãƒãƒƒãƒ—ã®é¸æŠ
    cmap = 'RdBu_r'  # é’ã‹ã‚‰èµ¤ã«å¤‰åŒ–ã™ã‚‹ã‚«ãƒ©ãƒ¼ãƒãƒƒãƒ—

    #df2_subset.index = df2_subset.index.strftime('%Y-%m-%d-%H')
    df.columns = df2.index.strftime('%Y-%m-%d-%H')

    #è¡Œã®ä¸¦ã³ã‚’åè»¢
    df_reversed = df.iloc[::-1]

    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ãƒªã‚»ãƒƒãƒˆ
    df2_subset_df = df2.to_frame().reset_index()

    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®è¡Œã¨åˆ—ã‚’å…¥ã‚Œæ›¿ãˆ
    df_transposed = df.transpose()
    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¦æ—¥æ™‚åˆ—ã‚’ä½œæˆ
    df_transposed.reset_index(inplace=True)
    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åˆ—ã®åå‰ã‚’ 'æ—¥æ™‚' ã«å¤‰æ›´
    df_transposed.rename(columns={'index': 'æ—¥æ™‚'}, inplace=True)

    #èª¬æ˜å¤‰æ•°
    zzz = X.iloc[start_index_int:end_index_int]#[start_idx:end_idx]
    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ãƒªã‚»ãƒƒãƒˆ
    zzz = zzz.reset_index(drop=True)
    #æ—¥æ™‚åˆ—
    temp_time = df_transposed.reset_index(drop=True)

    first_datetime_df1 = data['æ—¥æ™‚'].iloc[0]
    first_datetime_df2 = temp_time['æ—¥æ™‚'].iloc[0]
    first_datetime_df3 = df_transposed['æ—¥æ™‚'].iloc[0]
    print(f"dataã®æ—¥æ™‚åˆ—ã®æœ€åˆã®å€¤: {first_datetime_df1}")
    print(f"df_transposedã®æ—¥æ™‚åˆ—ã®æœ€åˆã®å€¤: {first_datetime_df3}")
    print(f"temp_timeã®æ—¥æ™‚åˆ—ã®æœ€åˆã®å€¤: {first_datetime_df2}")

    # data1ã¨data2ã‚’çµåˆ
    merged_df = pd.concat([temp_time[['æ—¥æ™‚']], zzz], axis=1)

    # é–¢æ•°ã‚’å‘¼ã³å‡ºã—ã¦è¡¨ç¤º
    #display_data_app(df2_subset_df, df_transposed, merged_df)
    
    line_data = df2_subset_df
    bar_data = df_transposed
    df2 = merged_df
    
    line_df = pd.DataFrame(line_data)
    line_df['æ—¥æ™‚'] = pd.to_datetime(line_df['æ—¥æ™‚'], format='%Y%m%d%H')

    bar_df = pd.DataFrame(bar_data)
    bar_df['æ—¥æ™‚'] = pd.to_datetime(bar_df['æ—¥æ™‚'])
    
    df2 = pd.DataFrame(df2)
    df2['æ—¥æ™‚'] = pd.to_datetime(df2['æ—¥æ™‚'])

    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«ä½¿ã„æ–¹ã‚’è¡¨ç¤º
    #st.sidebar.header("ä½¿ã„æ–¹")
    #st.sidebar.markdown("""
    #1. ä¸Šéƒ¨ã®æŠ˜ã‚Œç·šã‚°ãƒ©ãƒ•ã§å…¨ä½“ã®ãƒ‡ãƒ¼ã‚¿æ¨ç§»ã‚’ç¢ºèªã§ãã¾ã™ã€‚
    #2. ä¸‹éƒ¨ã®æ£’ã‚°ãƒ©ãƒ•ã§ã¯ã€ç‰¹å®šã®æ—¥æ™‚ã«ãŠã‘ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’è©³ç´°ã«è¡¨ç¤ºã—ã¾ã™ã€‚
    #3. ã‚¹ãƒ©ã‚¤ãƒ‰ãƒãƒ¼ã§æ—¥æ™‚ã‚’é¸æŠã—ã€çµæœãŒå‹•çš„ã«å¤‰æ›´ã•ã‚Œã¾ã™ã€‚
    #""")

    # ä¸Šã«æŠ˜ã‚Œç·šã‚°ãƒ©ãƒ•
    fig_line = go.Figure()
    for var in line_df.columns[1:]:
        fig_line.add_trace(go.Scatter(x=line_df['æ—¥æ™‚'].dt.strftime('%Y-%m-%d-%H'), y=line_df[var], mode='lines+markers', name=var))
        
    print("å¢—æ¸›")
    print(y_pred_subset)
    #st.dataframe(y_pred_subset)
    print("ãƒ™ãƒ¼ã‚¹")
    print(y_base_subset)
    #st.dataframe(y_base_subset)
    
    #åœ¨åº«å¢—æ¸›æ•°ãªã®ã§ã€åœ¨åº«æ•°ã‚’è¨ˆç®—ã™ã‚‹æ™‚ã¯ã€ä»¥ä¸‹ã®å‡¦ç†ã‚’ã™ã‚‹
    # 2ã¤ç›®ã®æŠ˜ã‚Œç·šã‚°ãƒ©ãƒ•
    fig_line.add_trace(go.Scatter(
        x=line_df['æ—¥æ™‚'].dt.strftime('%Y-%m-%d-%H'),#df2_subset.index.strftime('%Y-%m-%d-%H'),
        #â˜…
        y=y_pred_subset+y_base_subset,
        #y=y_pred_subset+df2_subset.shift(1),
        mode='lines+markers',
        name='AIæ¨å®šå€¤'
    ))

    st.header('åœ¨åº«æ¨ç§»')
    fig_line.update_layout(
        #title="åœ¨åº«æ¨ç§»",
        xaxis_title="æ—¥æ™‚",
        yaxis_title="åœ¨åº«æ•°ï¼ˆç®±ï¼‰",
        height=500,  # é«˜ã•ã‚’èª¿æ•´
        width=100,   # å¹…ã‚’èª¿æ•´
        margin=dict(l=0, r=0, t=30, b=0)
    )

    # æŠ˜ã‚Œç·šã‚°ãƒ©ãƒ•ã‚’è¡¨ç¤º
    st.plotly_chart(fig_line, use_container_width=True)

    # ã‚¹ãƒ©ã‚¤ãƒ‰ãƒãƒ¼ã‚’ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢ã«é…ç½®
    min_datetime = bar_df['æ—¥æ™‚'].min().to_pydatetime()
    max_datetime = bar_df['æ—¥æ™‚'].max().to_pydatetime()
    
    print(min_datetime,max_datetime)
    
    return min_datetime, max_datetime, bar_df, df2

    # å…¨ä½“SHAPãƒ—ãƒ­ãƒƒãƒˆã®ç”Ÿæˆ
    #fig, ax = plt.subplots()
    #shap.summary_plot(shap_values, X, feature_names=X.columns, show=False)
    # ãƒ—ãƒ­ãƒƒãƒˆã‚’Streamlitã§è¡¨ç¤º
    #st.pyplot(fig)
    
def step3(bar_df, df2, selected_datetime):

    # st.write(bar_df.columns)
    # st.write(df2.columns)

    # st.dataframe(bar_df)

    # # å„ã‚«ãƒ©ãƒ ã®çµ±è¨ˆæƒ…å ±ã‚’è¨ˆç®—
    # stats = bar_df.describe().loc[['mean', 'min', 'max']]

    # # ã‚«ãƒ©ãƒ ã”ã¨ã®çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º
    # st.write("å„ã‚«ãƒ©ãƒ ã®å¹³å‡ã€æœ€å°ã€æœ€å¤§å€¤:")
    # st.dataframe(stats)

    # # å„ã‚«ãƒ©ãƒ ã®çµ±è¨ˆæƒ…å ±ã‚’ç¹°ã‚Šè¿”ã—è¡¨ç¤º
    # for column in bar_df.select_dtypes(include='number').columns:
    #     st.write(f"**{column}** ã®çµ±è¨ˆæƒ…å ±:")
    #     st.write(f"å¹³å‡: {bar_df[column].mean()}")
    #     st.write(f"æœ€å°å€¤: {bar_df[column].min()}")
    #     st.write(f"æœ€å¤§å€¤: {bar_df[column].max()}")
    #     st.write("---")

    # # æŠ˜ã‚Šè¿”ã—ç·šã‚’è¿½åŠ 
    st.markdown("---")

    st.header('è¦å› åˆ†æ')

    bar_df['æ—¥æ™‚'] = pd.to_datetime(bar_df['æ—¥æ™‚'])
    df2['æ—¥æ™‚'] = pd.to_datetime(df2['æ—¥æ™‚'])

    # é¸æŠã•ã‚ŒãŸæ—¥æ™‚ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
    filtered_df1 = bar_df[bar_df['æ—¥æ™‚'] == pd.Timestamp(selected_datetime)]
    filtered_df2 = df2[df2['æ—¥æ™‚'] == pd.Timestamp(selected_datetime)]
    
    if not filtered_df1.empty:
        st.write(f"##### é¸æŠã•ã‚ŒãŸæ—¥æ™‚: {selected_datetime}")

        # è¤‡æ•°è¡Œã®æ–‡ç« ã‚’è¡¨ç¤º
        #st.info("""
        #ğŸ”– ã€è§£ããŸã„å•é¡Œã®è§£é‡ˆã€ã‚’è¡Œã£ã¦ã„ã‚‹ã‚ã‘ã§ã¯ãªãã€ã‚ãã¾ã§ ã€å­¦ç¿’æ¸ˆã¿AIãƒ¢ãƒ‡ãƒ«ã®è§£é‡ˆã€ã®çµæœã‚’è¡¨ç¤ºã—ã¦ã„ã¾ã™ã€‚
        #""")

        # ãƒ‡ãƒ¼ã‚¿ã‚’é•·ã„å½¢å¼ã«å¤‰æ›
        df1_long = filtered_df1.melt(id_vars=['æ—¥æ™‚'], var_name='å¤‰æ•°', value_name='å¯„ä¸åº¦ï¼ˆSHAPå€¤ï¼‰')
        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å€¤ã®é™é †ã«ã‚½ãƒ¼ãƒˆ
        df1_long = df1_long.sort_values(by='å¯„ä¸åº¦ï¼ˆSHAPå€¤ï¼‰', ascending=True)

        # ãƒ›ãƒãƒ¼ãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ ã®æƒ…å ±ã‚’å«ã‚ã‚‹
        hover_data = {}
        for i, row in filtered_df2.iterrows():
            for idx, value in row.items():#iteritemsã¯ã€pandasã®Seriesã§ã¯itemsã«åç§°ãŒå¤‰æ›´
            #for idx, value in row.iteritems():
                if idx != 'æ—¥æ™‚':
                    hover_data[idx] = f"<b>æ—¥æ™‚:</b> {row['æ—¥æ™‚']}<br><b>{idx}:</b> {value:.2f}<br>"

        # æ¨ªæ£’ã‚°ãƒ©ãƒ•
        fig_bar = px.bar(df1_long,
                         x='å¯„ä¸åº¦ï¼ˆSHAPå€¤ï¼‰', y='å¤‰æ•°',
                         orientation='h',
                         labels={'å¯„ä¸åº¦ï¼ˆSHAPå€¤ï¼‰': 'å¯„ä¸åº¦ï¼ˆSHAPå€¤ï¼‰', 'å¤‰æ•°': 'å¤‰æ•°', 'æ—¥æ™‚': 'æ—¥æ™‚'},
                         title=f"{selected_datetime}ã®ãƒ‡ãƒ¼ã‚¿")

        
        # è‰²ã®è¨­å®š
        colors = ['red' if v >= 0 else 'blue' for v in df1_long['å¯„ä¸åº¦ï¼ˆSHAPå€¤ï¼‰']]
        # ãƒ›ãƒãƒ¼ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®è¨­å®š
        # SHAPå€¤ã§ã¯ãªã„ã‚‚ã®ã‚’è¡¨ç¤ºç”¨
        fig_bar.update_traces(
            marker_color=colors,
            hovertemplate=[hover_data[v] for v in df1_long['å¤‰æ•°']]
        )

        fig_bar.update_layout(
            #title="è¦å› åˆ†æ",
            height=500,  # é«˜ã•ã‚’èª¿æ•´
            width=100,   # å¹…ã‚’èª¿æ•´
            margin=dict(l=0, r=0, t=30, b=0)
        )

        # æ¨ªæ£’ã‚°ãƒ©ãƒ•ã‚’è¡¨ç¤º
        #st.plotly_chart(fig_bar, use_container_width=True)

        #display_shap_contributions(df1_long)

        # ã‚¿ãƒ–ã®ä½œæˆ
        tab1, tab2 = st.tabs(["ãƒ©ãƒ³ã‚­ãƒ³ã‚°è¡¨ç¤º", "æ£’ã‚°ãƒ©ãƒ•è¡¨ç¤º"])

        with tab1:
            display_shap_contributions(df1_long)

        with tab2:
            # ã‚°ãƒ©ãƒ•è¡¨ç¤º
            st.plotly_chart(fig_bar, use_container_width=True)


    else:
        st.write("åœ¨åº«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
