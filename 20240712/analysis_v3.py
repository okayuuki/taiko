#åˆ†æç”¨

#! ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®import
import os
import pandas as pd
import warnings
import numpy as np
import pandas as pd
#%matplotlib inline#Jupyter Notebook å°‚ç”¨ã®ãƒã‚¸ãƒƒã‚¯ã‚³ãƒãƒ³ãƒ‰ã€‚ãƒ¡ãƒ³ãƒ†ç”¨ã§åˆ©ç”¨
import matplotlib.pyplot as plt
import shap
import seaborn as sns
import matplotlib as mpl
import re
from dateutil.relativedelta import relativedelta
from IPython.display import display, clear_output
from sklearn.ensemble import RandomForestRegressor
from sklearn.impute import SimpleImputer
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
from matplotlib.gridspec import GridSpec
from datetime import datetime
from datetime import timedelta
from PIL import Image
from sklearn.linear_model import Ridge
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Lasso
from sklearn.metrics import mean_squared_error, max_error, mean_absolute_error
import streamlit as st
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime, date, time
import pickle
from sklearn.preprocessing import StandardScaler

#! è‡ªä½œãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®import
#ãƒ‡ãƒ¼ã‚¿èª­ã¿å–ã‚Šç”¨
from read_v3 import read_data, process_Activedata, read_activedata_from_IBMDB2
#ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ç”¨
from functions_v3 import display_corr_matrix, calculate_hourly_counts,calculate_business_time_base,calculate_business_time_order, \
    calculate_business_time_reception,calculate_median_lt,find_best_lag_range,create_lagged_features,add_part_supplier_info, \
        find_columns_with_word_in_name,calculate_elapsed_time_since_last_dispatch,timedelta_to_hhmmss,set_arrival_flag, \
            drop_columns_with_word,calculate_window_width,process_shiresakibin_flag,feature_engineering, \
                plot_inventory_graph, display_shap_contributions,plot_inventory_graph2

#! ãƒ•ã‚©ãƒ³ãƒˆè¨­å®šã®å¤‰æ›´ï¼ˆæ—¥æœ¬èªå¯¾å¿œã®ãŸã‚ï¼‰
mpl.rcParams['font.family'] = 'MS Gothic'
    
def show_analysis(product):

    #!å­¦ç¿’æœŸé–“ï¼ˆè§£ææœŸé–“ï¼‰ä»»æ„ã«è¨­å®šã§ãã‚‹ã‚ˆã†ã«ã€‚ç›´è¿‘1å¹´ã¨ã‹ã§
    #* ï¼œãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿åˆ©ç”¨ã™ã‚‹å ´åˆï¼
    start_date = '2024-05-01-00'
    end_date = '2024-08-31-00'
    #*ï¼œå®Ÿè¡Œæ™‚é–“ã§æ—¥æ™‚ã‚’é¸æŠã™ã‚‹å ´åˆï¼
    #current_time = datetime.now()# ç¾åœ¨ã®å®Ÿè¡Œæ™‚é–“ã‚’å–å¾—
    #end_date = (current_time - timedelta(days=1)).strftime('%Y-%m-%d-%H')# end_dateã‚’å®Ÿè¡Œæ™‚é–“ã®å‰æ—¥
    #start_date = (current_time - timedelta(days=1) - timedelta(days=180)).strftime('%Y-%m-%d-%H')# start_dateã‚’å®Ÿè¡Œæ™‚é–“ã®å‰æ—¥ã‹ã‚‰ã•ã‚‰ã«åŠå¹´å‰


    #! å‰å‡¦ç†æ¸ˆã¿ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    AutomatedRack_Details_df, arrival_times_df, kumitate_df, teikibin_df, Timestamp_df, zaiko_df = read_data(start_date, end_date)

    #! è¨­å®š
    order_time_col = 'ç™ºæ³¨æ—¥æ™‚'
    reception_time_col = 'æ¤œåæ—¥æ™‚'
    target_time_col = 'é †ç«‹è£…ç½®å…¥åº«æ—¥æ™‚'
    leave_time_col = 'é †ç«‹è£…ç½®å‡ºåº«æ—¥æ™‚'

    #! å…¨ã¦ã®è­¦å‘Šã‚’ç„¡è¦–ã™ã‚‹
    warnings.filterwarnings('ignore')
        
    #-------------------------------------------------------------
    
    #! çµæœã‚’ä¿å­˜ã™ã‚‹ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’åˆæœŸåŒ–
    results_df = pd.DataFrame(columns=['å“ç•ª','ä»•å…¥å…ˆå','å¹³å‡åœ¨åº«','Ridgeå›å¸°ã®å¹³å‡èª¤å·®', 'Ridgeå›å¸°ã®ãƒã‚¤ãƒŠã‚¹æ–¹å‘ã®æœ€å¤§èª¤å·®', 'Ridgeå›å¸°ã®ãƒ—ãƒ©ã‚¹æ–¹å‘ã®æœ€å¤§èª¤å·®',
                                           'ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã®å¹³å‡èª¤å·®', 'ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã®ãƒã‚¤ãƒŠã‚¹æ–¹å‘ã®æœ€å¤§èª¤å·®', 'ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã®ãƒ—ãƒ©ã‚¹æ–¹å‘ã®æœ€å¤§èª¤å·®'],dtype=object)
    

    #! å“ç•ªã®æ•°ã ã‘ãƒ«ãƒ¼ãƒ—ã‚’å›ã™
    #! ä»Šã¯1å“ç•ªã§
    count = 0
    for unique_product in [product]:
        
        # ç¢ºèªç”¨ï¼šå®Ÿè¡Œæ™‚ã®æ¡ä»¶ç¢ºèª
        # filtered_Timestamp_df = Timestamp_df[Timestamp_df['å“ç•ª'] == part_number]#ç‰¹å®šå“ç•ªã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
        # suppliers = filtered_Timestamp_df['ä»•å…¥å…ˆå'].unique()#è©²å½“ä»•å…¥å…ˆåã‚’æŠ½å‡º
        # supplier = str(suppliers[0])
        # count = count + 1
        # print("å“ç•ªï¼š", part_number)
        # print("ä»•å…¥å…ˆåï¼š", supplier)
        # print("ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªå“ç•ªã®æ•°ï¼š", len(Timestamp_df['å“ç•ª'].unique()))
        # print("ãƒ«ãƒ¼ãƒ—ï¼š", count)

        #! å“ç•ªã€æ•´å‚™å®¤ã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡º
        part_number = unique_product.split('_')[0]
        seibishitsu = unique_product.split('_')[1]

        #å®Ÿè¡Œçµæœã®ç¢ºèª
        #st.header(part_number)
        #st.header(seibishitsu)

        #! å†…å®¹ï¼šé–¢æ‰€æ¯ã®ã‹ã‚“ã°ã‚“æ•°ï¼ˆ1æ™‚é–“å˜ä½ï¼‰ã‚’è¨ˆç®—
        #! Argsï¼šé–¢æ‰€æ¯ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãƒ‡ãƒ¼ã‚¿ã€é–‹å§‹æ™‚é–“ã€çµ‚äº†æ™‚é–“
        #! Returnï¼šé–¢æ‰€æ¯ã®ã‹ã‚“ã°ã‚“æ•°ï¼ˆ1æ™‚é–“å˜ä½ï¼‰
        hourly_counts_of_order, _ , _ , kyoten = calculate_hourly_counts(Timestamp_df, part_number, seibishitsu, order_time_col, start_date, end_date)#ç™ºæ³¨
        hourly_counts_of_reception, delivery_info, reception_times, _ = calculate_hourly_counts(Timestamp_df, part_number, seibishitsu, reception_time_col, start_date, end_date)#æ¤œå
        hourly_counts_of_in, _ , _ , _  = calculate_hourly_counts(Timestamp_df, part_number, seibishitsu, target_time_col, start_date, end_date)#å…¥åº«
        hourly_counts_of_out, _ , _ , _ = calculate_hourly_counts(Timestamp_df, part_number, seibishitsu, leave_time_col, start_date, end_date)#å‡ºåº«

        #! å†…å®¹ï¼šæ™‚é–“é…ã‚Œã‚’è¨ˆç®—ã€‚ç™ºæ³¨ã‹ã‚‰å…¥åº«ã¾ã§ã®æ™‚é–“ã€æ¤œåã‹ã‚‰å…¥åº«ã¾ã§ã®æ™‚é–“ã‚’è¨ˆç®—ï¼ˆéç¨¼å‹•æ—¥æ™‚é–“ã‚’ã®å–ã‚Šé™¤ã„ã¦ï¼‰
        #! Argsï¼šå“ç•ªã€é–¢æ‰€æ¯ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãƒ‡ãƒ¼ã‚¿
        #! Returnï¼šç™ºæ³¨ã€œå…¥åº«LTã€æ¤œåã€œå…¥åº«LTï¼ˆæ—¥å˜ä½ï¼‰ã®ä¸­å¤®å€¤
        median_lt_order, median_lt_reception = calculate_median_lt(part_number,Timestamp_df)
        
        # Todoï¼šç™ºæ³¨æ—¥æ™‚ã¯2å±±ã‚ã‚‹ã€‚ç™ºæ³¨ã—ã¦4æ—¥å¾Œã«ç´å…¥ã›ã‚ˆã¨ã‹ã‚ã‚‹ã€åœŸæ—¥ã®å½±éŸ¿ï¼Ÿ
        #! å†…å®¹ï¼šç™ºæ³¨ã‹ã‚“ã°ã‚“æ•°ã®æœ€é©ãªå½±éŸ¿æ™‚é–“ç¯„å›²ã‚’è¦‹ã¤ã‘ã‚‹
        #! Argsï¼š1æ™‚é–“ã”ã¨ã®ç™ºæ³¨ã‹ã‚“ã°ã‚“æ•°ã€1æ™‚é–“ã”ã¨ã®å…¥åº«ã‹ã‚“ã°ã‚“æ•°ã€æ¢ç´¢æ™‚é–“ç¯„å›²
        #! Returnï¼šæœ€é©ç›¸é–¢å€¤ã€æœ€é©é–‹å§‹é…ã‚Œã€çµ‚äº†ç¯„å›²é…ã‚Œ
        min_lag =int(median_lt_order * 24)-4  # LTä¸­å¤®å€¤ã‚’åŸºæº–ã«æœ€å°é…ã‚Œæ™‚é–“ã‚’è¨­å®š
        max_lag =int(median_lt_order * 24)+4  # LTä¸­å¤®å€¤ã‚’åŸºæº–ã«æœ€å¤§é…ã‚Œæ™‚é–“ã‚’è¨­å®š
        best_corr_order, best_range_start_order, best_range_end_order = find_best_lag_range(hourly_counts_of_order, hourly_counts_of_in, min_lag, max_lag, 'ç™ºæ³¨ã‹ã‚“ã°ã‚“æ•°')

        #! å†…å®¹ï¼šç™ºæ³¨ã‹ã‚“ã°ã‚“æ•°ã®æœ€é©ãªå½±éŸ¿æ™‚é–“ç¯„å›²ã‚’è¦‹ã¤ã‘ã‚‹
        #! Argsï¼š1æ™‚é–“ã”ã¨ã®æ¤œåã‹ã‚“ã°ã‚“æ•°ã€1æ™‚é–“ã”ã¨ã®å…¥åº«ã‹ã‚“ã°ã‚“æ•°ã€æ¢ç´¢æ™‚é–“ç¯„å›²
        #! Returnï¼šæœ€é©ç›¸é–¢å€¤ã€æœ€é©é–‹å§‹é…ã‚Œã€çµ‚äº†ç¯„å›²é…ã‚Œ
        min_lag = int(median_lt_reception * 24)-4  # LTä¸­å¤®å€¤ã‚’åŸºæº–ã«æœ€å°é…ã‚Œæ™‚é–“ã‚’è¨­å®š
        max_lag = int(median_lt_reception * 24)+4  # LTä¸­å¤®å€¤ã‚’åŸºæº–ã«æœ€å¤§é…ã‚Œæ™‚é–“ã‚’è¨­å®š
        best_corr_reception, best_range_start_reception, best_range_end_reception = find_best_lag_range(hourly_counts_of_reception, hourly_counts_of_in, min_lag, max_lag, 'ç´å…¥ã‹ã‚“ã°ã‚“æ•°')
        
        # ç¢ºèªç”¨ï¼šå®Ÿè¡Œçµæœã®ç¢ºèª
        #print(f"Best range for ç™ºæ³¨: {best_range_start_order}æ™‚é–“å‰ã‹ã‚‰{best_range_end_order}æ™‚é–“å‰ã¾ã§")
        #print(f"Best correlation for ç™ºæ³¨: {best_corr_order}")
        #print(f"æ¤œåã€œå…¥åº«LTä¸­å¤®å€¤ï¼š{median_lt_reception}æ—¥,æ¤œåã€œå…¥åº«æ™‚é–“ä¸­å¤®å€¤ï¼š{median_lt_reception*24}æ™‚é–“")
        #print(f"Best range for æ¤œå: {best_range_start_reception}æ™‚é–“å‰ã‹ã‚‰{best_range_end_reception}æ™‚é–“å‰ã¾ã§")
        #print(f"Best correlation for æ¤œå: {best_corr_reception}")

        #! å†…å®¹ï¼šæœ€é©ãªå½±éŸ¿æ™‚é–“ç¯„å›²ã«åŸºã¥ã„ã¦ç™ºæ³¨ã‹ã‚“ã°ã‚“æ•°ã¨æ¤œåã‹ã‚“ã°ã‚“æ•°ã‚’è¨ˆç®—
        #! Argsï¼š1æ™‚é–“ã”ã¨ã®ç™ºæ³¨ã‹ã‚“ã°ã‚“æ•°ã€1æ™‚é–“ã”ã¨ã®å…¥åº«ã‹ã‚“ã°ã‚“æ•°ã€æœ€é©æ™‚é–“é…ã‚Œç¯„å›²
        #! Returnï¼šæœ€é©æ™‚é–“é…ã‚Œã§è¨ˆç®—ã—ãŸç™ºæ³¨ã‹ã‚“ã°ã‚“æ•°ã€å…¥åº«ã‹ã‚“ã°ã‚“æ•°
        lagged_features_order = create_lagged_features(hourly_counts_of_order, hourly_counts_of_in, hourly_counts_of_out, best_range_start_order, best_range_end_order, 'ç™ºæ³¨ã‹ã‚“ã°ã‚“æ•°', delivery_info, reception_times)
        lagged_features_reception = create_lagged_features(hourly_counts_of_reception, hourly_counts_of_in, hourly_counts_of_out, best_range_start_reception, best_range_end_reception, 'ç´å…¥ã‹ã‚“ã°ã‚“æ•°', delivery_info, reception_times)
        # å‰å‡¦ç†ï¼šé‡è¤‡ã®ã‚ã‚‹target åˆ—ã‚’å‰Šé™¤
        lagged_features_reception = lagged_features_reception.drop(columns=['å…¥åº«ã‹ã‚“ã°ã‚“æ•°ï¼ˆtï¼‰'])
        lagged_features_reception = lagged_features_reception.drop(columns=['å‡ºåº«ã‹ã‚“ã°ã‚“æ•°ï¼ˆtï¼‰'])
        # æœ€é©ãªå½±éŸ¿æ™‚é–“ç¯„å›²ã«åŸºã¥ã„ãŸç™ºæ³¨ã‹ã‚“ã°ã‚“æ•°ã¨ã€æ¤œåã‹ã‚“ã°ã‚“æ•°ã‚’çµ±åˆ
        lagged_features = lagged_features_order.join(lagged_features_reception, how='outer')

        #ç¢ºèªï¼šå®Ÿè¡Œçµæœ
        st.header("âœ…1æ™‚é–“ã‚ãŸã‚Šã®é–¢æ‰€åˆ¥ã®ã‹ã‚“ã°ã‚“æ•°ã®è¨ˆç®—å®Œäº†ã—ã¾ã—ãŸ")
        st.dataframe(lagged_features)

        #! lagged_featuresã«å¤‰æ•°è¿½åŠ 
        #! Resultï¼šã€Œæ‹ ç‚¹æ‰€ç•ªåœ°ã€åˆ—ã€ã€Œæ•´å‚™å®¤ã‚³ãƒ¼ãƒ‰ã€åˆ—ã®è¿½åŠ 
        lagged_features['å“ç•ª'] = part_number
        lagged_features['æ‹ ç‚¹æ‰€ç•ªåœ°'] = kyoten
        lagged_features['æ•´å‚™å®¤ã‚³ãƒ¼ãƒ‰'] = seibishitsu

        #! lagged_featuresã«å¤‰æ•°è¿½åŠ 
        #! Resultï¼šã€Œåœ¨åº«å¢—æ¸›æ•°ï¼ˆtï¼‰ã€åˆ—ã€ã€Œç™ºæ³¨ã‹ã‚“ã°ã‚“æ•°ï¼ˆtï¼‰ã€åˆ—ã€ã€Œç´å…¥ã‹ã‚“ã°ã‚“æ•°ï¼ˆtï¼‰ã€åˆ—ã®è¿½åŠ 
        lagged_features['åœ¨åº«å¢—æ¸›æ•°ï¼ˆtï¼‰'] = lagged_features['å…¥åº«ã‹ã‚“ã°ã‚“æ•°ï¼ˆtï¼‰'] - lagged_features['å‡ºåº«ã‹ã‚“ã°ã‚“æ•°ï¼ˆtï¼‰']#åœ¨åº«å¢—æ¸›æ•°ã‚’è¨ˆç®—
        lagged_features['ç™ºæ³¨ã‹ã‚“ã°ã‚“æ•°ï¼ˆtï¼‰'] = hourly_counts_of_order# ç™ºæ³¨ã‹ã‚“ã°ã‚“æ•°(t)ã‚’è¨ˆç®—
        lagged_features['ç´å…¥ã‹ã‚“ã°ã‚“æ•°ï¼ˆtï¼‰'] = hourly_counts_of_reception# ç´å…¥ã‹ã‚“ã°ã‚“æ•°(t)ã‚’è¨ˆç®—

        #! lagged_featuresã«å¤‰æ•°è¿½åŠ 
        #! Whatï¼šã€Œå“ç•ªã€åˆ—ã¨ã€Œæ•´å‚™å®¤ã‚³ãƒ¼ãƒ‰ã€åˆ—ã‚’ã‚‚ã¨ã«ã€ã€Œä»•å…¥å…ˆåã€åˆ—ã€ã€Œç™ºé€å ´æ‰€åã€åˆ—ã‚’æ¢ã—ã€çµ±åˆ
        #! Resultï¼šã€Œä»•å…¥å…ˆåã€åˆ—ã€ã€Œç™ºé€å ´æ‰€åï¼ˆåç§°å¤‰æ›´ã€‚æ—§ä»•å…¥ã‚Œå…ˆå·¥å ´åï¼‰ã€åˆ—ã®è¿½åŠ 
        lagged_features = add_part_supplier_info(Timestamp_df, lagged_features, seibishitsu)
        lagged_features = lagged_features.rename(columns={'ä»•å…¥å…ˆå·¥å ´å': 'ç™ºé€å ´æ‰€å'})# ã‚³ãƒ©ãƒ åå¤‰æ›´

        #! éå»ã®å‡ºåº«ã‹ã‚‰ã®çµŒéæ™‚é–“ã‚’è¨ˆç®—
        lagged_features, median_interval = calculate_elapsed_time_since_last_dispatch(lagged_features)

        #! lagged_featuresã«å¤‰æ•°è¿½åŠ 
        #! Whatï¼šã€Œæ‹ ç‚¹æ‰€ç•ªåœ°ã€åˆ—ã‚’ã‚‚ã¨ã«åœ¨åº«æ•°ã‚’ç´ã¥ã‘ã‚‹
        # ã¾ãšã€ç„¡åŠ¹ãªå€¤ã‚’ NaN ã«å¤‰æ›
        zaiko_df['æ‹ ç‚¹æ‰€ç•ªåœ°'] = pd.to_numeric(zaiko_df['æ‹ ç‚¹æ‰€ç•ªåœ°'], errors='coerce')
        # å“ç•ªã”ã¨ã«æ¬ æå€¤ï¼ˆNaNï¼‰ã‚’åŸ‹ã‚ã‚‹(å‰æ–¹åŸ‹ã‚å¾Œæ–¹åŸ‹ã‚)
        zaiko_df['æ‹ ç‚¹æ‰€ç•ªåœ°'] = zaiko_df.groupby('å“ç•ª')['æ‹ ç‚¹æ‰€ç•ªåœ°'].transform(lambda x: x.fillna(method='ffill').fillna(method='bfill'))
        # ãã‚Œã§ã‚‚ç½®æ›ã§ããªã„ã‚‚ã®ã¯NaN ã‚’ 0 ã§åŸ‹ã‚ã‚‹
        zaiko_df['æ‹ ç‚¹æ‰€ç•ªåœ°'] = zaiko_df['æ‹ ç‚¹æ‰€ç•ªåœ°'].fillna(0).astype(int).astype(str)
        # ä¸¡æ–¹ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã® 'æ‹ ç‚¹æ‰€ç•ªåœ°' åˆ—ã‚’æ–‡å­—åˆ—å‹ã«å¤‰æ›
        lagged_features['æ‹ ç‚¹æ‰€ç•ªåœ°'] = lagged_features['æ‹ ç‚¹æ‰€ç•ªåœ°'].astype(int).astype(str)
        zaiko_df['æ‹ ç‚¹æ‰€ç•ªåœ°'] = zaiko_df['æ‹ ç‚¹æ‰€ç•ªåœ°'].astype(int).astype(str)
        lagged_features = pd.merge(lagged_features, zaiko_df[['æ—¥æ™‚', 'å“ç•ª', 'åœ¨åº«æ•°ï¼ˆç®±ï¼‰','æ‹ ç‚¹æ‰€ç•ªåœ°']], on=['å“ç•ª', 'æ—¥æ™‚', 'æ‹ ç‚¹æ‰€ç•ªåœ°'], how='left')#! è‡ªå‹•ãƒ©ãƒƒã‚¯åœ¨åº«çµåˆ
        
        
        #! è™«ç©ºãæ™‚é–“ã‚’åŸ‹ã‚ã‚‹
        # 'æ—¥æ™‚' åˆ—ã§ãƒ‡ãƒ¼ã‚¿ã‚’ã‚½ãƒ¼ãƒˆ
        lagged_features = lagged_features.sort_values(by=['å“ç•ª', 'æ—¥æ™‚'])
        # åœ¨åº«æ•°ï¼ˆç®±ï¼‰ãŒ NULL ã®å ´åˆã€å‰ã®æ™‚é–“ã®åœ¨åº«æ•°ï¼ˆç®±ï¼‰ã§è£œå®Œ
        lagged_features['åœ¨åº«æ•°ï¼ˆç®±ï¼‰'] = lagged_features.groupby('å“ç•ª')['åœ¨åº«æ•°ï¼ˆç®±ï¼‰'].transform(lambda x: x.fillna(method='ffill'))

        lagged_features = pd.merge(lagged_features, AutomatedRack_Details_df, on=['æ—¥æ™‚'], how='left')#! 1æ™‚é–“ã‚ã‚ãŸã‚Šã®é–“å£åˆ¥åœ¨åº«ã®è¨ˆç®—
        for col in lagged_features.columns:
            if pd.api.types.is_timedelta64_dtype(lagged_features[col]):
                lagged_features[col] = lagged_features[col].fillna(pd.Timedelta(0))
            else:
                lagged_features[col] = lagged_features[col].fillna(0)

        #! ä»•å…¥å…ˆä¾¿åˆ°ç€ãƒ•ãƒ©ã‚°è¨ˆç®—
        lagged_features = process_shiresakibin_flag(lagged_features, arrival_times_df)

        #! lagged_features ã¨ kumitate_df ã‚’æ—¥æ™‚ã§çµ±åˆ
        lagged_features = pd.merge(lagged_features, kumitate_df[['æ—¥æ™‚','æ•´å‚™å®¤ã‚³ãƒ¼ãƒ‰','ç”Ÿç”£å°æ•°_åŠ é‡å¹³å‡æ¸ˆ','è¨ˆç”»ç”Ÿç”£å°æ•°_åŠ é‡å¹³å‡æ¸ˆ','è¨ˆç”»é”æˆç‡_åŠ é‡å¹³å‡æ¸ˆ']], on=['æ—¥æ™‚', 'æ•´å‚™å®¤ã‚³ãƒ¼ãƒ‰'], how='left')
    
        #! æœ€é©ãªé…ã‚Œæ™‚é–“ã‚’è¨ˆç®—
        best_range_order = int((best_range_start_order + best_range_end_order)/2)#æœ€é©ãªç™ºæ³¨ã‹ã‚“ã°ã‚“æ•°ã®å¹…
        best_range_reception = int((best_range_start_reception + best_range_end_reception)/2)#æœ€é©ãªç´å…¥ã‹ã‚“ã°ã‚“æ•°ã®å¹…
        
        #!å®šæœŸä¾¿
        lagged_features = pd.merge(lagged_features, teikibin_df[['æ—¥æ™‚', 'è·å½¹æ™‚é–“(t-4)','è·å½¹æ™‚é–“(t-5)','è·å½¹æ™‚é–“(t-6)']], on='æ—¥æ™‚', how='left')
        #!ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
        lagged_features = feature_engineering(lagged_features)

        #!è§£æçª“
        timelag = 48#best_range_order
        end_hours_ago = 0

        #!ãƒ­ãƒ¼ãƒªãƒ³ã‚°ç‰¹å¾´é‡
        #! è§£æçª“ã§è¨ˆç®—
        lagged_features = calculate_window_width(lagged_features, timelag, best_range_order, best_range_reception)

        #! NaNå€¤ã‚’å‡¦ç†ã™ã‚‹ï¼ˆä¾‹: 0ã§åŸ‹ã‚ã‚‹ï¼‰
        lagged_features = lagged_features.fillna(0)
        
        #    ##todo å…¨éƒ¨çµ‚ã‚ã£ãŸå¾Œã«éç¨¼å‹•æ—¥æ™‚é–“ã®ãƒ‡ãƒ¼ã‚¿è¿½åŠ ã€‚ä¸Šã¾ã§é…ã‚Œè¨ˆç®—ã§åœŸæ—¥ãªã©ã‚’é™¤å¤–ã—ã¦ã„ã‚‹ã®ã§ã€‚
        #    # è£œå®Œã™ã‚‹æ™‚é–“ç¯„å›²ã‚’æ±ºå®š
        #    full_range = pd.date_range(start=start_date, end=end_date, freq='H')
        #    # full_rangeã‚’ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«å¤‰æ›
        #    full_df = pd.DataFrame(full_range, columns=['æ—¥æ™‚'])
        #    # å…ƒã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã¨ãƒãƒ¼ã‚¸ã—ã¦æ¬ æå€¤ã‚’è£œå®Œ
        #    lagged_features = pd.merge(full_df, lagged_features, on='æ—¥æ™‚', how='left')
        #    # æ¬ æå€¤ã‚’0ã§è£œå®Œ
        #    lagged_features.fillna(0, inplace=True)
        #    #
        #    lagged_features = lagged_features.drop(columns=['åœ¨åº«æ•°ï¼ˆç®±ï¼‰'])
        #    lagged_features['å“ç•ª']=part_number
        #    lagged_features = pd.merge(lagged_features, df2[['æ—¥æ™‚', 'å“ç•ª','åœ¨åº«æ•°ï¼ˆç®±ï¼‰']], on=['å“ç•ª', 'æ—¥æ™‚'], how='left')#è‡ªå‹•ãƒ©ãƒƒã‚¯åœ¨åº«çµåˆ

        #------------------------------------------------------------------------------------------------------------------
        #todo é•·æœŸä¼‘æš‡åˆ†å‰Šé™¤
        start = '2024-08-12'
        end = '2024-08-16'
        #! æ—¥ä»˜ç¯„å›²ã«åŸºã¥ã„ã¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ã¦å‰Šé™¤
        lagged_features= lagged_features[~((lagged_features['æ—¥æ™‚'] >= start) & (lagged_features['æ—¥æ™‚'] <= end))]

        start = '2024-05-06'
        end = '2024-05-10'
        #! æ—¥ä»˜ç¯„å›²ã«åŸºã¥ã„ã¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ã¦å‰Šé™¤
        lagged_features= lagged_features[~((lagged_features['æ—¥æ™‚'] >= start) & (lagged_features['æ—¥æ™‚'] <= end))]
        #------------------------------------------------------------------------------------------------------------------

        #!é…ã‚Œåˆ†å‰Šé™¤
        data = lagged_features.iloc[300:]

        #data = data.rename(columns={'ä»•å…¥å…ˆä¾¿åˆ°ç€ãƒ•ãƒ©ã‚°': f'ä»•å…¥å…ˆä¾¿åˆ°ç€çŠ¶æ³ï¼ˆt-{best_range_reception}~t-{best_range_reception + timelag}ï¼‰'})#ã‚³ãƒ©ãƒ åå¤‰æ›´
        data['å®šæœŸä¾¿å‡ºç™ºçŠ¶æ³ï¼ˆt-4~t-6ï¼‰']=data['è·å½¹æ™‚é–“(t-4)']/50+data['è·å½¹æ™‚é–“(t-5)']/50+data['è·å½¹æ™‚é–“(t-6)']/50

        #ç¢ºèªï¼šå®Ÿè¡Œçµæœ
        st.header("âœ…å‰å‡¦ç†ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ãŒå®Œäº†ã—ã¾ã—ãŸ")
        st.dataframe(lagged_features)

        temp_data = data

        # ãƒ¢ãƒ‡ãƒ«ã‚’æ ¼ç´ã™ã‚‹ãŸã‚ã®ãƒªã‚¹ãƒˆã‚’ä½œæˆ
        rf_models = []


        #
        for i in range(3):

            if i == 0:
                data = temp_data
            elif i == 1:
                one_and_half_months_ago = pd.to_datetime(end_date) - pd.Timedelta(days=45)
                # 1ã‹æœˆåŠå‰ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
                data = temp_data[temp_data['æ—¥æ™‚'] >= one_and_half_months_ago]
            elif i == 2:
                three_and_half_months_ago_manual = pd.to_datetime(end_date) - pd.Timedelta(days=105)
                # 3ã‹æœˆåŠå‰ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
                data = temp_data[temp_data['æ—¥æ™‚'] >= three_and_half_months_ago_manual]

            #! ç•ªå·ã‚’å‰²ã‚Šå½“ã¦ã‚‹
            delay_No1 = best_range_order
            timelag_No1 = timelag
            data[f'No1_ç™ºæ³¨ã‹ã‚“ã°ã‚“æ•°ï¼ˆt-{delay_No1}~t-{delay_No1+timelag_No1}ï¼‰'] = data[f'ç™ºæ³¨ã‹ã‚“ã°ã‚“æ•°ï¼ˆt-{delay_No1}~t-{delay_No1+timelag_No1}ï¼‰']

            delay_No2 = end_hours_ago
            timelag_No2 = timelag
            data[f'No2_è¨ˆç”»çµ„ç«‹ç”Ÿç”£å°æ•°_åŠ é‡å¹³å‡ï¼ˆt-{delay_No2}~t-{delay_No2+timelag_No2}ï¼‰'] = data[f'è¨ˆç”»çµ„ç«‹ç”Ÿç”£å°æ•°_åŠ é‡å¹³å‡ï¼ˆt-{delay_No2}~t-{delay_No2+timelag_No2}ï¼‰']
            
            delay_No3 = end_hours_ago
            timelag_No3 = timelag
            data[f'No3_è¨ˆç”»é”æˆç‡_åŠ é‡å¹³å‡ï¼ˆt-{delay_No3}~t-{delay_No3+timelag_No3}ï¼‰'] = data[f'è¨ˆç”»é”æˆç‡_åŠ é‡å¹³å‡ï¼ˆt-{delay_No3}~t-{delay_No3+timelag_No3}ï¼‰']
            
            #delay_No4 = best_range_reception
            #timelag_No4 = timelag
            #data[f'No4_ç´å…¥ãƒ•ãƒ¬ï¼ˆt-{delay_No4}~t-{delay_No4+timelag_No4}ï¼‰'] = data[f'ç´å…¥ãƒ•ãƒ¬ï¼ˆt-{delay_No4}~t-{delay_No4+timelag_No4}ï¼‰']
            
            delay_No5 = best_range_reception
            timelag_No5 = 2
            data[f'No5_ä»•å…¥å…ˆä¾¿åˆ°ç€çŠ¶æ³ï¼ˆt-{delay_No5}~t-{delay_No5+timelag_No5}ï¼‰'] = data[f'ä»•å…¥å…ˆä¾¿åˆ°ç€çŠ¶æ³ï¼ˆt-{delay_No5}~t-{delay_No5+timelag_No5}ï¼‰']
            
            data['No6_å®šæœŸä¾¿å‡ºç™ºçŠ¶æ³ï¼ˆt-4~t-6ï¼‰'] = data['å®šæœŸä¾¿å‡ºç™ºçŠ¶æ³ï¼ˆt-4~t-6ï¼‰']
            
            delay_No7 = end_hours_ago
            timelag_No7 = timelag
            data[f'No7_é–“å£ã®å¹³å‡å……è¶³ç‡ï¼ˆt-{delay_No7}~t-{delay_No7+timelag_No7}ï¼‰'] = data[f'é–“å£ã®å¹³å‡å……è¶³ç‡ï¼ˆt-{delay_No7}~t-{delay_No7+timelag_No7}ï¼‰']
            
            delay_No8 = end_hours_ago
            timelag_No8 = timelag
            data[f'No8_éƒ¨å“ç½®ãå ´ã®å…¥åº«æ»ç•™çŠ¶æ³ï¼ˆt-{delay_No8}~t-{delay_No8+timelag_No8}ï¼‰'] = data[f'éƒ¨å“ç½®ãå ´ã®å…¥åº«æ»ç•™çŠ¶æ³ï¼ˆt-{delay_No8}~t-{delay_No8+timelag_No8}ï¼‰']
            
            #delay_No9 = end_hours_ago
            #timelag_No9 = timelag
            #data[f'No9_å®šæœŸä¾¿ã«ãƒ¢ãƒç„¡ã—ï¼ˆt-{delay_No9}~t-{delay_No9+timelag_No9}ï¼‰'] = data[f'å®šæœŸä¾¿ã«ãƒ¢ãƒç„¡ã—ï¼ˆt-{delay_No9}~t-{delay_No9+timelag_No9}ï¼‰']

            #! èª¬æ˜å¤‰æ•°ã®è¨­å®š
            X = data[[f'No1_ç™ºæ³¨ã‹ã‚“ã°ã‚“æ•°ï¼ˆt-{delay_No1}~t-{delay_No1+timelag_No1}ï¼‰',
                    f'No2_è¨ˆç”»çµ„ç«‹ç”Ÿç”£å°æ•°_åŠ é‡å¹³å‡ï¼ˆt-{delay_No2}~t-{delay_No2+timelag_No2}ï¼‰',
                    f'No3_è¨ˆç”»é”æˆç‡_åŠ é‡å¹³å‡ï¼ˆt-{delay_No3}~t-{delay_No3+timelag_No3}ï¼‰',
                    #f'No4_ç´å…¥ãƒ•ãƒ¬ï¼ˆt-{delay_No4}~t-{delay_No4+timelag_No4}ï¼‰',
                    f'No5_ä»•å…¥å…ˆä¾¿åˆ°ç€çŠ¶æ³ï¼ˆt-{delay_No5}~t-{delay_No5+timelag_No5}ï¼‰',
                    'No6_å®šæœŸä¾¿å‡ºç™ºçŠ¶æ³ï¼ˆt-4~t-6ï¼‰',#'è·å½¹æ™‚é–“(t-4)','è·å½¹æ™‚é–“(t-5)','è·å½¹æ™‚é–“(t-6)',
                    f'No7_é–“å£ã®å¹³å‡å……è¶³ç‡ï¼ˆt-{delay_No7}~t-{delay_No7+timelag_No7}ï¼‰',#f'é–“å£_A1ã®å……è¶³ç‡ï¼ˆt-{end_hours_ago}~t-{best_range_order}ï¼‰',f'é–“å£_A2ã®å……è¶³ç‡ï¼ˆt-{end_hours_ago}~t-{best_range_order}ï¼‰', f'é–“å£_B1ã®å……è¶³ç‡ï¼ˆt-{end_hours_ago}~t-{best_range_order}ï¼‰', f'é–“å£_B2ã®å……è¶³ç‡ï¼ˆt-{end_hours_ago}~t-{best_range_order}ï¼‰',f'é–“å£_B3ã®å……è¶³ç‡ï¼ˆt-{end_hours_ago}~t-{best_range_order}ï¼‰', f'é–“å£_B4ã®å……è¶³ç‡ï¼ˆt-{end_hours_ago}~t-{best_range_order}ï¼‰',
                    f'No8_éƒ¨å“ç½®ãå ´ã®å…¥åº«æ»ç•™çŠ¶æ³ï¼ˆt-{delay_No8}~t-{delay_No8+timelag_No8}ï¼‰'#f'éƒ¨å“ç½®ãå ´ã‹ã‚‰ã®å…¥åº«ï¼ˆt-{end_hours_ago}~t-{best_range_order}ï¼‰',f'éƒ¨å“ç½®ãå ´ã§æ»ç•™ï¼ˆt-{end_hours_ago}~t-{best_range_order}ï¼‰',
                    #f'No9_å®šæœŸä¾¿ã«ãƒ¢ãƒç„¡ã—ï¼ˆt-{delay_No9}~t-{delay_No9+timelag_No9}ï¼‰']
                    ]]
            
            #ç¢ºèªï¼šå®Ÿè¡Œçµæœ
            st.header("âœ…è§£æãƒ‡ãƒ¼ã‚¿ï¼ˆç›®çš„å¤‰æ•°ã¨è¦å› å¤‰æ•°ï¼‰ã®æº–å‚™ãŒå®Œäº†ã—ã¾ã—ãŸ")
            st.dataframe(X.head(300))

            #! ç›®çš„å¤‰æ•°ã®å®šç¾©
            y = data[f'åœ¨åº«å¢—æ¸›æ•°ï¼ˆt-0~t-{timelag}ï¼‰']
            #y = data[f'åœ¨åº«å¢—æ¸›æ•°(t)']

            # DataFrame ã«å¤‰æ›ï¼ˆåˆ—åã‚’æŒ‡å®šã™ã‚‹ï¼‰
            #y = pd.DataFrame(y, columns=[f'åœ¨åº«å¢—æ¸›æ•°ï¼ˆt-0~t-{best_range_order}ï¼‰'])

            # StandardScalerã‚’ä½¿ç”¨ã—ã¦æ¨™æº–åŒ–
            #scaler = StandardScaler()
            #y_scaled = pd.DataFrame(scaler.fit_transform(y), columns=y.columns)

            #st.dataframe(X)

            #! ãƒ‡ãƒ¼ã‚¿ã‚’å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«åˆ†å‰²
            #todo å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®å‰²åˆ
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=42)

            #! Lassoå›å¸°ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ
            ridge = Ridge(alpha=0.1)
            # ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´
            ridge.fit(X_train, y_train)
            # äºˆæ¸¬
            y_pred_train = ridge.predict(X_train)
            y_pred_test = ridge.predict(X_test)
            # è©•ä¾¡
            mse_train = mean_squared_error(y_train, y_pred_train)
            mse_test = mean_squared_error(y_test, y_pred_test)
            max_error_train = max_error(y_train, y_pred_train)
            max_error_test = max_error(y_test, y_pred_test)
            # ãƒã‚¤ãƒŠã‚¹æ–¹å‘ã®æœ€å¤§èª¤å·®ã‚’è¨ˆç®—
            min_error_train = np.min(y_train - y_pred_train)
            min_error_test = np.min(y_test - y_pred_test)

            #print(f'Ridgeå›å¸° - è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®MSE: {mse_train}')
            #print(f'Ridgeå›å¸° - ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®MSE: {mse_test}')
            #print(f'Ridgeå›å¸° - è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®æœ€å¤§èª¤å·®: {max_error_train}')
            #print(f'Ridgeå›å¸° - ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æœ€å¤§èª¤å·®: {max_error_test}')
            #print(f'Ridgeå›å¸° - è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ãƒã‚¤ãƒŠã‚¹æ–¹å‘ã®æœ€å¤§èª¤å·®: {min_error_train}')
            #print(f'Ridgeå›å¸° - ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ãƒã‚¤ãƒŠã‚¹æ–¹å‘ã®æœ€å¤§èª¤å·®: {min_error_test}')
            # å¹³å‡èª¤å·®ã‚’è¨ˆç®—
            mae = mean_absolute_error(y_test, y_pred_test)
            #print(f'Ridgeå›å¸° - ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å¹³å‡èª¤å·®: {mae}')

            #! ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´
            if i == 2:
                rf_model = RandomForestRegressor(n_estimators=10, max_depth=30, random_state=42)
                rf_model.fit(X_train, y_train)
            elif i == 1:
                rf_model2 = RandomForestRegressor(n_estimators=10, max_depth=30, random_state=42)
                rf_model2.fit(X_train, y_train)
            elif i == 0:
                rf_model3 = RandomForestRegressor(n_estimators=10, max_depth=30, random_state=42)
                rf_model3.fit(X_train, y_train)

            # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§äºˆæ¸¬ã—ã€MSEã‚’è¨ˆç®—
            #y_pred = rf_model.predict(X_test)
            #mse = mean_squared_error(y_test, y_pred)
            #print(f'ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ - ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®MSE: {mse}')
            # æœ€å¤§èª¤å·®ã‚’è¨ˆç®—
            #max_err = max_error(y_test, y_pred)
            #print(f'ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ - ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æœ€å¤§èª¤å·®: {max_err}')
            # ãƒã‚¤ãƒŠã‚¹æ–¹å‘ã®æœ€å¤§èª¤å·®ã‚’è¨ˆç®—
            #min_err = np.min(y_test - y_pred)
            #print(f'ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ - ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ãƒã‚¤ãƒŠã‚¹æ–¹å‘ã®æœ€å¤§èª¤å·®: {min_err}')
            # å¹³å‡èª¤å·®ã‚’è¨ˆç®—
            #mae2 = mean_absolute_error(y_test, y_pred)
            #st.header(mae2)
            #print(f'ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ - ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å¹³å‡èª¤å·®: {mae2}')
            #--------------------------------------------------------------------------------------------------------
            
            unique_hinban_list = lagged_features['ä»•å…¥å…ˆå'].unique()
            supply = str(unique_hinban_list[0])
            zaikozaiko = lagged_features['åœ¨åº«æ•°ï¼ˆç®±ï¼‰'].mean()
            
            #appendãƒ¡ã‚½ãƒƒãƒ‰ã¯pandasã®æœ€æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§ã¯å»ƒæ­¢
            # çµæœã‚’ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«è¿½åŠ 
            #results_df = results_df.append({'å“ç•ª': part_number,'ä»•å…¥å…ˆå':supply,'å¹³å‡åœ¨åº«':zaikozaiko,'Ridgeå›å¸°ã®å¹³å‡èª¤å·®': mae, 'Ridgeå›å¸°ã®ãƒã‚¤ãƒŠã‚¹æ–¹å‘ã®æœ€å¤§èª¤å·®': min_error_test, 'Ridgeå›å¸°ã®ãƒ—ãƒ©ã‚¹æ–¹å‘ã®æœ€å¤§èª¤å·®': max_error_test,
                                            #'ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã®å¹³å‡èª¤å·®': mae2, 'ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã®ãƒã‚¤ãƒŠã‚¹æ–¹å‘ã®æœ€å¤§èª¤å·®': min_err, 'ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã®ãƒ—ãƒ©ã‚¹æ–¹å‘ã®æœ€å¤§èª¤å·®': max_err}, ignore_index=True)

            #! å®Ÿè¡Œçµæœåé›†
            new_row = pd.DataFrame([{'å“ç•ª': part_number,'ä»•å…¥å…ˆå':supply,'å¹³å‡åœ¨åº«':zaikozaiko,'Ridgeå›å¸°ã®å¹³å‡èª¤å·®': mae, 'Ridgeå›å¸°ã®ãƒã‚¤ãƒŠã‚¹æ–¹å‘ã®æœ€å¤§èª¤å·®': min_error_test, 'Ridgeå›å¸°ã®ãƒ—ãƒ©ã‚¹æ–¹å‘ã®æœ€å¤§èª¤å·®': max_error_test}])
            results_df = pd.concat([results_df, new_row], ignore_index=True)

            #! çµ‚äº†é€šçŸ¥
            print("çµ‚äº†")
            
            #! CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            with open("temp/ä¸€æ™‚ä¿å­˜ãƒ‡ãƒ¼ã‚¿.csv", mode='w',newline='', encoding='shift_jis',errors='ignore') as f:
                data.to_csv(f)
        
        return data, rf_model, rf_model2, rf_model3, X

        ###å…¨éƒ¨çµ‚ã‚ã£ãŸå¾Œã«éç¨¼å‹•æ—¥æ™‚é–“ã®ãƒ‡ãƒ¼ã‚¿è¿½åŠ ã€‚ä¸Šã¾ã§é…ã‚Œè¨ˆç®—ã§åœŸæ—¥ãªã©ã‚’é™¤å¤–ã—ã¦ã„ã‚‹ã®ã§ã€‚
        ## è£œå®Œã™ã‚‹æ™‚é–“ç¯„å›²ã‚’æ±ºå®š
        #full_range = pd.date_range(start=start_date, end=end_date, freq='H')
        ## full_rangeã‚’ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«å¤‰æ›
        #full_df = pd.DataFrame(full_range, columns=['æ—¥æ™‚'])
        ## å…ƒã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã¨ãƒãƒ¼ã‚¸ã—ã¦æ¬ æå€¤ã‚’è£œå®Œ
        #lagged_features = pd.merge(full_df, lagged_features, on='æ—¥æ™‚', how='left')
        ## æ¬ æå€¤ã‚’0ã§è£œå®Œ
        #lagged_features.fillna(0, inplace=True)
        ##
        #lagged_features = lagged_features.drop(columns=['åœ¨åº«æ•°ï¼ˆç®±ï¼‰'])
        #lagged_features['å“ç•ª']=part_number
        #lagged_features = pd.merge(lagged_features, df2[['æ—¥æ™‚', 'å“ç•ª','åœ¨åº«æ•°ï¼ˆç®±ï¼‰']], on=['å“ç•ª', 'æ—¥æ™‚'], how='left')#è‡ªå‹•ãƒ©ãƒƒã‚¯åœ¨åº«çµåˆ

#---------------------------------------------------------------------------------------------------------------------------------

#! ã‚¹ãƒ†ãƒƒãƒ—ï¼’ã®å‡¦ç†
def step2(data, rf_model, X, start_index, end_index, step3_flag, highlight_time=None):

    #Todo å“ç•ªåã‚’å–ã‚Šå‡ºã™ãŸã‚ã«å®Ÿè¡Œã€ãã‚Œã„ã˜ã‚ƒãªã„ã‹ã‚‰è¦ä¿®æ­£
    with open('temp/model_and_data.pkl', 'rb') as file:
        rf_model, rf_model2, rf_model3, X, data, product = pickle.load(file)

    #! Activeãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    #start_date2 = '202405'
    #end_date2 = '202408'
    #ver = '00'
    #Activedata = read_activedata_from_IBMDB2(start_date2, end_date2, ver)#process_Activedata()
    #st.header(product)
    file_path = 'temp/activedata.csv'
    Activedata = pd.read_csv(file_path, encoding='shift_jis')
    # æ—¥ä»˜åˆ—ã‚’datetimeå‹ã«å¤‰æ›
    Activedata['æ—¥ä»˜'] = pd.to_datetime(Activedata['æ—¥ä»˜'], errors='coerce')
    #! å“ç•ªã€æ•´å‚™å®¤æƒ…å ±èª­ã¿è¾¼ã¿
    seibishitsu = product.split('_')[1]#æ•´å‚™å®¤ã®ã¿
    product = product.split('_')[0]#å“ç•ªã®ã¿
    #! åŒå“ç•ªã€åŒæ•´å‚™å®¤ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
    Activedata = Activedata[(Activedata['å“ç•ª'] == product) & (Activedata['æ•´å‚™å®¤'] == seibishitsu)]

    #å®Ÿè¡Œçµæœã®ç¢ºèª
    #st.header(start_index)
    #st.header(end_index)

    # åœ¨åº«ãƒ‡ãƒ¼ã‚¿ã«åˆã‚ã›ã¦æ™‚é–“ç²’åº¦ã‚’1æ™‚é–“ã”ã¨ã«ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    # å†…ç¤ºãƒ‡ãƒ¼ã‚¿ã‚’æ—¥ä»˜ã”ã¨ã«é›†ç´„ã—ã¦é‡è¤‡ã‚’æ’é™¤
    #Activedata = Activedata.groupby('æ—¥ä»˜').mean(numeric_only=True).reset_index()
    st.dataframe(Activedata)
    Activedata = Activedata.set_index('æ—¥ä»˜').resample('H').ffill().reset_index()

    #st.dataframe(Activedata.head(300))

    #æŠ˜ã‚Šè¿”ã—ç·šã‚’è¿½åŠ 
    st.markdown("---")

    #ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒ300ã‚¹ã‚¿ãƒ¼ãƒˆãªã®ã§ãƒªã‚»ãƒƒãƒˆ
    #é…ã‚Œæ™‚é–“ã®è¨ˆç®—ã®ãŸã‚
    data = data.reset_index(drop=True)
    #st.dataframe(data.head(300))

    # SHAPè¨ˆç®—
    #before
    #explainer = shap.TreeExplainer(rf_model, feature_dependence='tree_path_dependent', model_output='margin')

    #after
    explainer = shap.TreeExplainer(rf_model, model_output='raw')
    shap_values = explainer.shap_values(X)

    explainer = shap.TreeExplainer(rf_model2, model_output='raw')
    shap_values2 = explainer.shap_values(X)

    explainer = shap.TreeExplainer(rf_model3, model_output='raw')
    shap_values3 = explainer.shap_values(X)

    #ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«è©¦é¨“
    shap_values = shap_values# + shap_values2 + shap_values3
    #shap_values = shap_values
    #shap_values = shap_values2
    #shap_values = shap_values3

    first_datetime_df = data['æ—¥æ™‚'].iloc[0]
    print(f"dataã®æ—¥æ™‚åˆ—ã®æœ€åˆã®å€¤: {first_datetime_df}")

    # ãƒªã‚¹ãƒˆã‹ã‚‰æ•´æ•°ã«å¤‰æ›
    start_index_int = start_index[0]#-300
    end_index_int = end_index[0]+1#-300

    #åœ¨åº«ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
    df = data.iloc[start_index_int:end_index_int]
    print(df.head())

    #st.dataframe(df.head(300))

    first_datetime_df = df.iloc[0]
    print(f"dfã®æ—¥æ™‚åˆ—ã®æœ€åˆã®å€¤: {first_datetime_df}")

    X_subset = X.iloc[start_index_int:end_index_int]
    # ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ã¦X_subsetã‹ã‚‰äºˆæ¸¬å€¤ã‚’è¨ˆç®—
    y_pred_subset = rf_model.predict(X_subset)

    df['æ—¥æ™‚'] = pd.to_datetime(df['æ—¥æ™‚'])
    df.set_index('æ—¥æ™‚', inplace=True)

    #df2 = df['åœ¨åº«å¢—æ¸›æ•°ï¼ˆt-52~t-0ï¼‰']
    df2 = df['åœ¨åº«æ•°ï¼ˆç®±ï¼‰']
    print(df2.head())

    #åœ¨åº«æ•°ï¼ˆç®±ï¼‰ã‚’è¨ˆç®—ã™ã‚‹
    best_range_order = find_columns_with_word_in_name(df, 'åœ¨åº«æ•°ï¼ˆç®±ï¼‰ï¼ˆt-')
    yyyy = df[f'{best_range_order}']
    y_base_subset = yyyy

    #st.dataframe(y_base_subset.head(300))

    #åœ¨åº«å¢—æ¸›æ•°ã®å¹³å‡å€¤ã‚’ç¢ºèªç”¨
    #mean_value = y.mean()

    # SHAPå€¤ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä½œæˆ
    shap_df = pd.DataFrame(shap_values, columns=X.columns)

    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®å¹³å‡SHAPå€¤ã«åŸºã¥ã„ã¦ç‰¹å¾´é‡ã‚’ä¸¦ã³æ›¿ãˆ
    shap_df_mean = shap_df.abs().mean().sort_values(ascending=False)
    sorted_columns = shap_df_mean.index

    shap_df_sorted = shap_df[sorted_columns]

    dfdf = shap_df_sorted.iloc[start_index_int:end_index_int].T

    # ãƒ—ãƒ­ãƒƒãƒˆã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã®ç¯„å›²ã‚’ã‚¹ãƒ©ã‚¤ã‚¹
    dfdf_subset = dfdf#.iloc[:, start_idx:end_idx]

    dfdf_subset2 = dfdf_subset

    # å‰ã®å€¤ã¨ã®å·®åˆ†ã‚’è¨ˆç®—
    # å·®åˆ†ã¨å·®åˆ†åˆ¤å®šã‚’foræ–‡ã§è¨ˆç®—
    #difference = [None]  # æœ€åˆã®å·®åˆ†ã¯ãªã—
    #for i in range(1,len(df2_subset)):
    #    diff = df2_subset.iloc[i] - df2_subset.iloc[i-1]
    #    difference.append(diff)
    #    if i < len(dfdf_subset2):
    #        if diff > 0:
    #            dfdf_subset2.iloc[i] = dfdf_subset2.iloc[i]
    #        elif diff < 0:
    #            dfdf_subset2.iloc[i] = -1*dfdf_subset2.iloc[i]

    #--------------------------------------------------------------------------------------------

    df = dfdf_subset2

    # dfã®åˆ—æ•°ã¨df2_subsetã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ•°ã‚’ç¢ºèª
    print(f"data index: {len(data)}")
    print(f"df columns: {len(df.columns)}")
    #print(f"df2_subset index: {len(df2_subset.index)}")
    print(f"shap_df_sorted index: {len(shap_df_sorted)}")
    print(f"dfdf index: {len(dfdf)}")
    print(f"dfdf_subset2 index: {len(dfdf_subset2)}")


    # ã‚«ãƒ©ãƒ¼ãƒãƒƒãƒ—ã®é¸æŠ
    #cmap = 'RdBu_r'  # é’ã‹ã‚‰èµ¤ã«å¤‰åŒ–ã™ã‚‹ã‚«ãƒ©ãƒ¼ãƒãƒƒãƒ—

    #df2_subset.index = df2_subset.index.strftime('%Y-%m-%d-%H')
    df.columns = df2.index.strftime('%Y-%m-%d-%H')

    #è¡Œã®ä¸¦ã³ã‚’åè»¢
    #df_reversed = df.iloc[::-1]

    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ãƒªã‚»ãƒƒãƒˆ
    df2_subset_df = df2.to_frame().reset_index()

    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®è¡Œã¨åˆ—ã‚’å…¥ã‚Œæ›¿ãˆ
    df_transposed = df.transpose()
    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¦æ—¥æ™‚åˆ—ã‚’ä½œæˆ
    df_transposed.reset_index(inplace=True)
    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åˆ—ã®åå‰ã‚’ 'æ—¥æ™‚' ã«å¤‰æ›´
    df_transposed.rename(columns={'index': 'æ—¥æ™‚'}, inplace=True)

    #èª¬æ˜å¤‰æ•°
    zzz = X.iloc[start_index_int:end_index_int]#[start_idx:end_idx]
    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ãƒªã‚»ãƒƒãƒˆ
    zzz = zzz.reset_index(drop=True)
    #æ—¥æ™‚åˆ—
    temp_time = df_transposed.reset_index(drop=True)

    #ç¢ºèªç”¨
    #first_datetime_df1 = data['æ—¥æ™‚'].iloc[0]
    #first_datetime_df2 = temp_time['æ—¥æ™‚'].iloc[0]
    #first_datetime_df3 = df_transposed['æ—¥æ™‚'].iloc[0]
    #print(f"dataã®æ—¥æ™‚åˆ—ã®æœ€åˆã®å€¤: {first_datetime_df1}")
    #print(f"df_transposedã®æ—¥æ™‚åˆ—ã®æœ€åˆã®å€¤: {first_datetime_df3}")
    #print(f"temp_timeã®æ—¥æ™‚åˆ—ã®æœ€åˆã®å€¤: {first_datetime_df2}")

    #! æ—¥æ™‚åˆ—ã¨èª¬æ˜å¤‰æ•°ã‚’çµåˆ
    merged_df = pd.concat([temp_time[['æ—¥æ™‚']], zzz], axis=1)
    
    #! å¤‰æ•°åã‚’å¤‰æ›´ã™ã‚‹
    line_data = df2_subset_df #åœ¨åº«ãƒ‡ãƒ¼ã‚¿
    bar_data = df_transposed #SHAPå€¤
    df2 = merged_df #å…ƒãƒ‡ãƒ¼ã‚¿
    
    #! åœ¨åº«ãƒ‡ãƒ¼ã‚¿ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ åŒ–
    line_df = pd.DataFrame(line_data)
    line_df['æ—¥æ™‚'] = pd.to_datetime(line_df['æ—¥æ™‚'], format='%Y%m%d%H')

    #! SHAPå€¤ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ åŒ–
    bar_df = pd.DataFrame(bar_data)
    bar_df['æ—¥æ™‚'] = pd.to_datetime(bar_df['æ—¥æ™‚'])
    
    #! å…ƒãƒ‡ãƒ¼ã‚¿ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ åŒ–
    df2 = pd.DataFrame(df2)
    df2['æ—¥æ™‚'] = pd.to_datetime(df2['æ—¥æ™‚'])

    #ç¢ºèª
    #st.dataframe(line_df.head(300))
    #print("å¢—æ¸›")
    #print(y_pred_subset)
    #st.dataframe(y_pred_subset)
    #print("ãƒ™ãƒ¼ã‚¹")
    #print(y_base_subset)
    #st.dataframe(y_base_subset)

    #! é–‹ç¤ºæ™‚é–“ã¨çµ‚äº†æ™‚é–“ã‚’è¨ˆç®—
    #start_datetime = bar_df['æ—¥æ™‚'].min().to_pydatetime()
    #end_datetime = bar_df['æ—¥æ™‚'].max().to_pydatetime()

    #Activedata = Activedata[(Activedata['æ—¥ä»˜'] >= start_datetime) & 
                                     # (Activedata['æ—¥ä»˜'] <= end_datetime)]
    
    # bar_dfã®æ™‚é–“å¸¯ã‚’æŠ½å‡º
    bar_times = bar_df['æ—¥æ™‚']
    
    #st.dataframe(bar_times)
    #st.dataframe(bar_df['æ—¥æ™‚'])
    #st.dataframe(Activedata['æ—¥ä»˜'])

    # Activedataã®æ™‚é–“å¸¯ã‚’æŠ½å‡ºã—ã€bar_dfã®æ™‚é–“å¸¯ã¨ä¸€è‡´ã™ã‚‹ã‚‚ã®ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    Activedata = Activedata[Activedata['æ—¥ä»˜'].isin(bar_times)]

    #! ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’è¡¨ç¤º
    st.header('åœ¨åº«æƒ…å ±')

    if step3_flag == 0:
        #! åœ¨åº«å¯è¦–åŒ–
        plot_inventory_graph(line_df, y_pred_subset, y_base_subset, Activedata)
    elif step3_flag == 1:
        plot_inventory_graph2(line_df, y_pred_subset, y_base_subset, Activedata, highlight_time)

    #å®Ÿè¡Œçµæœã®ç¢ºèª
    #st.dataframe(line_df)
    
    #å®Ÿè¡Œçµæœã®ç¢ºèªï¼›é–‹å§‹æ™‚åˆ»ã¨çµ‚äº†æ™‚åˆ»
    #print(strat_datetime,end_datetime)

    #å®Ÿè¡Œçµæœã®ç¢ºèªï¼šå…¨ä½“SHAPãƒ—ãƒ­ãƒƒãƒˆã®ç”Ÿæˆ
    #fig, ax = plt.subplots()
    #shap.summary_plot(shap_values, X, feature_names=X.columns, show=False)
    #ãƒ—ãƒ­ãƒƒãƒˆã‚’Streamlitã§è¡¨ç¤º
    #st.pyplot(fig)
    
    #! STEP3ã®è¦å› åˆ†æçµæœã®å¯è¦–åŒ–ã®ãŸã‚ã«ã€é–‹å§‹æ—¥æ™‚ï¼ˆstrat_datetimeï¼‰ã¨çµ‚äº†æ—¥æ™‚ï¼ˆend_datetimeï¼‰ã€
    #! SHAPå€¤ï¼ˆbar_dfï¼‰ã€å…ƒãƒ‡ãƒ¼ã‚¿å€¤ï¼ˆdf2ï¼‰ã‚’å‡ºåŠ›ã™ã‚‹
    return bar_df, df2, line_df

#! ã‚¹ãƒ†ãƒƒãƒ—ï¼“ã®å‡¦ç†
def step3(bar_df, df2, selected_datetime, line_df):

    st.dataframe(df2)
    st.dataframe(line_df)
    st.dataframe(bar_df)

    #! æŠ˜ã‚Šè¿”ã—ç·šã‚’è¿½åŠ 
    st.markdown("---")

    #! ãƒ˜ãƒƒãƒ€ãƒ¼è¡¨ç¤º
    st.header('è¦å› åˆ†æ')

    #! Activeãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
    #Todo å“ç•ªåã‚’å–ã‚Šå‡ºã™ãŸã‚ã«å®Ÿè¡Œã€ãã‚Œã„ã˜ã‚ƒãªã„ã‹ã‚‰è¦ä¿®æ­£
    with open('temp/model_and_data.pkl', 'rb') as file:
        rf_model,rf_model2,rf_model3, X, data, product = pickle.load(file)
    #ã€€Activeãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    #start_date2 = '202405'
    #end_date2 = '202408'
    #ver = '00'
    file_path = 'temp/activedata.csv'
    Activedata = pd.read_csv(file_path, encoding='shift_jis')
    # æ—¥ä»˜åˆ—ã‚’datetimeå‹ã«å¤‰æ›
    Activedata['æ—¥ä»˜'] = pd.to_datetime(Activedata['æ—¥ä»˜'], errors='coerce')
    #ã€€å“ç•ªã€æ•´å‚™å®¤æƒ…å ±èª­ã¿è¾¼ã¿
    seibishitsu = product.split('_')[1]#æ•´å‚™å®¤ã®ã¿
    product = product.split('_')[0]#å“ç•ªã®ã¿
    #ã€€åŒå“ç•ªã€åŒæ•´å‚™å®¤ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
    Activedata = Activedata[(Activedata['å“ç•ª'] == product) & (Activedata['æ•´å‚™å®¤'] == seibishitsu)]
    #ã€€1æ™‚é–“å˜ä½ã«å¤‰æ›
    Activedata = Activedata.set_index('æ—¥ä»˜').resample('H').ffill().reset_index()

    #! SHAPå€¤ï¼ˆbar_dfï¼‰ã€å…ƒãƒ‡ãƒ¼ã‚¿å€¤ï¼ˆdf2ï¼‰ã®æ—¥æ™‚ã‚’datetimeå‹ã«ã™ã‚‹ã€€
    bar_df['æ—¥æ™‚'] = pd.to_datetime(bar_df['æ—¥æ™‚'])
    df2['æ—¥æ™‚'] = pd.to_datetime(df2['æ—¥æ™‚'])

    #! é¸æŠã•ã‚ŒãŸæ—¥æ™‚ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
    filtered_df1 = bar_df[bar_df['æ—¥æ™‚'] == pd.Timestamp(selected_datetime)]
    filtered_df2 = df2[df2['æ—¥æ™‚'] == pd.Timestamp(selected_datetime)]
    
    #! 
    if not filtered_df1.empty:

        zaikosu = line_df.loc[line_df['æ—¥æ™‚'] == selected_datetime, 'åœ¨åº«æ•°ï¼ˆç®±ï¼‰'].values[0]

        #! 2ã¤ã®metricã‚’ä½œæˆ
        col1, col2 = st.columns(2)
        col1.metric(label="é¸æŠã•ã‚ŒãŸæ—¥æ™‚", value=selected_datetime)#, delta="1 mph")
        col2.metric(label="åœ¨åº«æ•°ï¼ˆç®±ï¼‰", value=int(zaikosu))

        #! ãƒ‡ãƒ¼ã‚¿ã‚’é•·ã„å½¢å¼ã«å¤‰æ›
        df1_long = filtered_df1.melt(id_vars=['æ—¥æ™‚'], var_name='å¤‰æ•°', value_name='å¯„ä¸åº¦ï¼ˆSHAPå€¤ï¼‰')
        #! ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å€¤ã®é™é †ã«ã‚½ãƒ¼ãƒˆ
        df1_long = df1_long.sort_values(by='å¯„ä¸åº¦ï¼ˆSHAPå€¤ï¼‰', ascending=True)

        # ãƒ›ãƒãƒ¼ãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ ã®æƒ…å ±ã‚’å«ã‚ã‚‹
        hover_data = {}
        for i, row in filtered_df2.iterrows():
            for idx, value in row.items():#iteritemsã¯ã€pandasã®Seriesã§ã¯itemsã«åç§°ãŒå¤‰æ›´
            #for idx, value in row.iteritems():
                if idx != 'æ—¥æ™‚':
                    hover_data[idx] = f"<b>æ—¥æ™‚:</b> {row['æ—¥æ™‚']}<br><b>{idx}:</b> {value:.2f}<br>"

        # æ¨ªæ£’ã‚°ãƒ©ãƒ•
        fig_bar = px.bar(df1_long,
                         x='å¯„ä¸åº¦ï¼ˆSHAPå€¤ï¼‰', y='å¤‰æ•°',
                         orientation='h',
                         labels={'å¯„ä¸åº¦ï¼ˆSHAPå€¤ï¼‰': 'å¯„ä¸åº¦ï¼ˆSHAPå€¤ï¼‰', 'å¤‰æ•°': 'å¤‰æ•°', 'æ—¥æ™‚': 'æ—¥æ™‚'},
                         title=f"{selected_datetime}ã®ãƒ‡ãƒ¼ã‚¿")

        
        # è‰²ã®è¨­å®š
        colors = ['red' if v >= 0 else 'blue' for v in df1_long['å¯„ä¸åº¦ï¼ˆSHAPå€¤ï¼‰']]
        # ãƒ›ãƒãƒ¼ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®è¨­å®š
        # SHAPå€¤ã§ã¯ãªã„ã‚‚ã®ã‚’è¡¨ç¤ºç”¨
        fig_bar.update_traces(
            marker_color=colors,
            hovertemplate=[hover_data[v] for v in df1_long['å¤‰æ•°']]
        )

        fig_bar.update_layout(
            #title="è¦å› åˆ†æ",
            height=500,  # é«˜ã•ã‚’èª¿æ•´
            width=100,   # å¹…ã‚’èª¿æ•´
            margin=dict(l=0, r=0, t=30, b=0)
        )

        #! ã‚¿ãƒ–ã®ä½œæˆ
        tab1, tab2 = st.tabs(["ãƒ©ãƒ³ã‚­ãƒ³ã‚°è¡¨ç¤º", "æ£’ã‚°ãƒ©ãƒ•è¡¨ç¤º"])

        with tab1:

            #! ã‚‚ã— 'Unnamed: 0' ã‚„ 'æ—¥æ™‚' ãŒå­˜åœ¨ã™ã‚‹å ´åˆã«ã®ã¿å‰Šé™¤ã™ã‚‹ã‚ˆã†å¤‰æ•°ã‚’ä½œæˆ
            columns_to_drop = []
            if 'Unnamed: 0' in df2.columns:
                columns_to_drop.append('Unnamed: 0')
            if 'æ—¥æ™‚' in df2.columns:
                columns_to_drop.append('æ—¥æ™‚')

            #!  'Unnamed: 0' ã‚„ 'æ—¥æ™‚' ã‚’å‰Šé™¤ã™ã‚‹
            df2_cleaned = df2.drop(columns=columns_to_drop)

            #! å„è¦å› ã®å€¤ã®å¹³å‡å€¤ã¨ä¸­å¤®å€¤ã‚’è¨ˆç®—
            average_values = df2_cleaned.mean()
            median_values = df2_cleaned.median()

            #st.dataframe(median_values)
            #st.header(type(median_values))
            #st.header(median_values.index)

            def extract_kanban_t_values_from_index(df):
                # æ­£è¦è¡¨ç¾ãƒ‘ã‚¿ãƒ¼ãƒ³: ç™ºæ³¨ã‹ã‚“ã°ã‚“æ•° + t-X~t-Y
                pattern = r'ç™ºæ³¨ã‹ã‚“ã°ã‚“æ•°.*\ï¼ˆt-(\d+)~t-(\d+)\ï¼‰'
                
                # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’èµ°æŸ»
                for index in df.index:
                    index_value = str(index)  # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®å€¤ã‚’æ–‡å­—åˆ—ã¨ã—ã¦å–å¾—
                    match = re.search(pattern, index_value)  # æ­£è¦è¡¨ç¾ã§ãƒãƒƒãƒãƒ³ã‚°
                    if match:
                        X = match.group(1)  # Xã®å€¤
                        Y = match.group(2)  # Yã®å€¤
                        return X, Y  # ä¸€ã¤ã®ã‚»ãƒ«ãŒè¦‹ã¤ã‹ã‚Œã°çµ‚äº†
                return None, None
            
            # é–¢æ•°ã®å‘¼ã³å‡ºã—
            hacchu_start, hacchu_end = extract_kanban_t_values_from_index(median_values)
            #st.write(hacchu_start, hacchu_end)

            def calculate_hacchu_times(hacchu_start, hacchu_end, time_str):
                # hacchu_startã¨hacchu_endãŒæ–‡å­—åˆ—ã®å ´åˆã€æ•´æ•°ã«å¤‰æ›
                hacchu_start = int(hacchu_start)
                hacchu_end = int(hacchu_end)

                #hacchu_start = 2
                #hacchu_end = 24

                # æ™‚é–“ã®æ–‡å­—åˆ—ã‚’datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
                base_time = datetime.strptime(time_str, "%Y-%m-%d %H:%M")
                
                # hacchu_startæ™‚é–“ã¨hacchu_endæ™‚é–“ã‚’å¼•ã
                time_start = base_time - timedelta(hours=hacchu_start)
                time_end = base_time - timedelta(hours=hacchu_end)
                
                # çµæœã‚’è¡¨ç¤º
                #st.write(f"{time_str} - {hacchu_start} æ™‚é–“ = {time_start}")
                #st.write(f"{time_str} - {hacchu_end} æ™‚é–“ = {time_end}")

                return time_start, time_end

            # ä¾‹ã¨ã—ã¦ã€2024-06-11 17:00 ã‹ã‚‰è¨ˆç®—
            hacchu_start_time, hacchu_end_time = calculate_hacchu_times(hacchu_start, hacchu_end, selected_datetime)

            # ç‰¹å®šã®æ™‚é–“å¸¯ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
            filtered_data = Activedata[(Activedata['æ—¥ä»˜'] >= hacchu_end_time) & (Activedata['æ—¥ä»˜'] <= hacchu_start_time)]

            #st.dataframe(filtered_data)

            total_ave = filtered_data['ä¾¿Ave'].sum()/24*filtered_data['ã‚µã‚¤ã‚¯ãƒ«å›æ•°'].median()

            #st.header(total_ave)

            # DataFrameã«å¤‰æ›
            average_df = pd.DataFrame(average_values, columns=["å¹³å‡å€¤"])
            average_df.index.name = 'å¤‰æ•°'
            median_df = pd.DataFrame(median_values, columns=["åŸºæº–å€¤"])
            median_df.index.name = 'å¤‰æ•°'

            def update_values_for_kanban(df,total_ave):
                # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’èµ°æŸ»
                for index in df.index:
                    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«ã€Œç™ºæ³¨ã‹ã‚“ã°ã‚“æ•°ã€ã‚’å«ã‚€ã‹ãƒã‚§ãƒƒã‚¯
                    if "ç™ºæ³¨ã‹ã‚“ã°ã‚“æ•°" in str(index):
                        # è©²å½“ã™ã‚‹è¡Œã®ã™ã¹ã¦ã®å€¤ã‚’ 10 ã«è¨­å®š
                        df.loc[index] = total_ave
                return df

            # é–¢æ•°ã®é©ç”¨ä¾‹
            median_df = update_values_for_kanban(median_df,total_ave)

            #çµ±åˆ
            df1_long = pd.merge(df1_long, average_df, left_on="å¤‰æ•°", right_on="å¤‰æ•°", how="left")
            df1_long = pd.merge(df1_long, median_df, left_on="å¤‰æ•°", right_on="å¤‰æ•°", how="left")

            # SHAPãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ç¹°ã‚Šè¿”ã—å‡¦ç†ã—ã€å¯¾å¿œã™ã‚‹å…ƒè¦å› ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®å€¤ã‚’è¿½åŠ 
            for index, row in df1_long.iterrows():
                variable = row['å¤‰æ•°']  # SHAPãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®ã€Œå¤‰æ•°ã€åˆ—ã‚’å–å¾—
                if variable in filtered_df2.columns:  # å¤‰æ•°åãŒå…ƒè¦å› ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®åˆ—åã«å­˜åœ¨ã™ã‚‹å ´åˆ
                    # SHAPãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®ç¾åœ¨ã®è¡Œã«å…ƒè¦å› ã®å€¤ã‚’è¿½åŠ 
                    df1_long.at[index, 'è¦å› ã®å€¤'] = filtered_df2.loc[filtered_df2['æ—¥æ™‚'] == row['æ—¥æ™‚'], variable].values[0]

            #! é †ä½è¡¨ã‚’è¡¨ç¤º
            display_shap_contributions(df1_long)
          
            # èƒŒæ™¯ã‚’é’ãã—ã¦ã€æƒ…å ±ãƒœãƒƒã‚¯ã‚¹ã®ã‚ˆã†ã«è¦‹ã›ã‚‹
            st.markdown("""
            <div style="background-color: #ffffff; padding: 10px; border-radius: 5px;">
            ğŸ“Œ <strong>åŸºæº–å€¤ã«ã¤ã„ã¦ã®èª¬æ˜ï¼ˆè¦å› ã®å€¤ãŒå¤§ãã„ã‹å°ã•ã„ã‹ã€æ­£å¸¸ãªã®ã‹ç•°å¸¸ãªã®ã‹ã‚’åˆ¤æ–­ã™ã‚‹ãŸã‚ã®æŒ‡æ¨™ï¼‰</strong><br>
            <ul>
            <li><strong>ç™ºæ³¨ã‹ã‚“ã°ã‚“æ•°ã®åŸºæº–å€¤</strong>ï¼šActiveã®æ—¥é‡æ•°ï¼ˆç®±æ•°ï¼‰Ã— å¯¾è±¡æœŸé–“</li>
            <li><strong>è¨ˆç”»çµ„ç«‹ç”Ÿç”£å°æ•°ã®åŸºæº–å€¤</strong>ï¼šéå»åŠå¹´ã®ä¸­å¤®å€¤</li>
            <li><strong>çµ„ç«‹ãƒ©ã‚¤ãƒ³ã®ç¨¼åƒç‡ã®åŸºæº–å€¤</strong>ï¼šéå»åŠå¹´ã®ä¸­å¤®å€¤</li>
            <li><strong>é–“å£ã®å……è¶³ç‡ã®åŸºæº–å€¤</strong>ï¼šéå»åŠå¹´ã®ä¸­å¤®å€¤</li>
            <li><strong>è¥¿å°¾æ±ã‹éƒ¨å“ç½®ãå ´ã§æ»ç•™ã—ã¦ã„ã‚‹ã®åŸºæº–å€¤</strong>ï¼šéå»åŠå¹´ã®ä¸­å¤®å€¤</li>
            <li><strong>ä»•å…¥å…ˆä¾¿ã®åˆ°ç€ãƒ•ãƒ©ã‚°ã®åŸºæº–å€¤</strong>ï¼šéå»åŠå¹´ã®ä¸­å¤®å€¤</li>
            <li><strong>å®šæœŸä¾¿ã®å‡ºç™ºãƒ•ãƒ©ã‚°ã®åŸºæº–å€¤</strong>ï¼šéå»åŠå¹´ã®ä¸­å¤®å€¤</li>
            </ul>
            </div>
            """, unsafe_allow_html=True)

        with tab2:

            #! æ£’ã‚°ãƒ©ãƒ•è¡¨ç¤º
            st.plotly_chart(fig_bar, use_container_width=True)


    else:
        st.write("åœ¨åº«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
